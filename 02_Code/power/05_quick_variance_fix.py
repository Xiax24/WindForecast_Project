#!/usr/bin/env python3
"""
ç‹¬ç«‹çš„æ–¹å·®è®¡ç®—è¯Šæ–­è„šæœ¬
ä¸ä¾èµ–å…¶ä»–æ¨¡å—ï¼Œç›´æ¥ä»ä¿å­˜çš„ç»“æœæ–‡ä»¶ä¸­åˆ†æé—®é¢˜
"""

import numpy as np
import pandas as pd
import joblib
from sklearn.preprocessing import StandardScaler

def load_analysis_data():
    """åŠ è½½å·²ä¿å­˜çš„åˆ†ææ•°æ®"""
    print("ğŸ“¦ åŠ è½½å·²ä¿å­˜çš„åˆ†ææ•°æ®...")
    
    # è·¯å¾„é…ç½®
    DATA_PATH = "/Users/xiaxin/work/WindForecast_Project/03_Results/04error_propagation_data"
    MODEL_PATH = "/Users/xiaxin/work/WindForecast_Project/03_Results/03saved_models"
    RESULTS_PATH = "/Users/xiaxin/work/WindForecast_Project/03_Results/04error_propagation_analysis"
    
    try:
        # åŠ è½½é¢„å¤„ç†æ•°æ®
        obs_features = np.load(f"{DATA_PATH}/obs_features.npy")
        ecmwf_features = np.load(f"{DATA_PATH}/ecmwf_features.npy")
        gfs_features = np.load(f"{DATA_PATH}/gfs_features.npy")
        actual_power = np.load(f"{DATA_PATH}/actual_power.npy")
        
        # åŠ è½½ç‰¹å¾åç§°
        feature_mapping = joblib.load(f"{DATA_PATH}/feature_mapping.pkl")
        feature_names = feature_mapping['obs_features']
        
        # åŠ è½½æ¨¡å‹
        model = joblib.load(f"{MODEL_PATH}/best_lightgbm_model.pkl")
        
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ:")
        print(f"  æ ·æœ¬æ•°: {len(actual_power)}")
        print(f"  ç‰¹å¾æ•°: {len(feature_names)}")
        
        return {
            'obs_features': obs_features,
            'ecmwf_features': ecmwf_features, 
            'gfs_features': gfs_features,
            'actual_power': actual_power,
            'feature_names': feature_names,
            'model': model
        }
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
        return None

def perform_error_decomposition(data):
    """æ‰§è¡Œè¯¯å·®åˆ†è§£"""
    print("ğŸ”¬ æ‰§è¡Œè¯¯å·®åˆ†è§£...")
    
    model = data['model']
    
    # é¢„æµ‹
    P_obs = model.predict(data['obs_features'])
    P_ecmwf = model.predict(data['ecmwf_features'])
    P_gfs = model.predict(data['gfs_features'])
    
    # è¯¯å·®åˆ†è§£
    modeling_error = P_obs - data['actual_power']
    ecmwf_propagation = P_ecmwf - P_obs
    gfs_propagation = P_gfs - P_obs
    
    print(f"  å»ºæ¨¡è¯¯å·® RMSE: {np.sqrt(np.mean(modeling_error**2)):.3f}")
    print(f"  ECMWFä¼ æ’­è¯¯å·® RMSE: {np.sqrt(np.mean(ecmwf_propagation**2)):.3f}")
    print(f"  GFSä¼ æ’­è¯¯å·® RMSE: {np.sqrt(np.mean(gfs_propagation**2)):.3f}")
    
    return {
        'P_obs': P_obs,
        'P_ecmwf': P_ecmwf, 
        'P_gfs': P_gfs,
        'modeling_error': modeling_error,
        'ecmwf_propagation': ecmwf_propagation,
        'gfs_propagation': gfs_propagation
    }

def compute_corrected_sensitivity(data, n_samples=3000):
    """é‡æ–°è®¡ç®—æ­£ç¡®çš„æ•æ„Ÿæ€§"""
    print("ğŸ” é‡æ–°è®¡ç®—æ•æ„Ÿæ€§...")
    
    # é‡è¦ç‰¹å¾
    important_features = [
        'obs_wind_speed_70m',
        'obs_wind_speed_50m', 
        'obs_wind_speed_30m',
        'obs_wind_speed_10m',
        'obs_temperature_10m',
        'obs_wind_dir_sin_70m',
        'obs_wind_dir_cos_70m'
    ]
    
    # è¿‡æ»¤å­˜åœ¨çš„ç‰¹å¾
    analyzed_features = [f for f in important_features if f in data['feature_names']]
    feature_indices = [data['feature_names'].index(f) for f in analyzed_features]
    
    print(f"  åˆ†æç‰¹å¾: {analyzed_features}")
    
    # é‡‡æ ·
    if len(data['obs_features']) > n_samples:
        indices = np.random.choice(len(data['obs_features']), n_samples, replace=False)
        sample_features = data['obs_features'][indices]
    else:
        sample_features = data['obs_features']
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    normalized_features = scaler.fit_transform(sample_features)
    
    # é¢„æµ‹åŒ…è£…å™¨
    def predict_from_normalized(norm_input):
        original_input = scaler.inverse_transform(norm_input)
        return data['model'].predict(original_input)
    
    # è®¡ç®—æ•æ„Ÿæ€§
    sensitivities = {}
    
    for i, feature_name in enumerate(analyzed_features):
        feature_idx = feature_indices[i]
        
        # æ‰°åŠ¨è®¡ç®—
        delta = 0.01
        
        features_plus = normalized_features.copy()
        features_plus[:, feature_idx] += delta
        pred_plus = predict_from_normalized(features_plus)
        
        features_minus = normalized_features.copy()
        features_minus[:, feature_idx] -= delta
        pred_minus = predict_from_normalized(features_minus)
        
        # æ ‡å‡†åŒ–æ¢¯åº¦
        normalized_gradient = (pred_plus - pred_minus) / (2 * delta)
        
        # ç‰¹å¾ç»Ÿè®¡
        feature_std = np.sqrt(scaler.var_[feature_idx])
        feature_mean = scaler.mean_[feature_idx]
        
        # ç‰©ç†æ•æ„Ÿæ€§ï¼ˆç‰¹å¾å˜åŒ–1ä¸ªæ ‡å‡†å·®çš„å½±å“ï¼‰
        physical_sensitivity = np.mean(normalized_gradient) * feature_std
        
        sensitivities[feature_name] = {
            'normalized_gradient': np.mean(normalized_gradient),
            'normalized_gradient_std': np.std(normalized_gradient),
            'physical_sensitivity': physical_sensitivity,
            'abs_physical_sensitivity': abs(physical_sensitivity),
            'feature_std': feature_std,
            'feature_mean': feature_mean
        }
        
        print(f"    {feature_name}:")
        print(f"      æ ‡å‡†åŒ–æ¢¯åº¦: {np.mean(normalized_gradient):.6f}")
        print(f"      ç‰©ç†æ•æ„Ÿæ€§: {physical_sensitivity:.6f} kW/std")
    
    return sensitivities, analyzed_features, scaler

def diagnose_variance_calculation(data, errors, sensitivities, analyzed_features):
    """è¯Šæ–­æ–¹å·®è®¡ç®—"""
    print("\nğŸ” è¯Šæ–­æ–¹å·®è®¡ç®—...")
    
    feature_indices = [data['feature_names'].index(f) for f in analyzed_features]
    
    # è®¡ç®—è¾“å…¥è¯¯å·®
    ecmwf_input_errors = data['ecmwf_features'] - data['obs_features']
    gfs_input_errors = data['gfs_features'] - data['obs_features']
    
    # å®é™…ä¼ æ’­è¯¯å·®æ–¹å·®
    actual_vars = {
        'ecmwf': np.var(errors['ecmwf_propagation']),
        'gfs': np.var(errors['gfs_propagation'])
    }
    
    print(f"å®é™…ä¼ æ’­è¯¯å·®æ–¹å·®:")
    print(f"  ECMWF: {actual_vars['ecmwf']:.6f}")
    print(f"  GFS: {actual_vars['gfs']:.6f}")
    
    # æ–¹æ³•1ï¼šé”™è¯¯çš„æ–¹æ³•ï¼ˆå¯èƒ½å¯¼è‡´ä¹‹å‰çš„é—®é¢˜ï¼‰
    print(f"\næ–¹æ³•1ï¼šå¯èƒ½é”™è¯¯çš„è®¡ç®—ï¼ˆé‡å¤æ ‡å‡†åŒ–ï¼‰")
    
    wrong_theoretical_vars = {}
    for source, input_errors in [('ecmwf', ecmwf_input_errors), ('gfs', gfs_input_errors)]:
        theoretical_var = 0
        
        for i, feature_name in enumerate(analyzed_features):
            feature_idx = feature_indices[i]
            
            # é”™è¯¯æ–¹æ³•ï¼šä½¿ç”¨ç‰©ç†æ•æ„Ÿæ€§çš„å¹³æ–¹ï¼ˆå¯èƒ½é‡å¤è€ƒè™‘äº†æ ‡å‡†å·®ï¼‰
            phys_sens = sensitivities[feature_name]['physical_sensitivity']
            input_var = np.var(input_errors[:, feature_idx])
            contribution = phys_sens**2 * input_var
            theoretical_var += contribution
        
        wrong_theoretical_vars[source] = theoretical_var
        print(f"  {source.upper()} é”™è¯¯ç†è®ºæ–¹å·®: {theoretical_var:.6f}")
    
    # æ–¹æ³•2ï¼šæ­£ç¡®çš„æ–¹æ³•ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–æ¢¯åº¦ï¼‰
    print(f"\næ–¹æ³•2ï¼šæ­£ç¡®çš„è®¡ç®—ï¼ˆæ ‡å‡†åŒ–æ¢¯åº¦ï¼‰")
    
    correct_theoretical_vars = {}
    for source, input_errors in [('ecmwf', ecmwf_input_errors), ('gfs', gfs_input_errors)]:
        theoretical_var = 0
        
        print(f"  {source.upper()} è¯¦ç»†è®¡ç®—:")
        
        for i, feature_name in enumerate(analyzed_features):
            feature_idx = feature_indices[i]
            
            # æ­£ç¡®æ–¹æ³•ï¼šç›´æ¥ä½¿ç”¨æ ‡å‡†åŒ–æ¢¯åº¦çš„å¹³æ–¹
            norm_grad = sensitivities[feature_name]['normalized_gradient']
            input_var = np.var(input_errors[:, feature_idx])
            contribution = norm_grad**2 * input_var
            theoretical_var += contribution
            
            print(f"    {feature_name}:")
            print(f"      æ ‡å‡†åŒ–æ¢¯åº¦Â²: {norm_grad**2:.8f}")
            print(f"      è¾“å…¥æ–¹å·®: {input_var:.8f}")
            print(f"      è´¡çŒ®: {contribution:.8f}")
        
        correct_theoretical_vars[source] = theoretical_var
        print(f"    æ€»ç†è®ºæ–¹å·®: {theoretical_var:.6f}")
    
    # æ–¹æ³•3ï¼šè’™ç‰¹å¡æ´›éªŒè¯
    print(f"\næ–¹æ³•3ï¼šè’™ç‰¹å¡æ´›éªŒè¯")
    mc_vars = monte_carlo_variance_check(data, sensitivities, analyzed_features)
    
    # æ¯”è¾ƒæ‰€æœ‰æ–¹æ³•
    print(f"\nğŸ“Š æ–¹æ³•æ¯”è¾ƒ:")
    print(f"{'æ–¹æ³•':<25} {'ECMWFç†è®º':<15} {'ECMWFæ¯”å€¼':<10} {'GFSç†è®º':<15} {'GFSæ¯”å€¼':<10}")
    print("-" * 80)
    
    methods = [
        ("é”™è¯¯æ–¹æ³•ï¼ˆé‡å¤æ ‡å‡†åŒ–ï¼‰", wrong_theoretical_vars),
        ("æ­£ç¡®æ–¹æ³•ï¼ˆæ ‡å‡†åŒ–æ¢¯åº¦ï¼‰", correct_theoretical_vars),
        ("è’™ç‰¹å¡æ´›éªŒè¯", mc_vars)
    ]
    
    for method_name, theoretical_vars in methods:
        ecmwf_ratio = actual_vars['ecmwf'] / theoretical_vars['ecmwf']
        gfs_ratio = actual_vars['gfs'] / theoretical_vars['gfs']
        
        print(f"{method_name:<25} {theoretical_vars['ecmwf']:<15.6f} {ecmwf_ratio:<10.4f} {theoretical_vars['gfs']:<15.6f} {gfs_ratio:<10.4f}")
    
    print(f"{'å®é™…æ–¹å·®':<25} {actual_vars['ecmwf']:<15.6f} {'1.0000':<10} {actual_vars['gfs']:<15.6f} {'1.0000':<10}")
    
    # æ¨è
    print(f"\nğŸ’¡ åˆ†æç»“è®º:")
    
    correct_ecmwf_ratio = actual_vars['ecmwf'] / correct_theoretical_vars['ecmwf']
    correct_gfs_ratio = actual_vars['gfs'] / correct_theoretical_vars['gfs']
    
    if 0.5 <= correct_ecmwf_ratio <= 2.0 and 0.5 <= correct_gfs_ratio <= 2.0:
        print("âœ… ä½¿ç”¨æ ‡å‡†åŒ–æ¢¯åº¦çš„æ–¹æ³•ç»“æœåˆç†")
        print("âœ… é—®é¢˜ç¡®å®æ˜¯é‡å¤æ ‡å‡†åŒ–å¯¼è‡´çš„")
        print("âœ… å»ºè®®æ›´æ–°ä¸»ç¨‹åºä½¿ç”¨æ­£ç¡®çš„è®¡ç®—æ–¹æ³•")
    else:
        print("âš ï¸  å³ä½¿ä¿®æ­£åæ¯”å€¼ä»ä¸ç†æƒ³ï¼Œå¯èƒ½å­˜åœ¨å…¶ä»–é—®é¢˜:")
        print("   - æ¨¡å‹éçº¿æ€§ç¨‹åº¦è¾ƒé«˜")
        print("   - å˜é‡é—´å­˜åœ¨æ˜¾è‘—ç›¸å…³æ€§")
        print("   - éœ€è¦è€ƒè™‘é«˜é˜¶é¡¹æˆ–äº¤äº’é¡¹")
    
    return correct_theoretical_vars, actual_vars

def monte_carlo_variance_check(data, sensitivities, analyzed_features, n_samples=1000):
    """è’™ç‰¹å¡æ´›æ–¹å·®éªŒè¯"""
    print("  ğŸ² è’™ç‰¹å¡æ´›éªŒè¯...")
    
    feature_indices = [data['feature_names'].index(f) for f in analyzed_features]
    
    # é‡‡æ ·
    if len(data['obs_features']) > n_samples:
        sample_indices = np.random.choice(len(data['obs_features']), n_samples, replace=False)
        sample_obs = data['obs_features'][sample_indices]
        sample_ecmwf = data['ecmwf_features'][sample_indices]
        sample_gfs = data['gfs_features'][sample_indices]
    else:
        sample_obs = data['obs_features']
        sample_ecmwf = data['ecmwf_features']
        sample_gfs = data['gfs_features']
    
    mc_vars = {}
    
    for source, sample_data in [('ecmwf', sample_ecmwf), ('gfs', sample_gfs)]:
        # ä½¿ç”¨çº¿æ€§è¿‘ä¼¼è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„è¯¯å·®
        linear_errors = []
        
        for j in range(len(sample_obs)):
            linear_error = 0
            
            for i, feature_name in enumerate(analyzed_features):
                feature_idx = feature_indices[i]
                
                # è¾“å…¥å·®å¼‚
                input_diff = sample_data[j, feature_idx] - sample_obs[j, feature_idx]
                
                # ç‰©ç†æ•æ„Ÿæ€§
                sensitivity = sensitivities[feature_name]['physical_sensitivity']
                
                # çº¿æ€§è¿‘ä¼¼
                linear_error += sensitivity * input_diff
            
            linear_errors.append(linear_error)
        
        mc_var = np.var(linear_errors)
        mc_vars[source] = mc_var
        
        print(f"    {source.upper()} è’™ç‰¹å¡æ´›æ–¹å·®: {mc_var:.6f}")
    
    return mc_vars

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ ç‹¬ç«‹æ–¹å·®è®¡ç®—è¯Šæ–­å·¥å…·")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®
    data = load_analysis_data()
    if data is None:
        return
    
    # è¯¯å·®åˆ†è§£
    errors = perform_error_decomposition(data)
    
    # é‡æ–°è®¡ç®—æ•æ„Ÿæ€§
    sensitivities, analyzed_features, scaler = compute_corrected_sensitivity(data)
    
    # è¯Šæ–­æ–¹å·®è®¡ç®—
    correct_vars, actual_vars = diagnose_variance_calculation(data, errors, sensitivities, analyzed_features)
    
    # åˆ›å»ºä¿®æ­£æŠ¥å‘Š
    print(f"\nğŸ“‹ ä¿®æ­£åçš„æ–¹å·®åˆ†ææŠ¥å‘Š:")
    print("=" * 50)
    
    for source in ['ecmwf', 'gfs']:
        theoretical = correct_vars[source]
        actual = actual_vars[source]
        ratio = actual / theoretical
        
        print(f"{source.upper()}:")
        print(f"  ä¿®æ­£åç†è®ºæ–¹å·®: {theoretical:.6f}")
        print(f"  å®é™…æ–¹å·®: {actual:.6f}")
        print(f"  æ¯”å€¼ (å®é™…/ç†è®º): {ratio:.4f}")
        
        if 0.5 <= ratio <= 2.0:
            print(f"  çŠ¶æ€: âœ… è‰¯å¥½")
        elif ratio < 0.5:
            print(f"  çŠ¶æ€: âš ï¸ ç†è®ºåé«˜")
        else:
            print(f"  çŠ¶æ€: âš ï¸ ç†è®ºåä½")
        print()

if __name__ == "__main__":
    main()