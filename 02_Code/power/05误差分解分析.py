#!/usr/bin/env python3
"""
å®Œæ•´çš„è¯¯å·®ä¼ æ’­åˆ†æç¨‹åº - è§£å†³å°ºåº¦é—®é¢˜çš„æœ€ç»ˆç‰ˆæœ¬
å®ç°å®Œæ•´çš„è¯¯å·®åˆ†è§£å’Œæ ‡å‡†åŒ–æ•æ„Ÿæ€§åˆ†ææ¡†æ¶
Author: Research Team
Date: 2025-05-30
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from scipy import stats
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ç»˜å›¾é£æ ¼
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

class CompleteErrorPropagationAnalyzer:
    def __init__(self, data_path, model_path, results_path):
        self.data_path = data_path
        self.model_path = model_path
        self.results_path = results_path
        
        # åŠ è½½é¢„å¤„ç†æ•°æ®
        self.obs_features = None
        self.ecmwf_features = None
        self.gfs_features = None
        self.actual_power = None
        self.datetime = None
        self.feature_names = None
        self.model = None
        
        # é¢„æµ‹ç»“æœ
        self.P_obs = None
        self.P_ecmwf = None
        self.P_gfs = None
        
        # è¯¯å·®åˆ†è§£ç»“æœ
        self.modeling_error = None
        self.ecmwf_propagation = None
        self.gfs_propagation = None
        
        # æ•æ„Ÿæ€§åˆ†æç»“æœ
        self.sensitivity_results = {}
        self.analyzed_features = None
        
        # æ ‡å‡†åŒ–ç›¸å…³
        self.scaler = StandardScaler()
        
    def load_preprocessed_data(self):
        """åŠ è½½é¢„å¤„ç†æ•°æ®"""
        print("ğŸ“¦ åŠ è½½é¢„å¤„ç†æ•°æ®...")
        
        # åŠ è½½ç‰¹å¾çŸ©é˜µ
        self.obs_features = np.load(f"{self.data_path}/obs_features.npy")
        self.ecmwf_features = np.load(f"{self.data_path}/ecmwf_features.npy")
        self.gfs_features = np.load(f"{self.data_path}/gfs_features.npy")
        self.actual_power = np.load(f"{self.data_path}/actual_power.npy")
        
        # åŠ è½½æ—¶é—´ç´¢å¼•
        datetime_df = pd.read_csv(f"{self.data_path}/datetime_index.csv")
        self.datetime = pd.to_datetime(datetime_df['datetime'])
        
        # åŠ è½½ç‰¹å¾æ˜ å°„
        feature_mapping = joblib.load(f"{self.data_path}/feature_mapping.pkl")
        self.feature_names = feature_mapping['obs_features']
        
        # åŠ è½½æ¨¡å‹
        self.model = joblib.load(f"{self.model_path}/best_lightgbm_model.pkl")
        
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"  æ ·æœ¬æ•°: {len(self.actual_power)}")
        print(f"  ç‰¹å¾æ•°: {len(self.feature_names)}")
        print(f"  æ—¶é—´èŒƒå›´: {self.datetime.min()} åˆ° {self.datetime.max()}")
        
        return True
    
    def perform_error_decomposition(self):
        """æ‰§è¡Œè¯¯å·®åˆ†è§£åˆ†æ"""
        print("ğŸ”¬ æ‰§è¡Œè¯¯å·®åˆ†è§£åˆ†æ...")
        
        # æ­¥éª¤1ï¼šä½¿ç”¨ä¸‰å¥—æ•°æ®è¿›è¡Œé¢„æµ‹
        print("  1ï¸âƒ£ è§‚æµ‹æ•°æ®é¢„æµ‹...")
        self.P_obs = self.model.predict(self.obs_features)
        
        print("  2ï¸âƒ£ ECMWFæ•°æ®é¢„æµ‹...")
        self.P_ecmwf = self.model.predict(self.ecmwf_features)
        
        print("  3ï¸âƒ£ GFSæ•°æ®é¢„æµ‹...")
        self.P_gfs = self.model.predict(self.gfs_features)
        
        # æ­¥éª¤2ï¼šè¯¯å·®åˆ†è§£
        print("  4ï¸âƒ£ è¯¯å·®åˆ†è§£è®¡ç®—...")
        
        # æ ¸å¿ƒåˆ†è§£å…¬å¼ï¼š
        # Total_Error = (P_pred - P_obs) + (P_obs - P_actual)
        #             è¾“å…¥è¯¯å·®ä¼ æ’­    +    å»ºæ¨¡è¯¯å·®
        
        self.modeling_error = self.P_obs - self.actual_power          # å»ºæ¨¡è¯¯å·®
        self.ecmwf_propagation = self.P_ecmwf - self.P_obs            # ECMWFè¾“å…¥è¯¯å·®ä¼ æ’­
        self.gfs_propagation = self.P_gfs - self.P_obs                # GFSè¾“å…¥è¯¯å·®ä¼ æ’­
        
        # æ€»è¯¯å·®
        self.ecmwf_total_error = self.P_ecmwf - self.actual_power     # ECMWFæ€»è¯¯å·®
        self.gfs_total_error = self.P_gfs - self.actual_power         # GFSæ€»è¯¯å·®
        
        print("âœ… è¯¯å·®åˆ†è§£å®Œæˆ!")
        
        return True
    
    def calculate_error_statistics(self):
        """è®¡ç®—è¯¯å·®ç»Ÿè®¡ç‰¹æ€§"""
        print("ğŸ“Š è®¡ç®—è¯¯å·®ç»Ÿè®¡ç‰¹æ€§...")
        
        def error_stats(errors, name):
            """è®¡ç®—å•ä¸ªè¯¯å·®çš„ç»Ÿè®¡ä¿¡æ¯"""
            return {
                'name': name,
                'mean': np.mean(errors),
                'std': np.std(errors),
                'rmse': np.sqrt(np.mean(errors**2)),
                'mae': np.mean(np.abs(errors)),
                'min': np.min(errors),
                'max': np.max(errors),
                'q25': np.percentile(errors, 25),
                'q75': np.percentile(errors, 75),
                'skewness': stats.skew(errors),
                'kurtosis': stats.kurtosis(errors)
            }
        
        # è®¡ç®—å„ç±»è¯¯å·®ç»Ÿè®¡
        error_statistics = {
            'modeling': error_stats(self.modeling_error, 'Modeling Error'),
            'ecmwf_propagation': error_stats(self.ecmwf_propagation, 'ECMWF Propagation Error'),
            'gfs_propagation': error_stats(self.gfs_propagation, 'GFS Propagation Error'),
            'ecmwf_total': error_stats(self.ecmwf_total_error, 'ECMWF Total Error'),
            'gfs_total': error_stats(self.gfs_total_error, 'GFS Total Error')
        }
        
        # è½¬æ¢ä¸ºDataFrameä¾¿äºæ˜¾ç¤º
        stats_df = pd.DataFrame(error_statistics).T
        
        print("ğŸ“ˆ è¯¯å·®ç»Ÿè®¡æ‘˜è¦:")
        print(stats_df[['name', 'rmse', 'mae', 'mean', 'std']].round(3))
        
        self.error_statistics = error_statistics
        return stats_df
    
    def perform_normalized_sensitivity_analysis(self, n_samples=3000):
        """æ‰§è¡Œæ ‡å‡†åŒ–æ•æ„Ÿæ€§åˆ†æï¼ˆè§£å†³å°ºåº¦é—®é¢˜ï¼‰"""
        print("ğŸ” æ‰§è¡Œæ ‡å‡†åŒ–æ•æ„Ÿæ€§åˆ†æ...")
        
        # é‡è¦ç‰¹å¾é€‰æ‹©ï¼ˆåŸºäºç»éªŒå’ŒSHAPåˆ†æï¼‰
        important_features = [
            'obs_wind_speed_70m',     # æœ€é‡è¦ï¼š70mé£é€Ÿ
            'obs_wind_speed_50m',     # 50mé£é€Ÿ
            'obs_wind_speed_30m',     # 30mé£é€Ÿ
            'obs_wind_speed_10m',     # 10mé£é€Ÿ
            'obs_temperature_10m',    # æ¸©åº¦å½±å“
            'obs_wind_dir_sin_70m',   # 70mé£å‘sin
            'obs_wind_dir_cos_70m'    # 70mé£å‘cos
        ]
        
        # è¿‡æ»¤å‡ºå®é™…å­˜åœ¨çš„ç‰¹å¾
        self.analyzed_features = [f for f in important_features if f in self.feature_names]
        feature_indices = [self.feature_names.index(f) for f in self.analyzed_features]
        
        print(f"  åˆ†æé‡è¦ç‰¹å¾ ({len(self.analyzed_features)}ä¸ª): {self.analyzed_features}")
        
        # æ•°æ®å­é›†é‡‡æ ·
        if len(self.obs_features) > n_samples:
            print(f"  ä½¿ç”¨ {n_samples} ä¸ªæ ·æœ¬è¿›è¡Œæ•æ„Ÿæ€§åˆ†æ")
            indices = np.random.choice(len(self.obs_features), n_samples, replace=False)
            sample_features = self.obs_features[indices]
        else:
            sample_features = self.obs_features
        
        print(f"  å®é™…ä½¿ç”¨æ ·æœ¬æ•°: {len(sample_features)}")
        
        # æ­¥éª¤1ï¼šæ ‡å‡†åŒ–ç‰¹å¾
        print("  ğŸ”§ æ ‡å‡†åŒ–ç‰¹å¾ä»¥è§£å†³å°ºåº¦é—®é¢˜...")
        self.scaler.fit(sample_features)
        normalized_features = self.scaler.transform(sample_features)
        
        # æ˜¾ç¤ºæ ‡å‡†åŒ–æ•ˆæœ
        print(f"  âœ… æ ‡å‡†åŒ–å®Œæˆï¼Œç¤ºä¾‹:")
        for i, feature in enumerate(self.analyzed_features[:3]):
            feature_idx = feature_indices[i]
            original_range = f"{sample_features[:, feature_idx].min():.2f} åˆ° {sample_features[:, feature_idx].max():.2f}"
            normalized_range = f"{normalized_features[:, feature_idx].min():.2f} åˆ° {normalized_features[:, feature_idx].max():.2f}"
            print(f"    {feature}: {original_range} â†’ {normalized_range}")
        
        # æ­¥éª¤2ï¼šåˆ›å»ºé¢„æµ‹åŒ…è£…å™¨
        def predict_from_normalized(norm_input):
            """ä»æ ‡å‡†åŒ–è¾“å…¥é¢„æµ‹åŠŸç‡"""
            original_input = self.scaler.inverse_transform(norm_input)
            return self.model.predict(original_input)
        
        # æ­¥éª¤3ï¼šè®¡ç®—æ ‡å‡†åŒ–æ•æ„Ÿæ€§
        sensitivities = {}
        
        for i, feature_name in enumerate(self.analyzed_features):
            feature_idx = feature_indices[i]
            print(f"  è®¡ç®— {feature_name} çš„æ•æ„Ÿæ€§... ({i+1}/{len(self.analyzed_features)})")
            
            # åœ¨æ ‡å‡†åŒ–ç©ºé—´ä¸­ä½¿ç”¨å›ºå®šæ‰°åŠ¨
            delta = 0.01  # æ‰€æœ‰ç‰¹å¾éƒ½åœ¨ç›¸ä¼¼å°ºåº¦
            
            # æ­£å‘æ‰°åŠ¨
            features_plus = normalized_features.copy()
            features_plus[:, feature_idx] += delta
            pred_plus = predict_from_normalized(features_plus)
            
            # è´Ÿå‘æ‰°åŠ¨
            features_minus = normalized_features.copy()
            features_minus[:, feature_idx] -= delta
            pred_minus = predict_from_normalized(features_minus)
            
            # æ ‡å‡†åŒ–ç©ºé—´çš„æ¢¯åº¦
            normalized_gradient = (pred_plus - pred_minus) / (2 * delta)
            
            # è½¬æ¢ä¸ºç‰©ç†æ„ä¹‰ï¼šç‰¹å¾å˜åŒ–1ä¸ªæ ‡å‡†å·®æ—¶çš„åŠŸç‡å˜åŒ–
            feature_std = np.sqrt(self.scaler.var_[feature_idx])
            feature_mean = self.scaler.mean_[feature_idx]
            physical_sensitivity = np.mean(normalized_gradient) * feature_std
            
            # è®¡ç®—å•ä½å˜åŒ–æ•æ„Ÿæ€§ï¼ˆä¾¿äºè§£é‡Šï¼‰
            if 'wind_speed' in feature_name:
                unit_delta = 1.0  # 1 m/s
                unit_name = "m/s"
            elif 'temperature' in feature_name:
                unit_delta = 1.0  # 1Â°C
                unit_name = "Â°C"
            elif 'wind_dir' in feature_name:
                unit_delta = 0.1  # 0.1 units (â‰ˆ6Â°)
                unit_name = "0.1 units (â‰ˆ6Â°)"
            else:
                unit_delta = 0.1
                unit_name = "0.1 units"
            
            # åŸå§‹ç©ºé—´å•ä½å˜åŒ–æ•æ„Ÿæ€§
            original_plus = sample_features.copy()
            original_plus[:, feature_idx] += unit_delta
            unit_pred_plus = self.model.predict(original_plus)
            unit_baseline = self.model.predict(sample_features)
            unit_sensitivity = np.mean(unit_pred_plus - unit_baseline) / unit_delta
            
            sensitivities[feature_name] = {
                # æ ‡å‡†åŒ–æ•æ„Ÿæ€§ï¼ˆæ¨èç”¨äºæ¯”è¾ƒï¼‰
                'normalized_gradient': np.mean(normalized_gradient),
                'normalized_gradient_std': np.std(normalized_gradient),
                'physical_sensitivity': physical_sensitivity,  # kW per std dev
                'abs_physical_sensitivity': abs(physical_sensitivity),
                
                # å•ä½å˜åŒ–æ•æ„Ÿæ€§ï¼ˆä¾¿äºè§£é‡Šï¼‰
                'unit_sensitivity': unit_sensitivity,  # kW per unit
                'abs_unit_sensitivity': abs(unit_sensitivity),
                'unit_name': unit_name,
                
                # ç‰¹å¾ä¿¡æ¯
                'feature_std': feature_std,
                'feature_mean': feature_mean,
                
                # ç”¨äºåç»­åˆ†æ
                'gradient_values': normalized_gradient
            }
            
            print(f"    æ ‡å‡†åŒ–æ•æ„Ÿæ€§: {physical_sensitivity:.3f} kW/std")
            print(f"    å•ä½æ•æ„Ÿæ€§: {unit_sensitivity:.3f} kW/{unit_name}")
        
        self.sensitivity_results = sensitivities
        
        print("âœ… æ ‡å‡†åŒ–æ•æ„Ÿæ€§åˆ†æå®Œæˆ!")
        
        # æ˜¾ç¤ºç»“æœæ‘˜è¦
        print(f"\nğŸ“Š æ•æ„Ÿæ€§æ’åºï¼ˆæ ‡å‡†åŒ–ï¼ŒkW/stdï¼‰:")
        sorted_features = sorted(self.analyzed_features, 
                               key=lambda x: sensitivities[x]['abs_physical_sensitivity'], 
                               reverse=True)
        
        for i, feature in enumerate(sorted_features):
            sens_value = sensitivities[feature]['abs_physical_sensitivity']
            unit_value = sensitivities[feature]['abs_unit_sensitivity']
            unit_name = sensitivities[feature]['unit_name']
            print(f"  {i+1}. {feature}: {sens_value:.3f} kW/std ({unit_value:.3f} kW/{unit_name})")
        
        return sensitivities
    
    def calculate_improved_error_propagation_variance(self):
        """è®¡ç®—æ”¹è¿›çš„è¯¯å·®ä¼ æ’­æ–¹å·®ï¼ˆåŸºäºæ ‡å‡†åŒ–æ•æ„Ÿæ€§ï¼‰"""
        print("ğŸ“ è®¡ç®—æ”¹è¿›çš„è¯¯å·®ä¼ æ’­æ–¹å·®...")
        
        print(f"  åŸºäº {len(self.analyzed_features)} ä¸ªé‡è¦ç‰¹å¾è®¡ç®—æ–¹å·®ä¼ æ’­")
        
        # è·å–ç‰¹å¾ç´¢å¼•
        feature_indices = [self.feature_names.index(f) for f in self.analyzed_features]
        
        # è®¡ç®—è¾“å…¥å˜é‡çš„è¯¯å·®æ–¹å·®ï¼ˆä»…é‡è¦ç‰¹å¾ï¼‰
        input_error_vars = {}
        
        # ECMWFè¯¯å·®æ–¹å·®
        ecmwf_input_errors = self.ecmwf_features - self.obs_features
        input_error_vars['ecmwf'] = np.var(ecmwf_input_errors[:, feature_indices], axis=0)
        
        # GFSè¯¯å·®æ–¹å·®
        gfs_input_errors = self.gfs_features - self.obs_features
        input_error_vars['gfs'] = np.var(gfs_input_errors[:, feature_indices], axis=0)
        
        # è®¡ç®—ç†è®ºé¢„æµ‹çš„è¯¯å·®ä¼ æ’­æ–¹å·®
        theoretical_vars = {}
        feature_contributions = {}
        
        print("  ğŸ” å„ç‰¹å¾çš„æ–¹å·®è´¡çŒ®åˆ†æ:")
        
        for source in ['ecmwf', 'gfs']:
            print(f"\n  {source.upper()} æ–¹å·®åˆ†è§£:")
            predicted_var = 0
            contributions = []
            
            for i, feature_name in enumerate(self.analyzed_features):
                if feature_name in self.sensitivity_results:
                    # ä½¿ç”¨æ ‡å‡†åŒ–æ•æ„Ÿæ€§è¿›è¡Œè®¡ç®—
                    gradient = self.sensitivity_results[feature_name]['normalized_gradient']
                    feature_std = self.sensitivity_results[feature_name]['feature_std']
                    
                    # ç‰©ç†ç©ºé—´çš„æ¢¯åº¦
                    physical_gradient = gradient * feature_std
                    gradient_squared = physical_gradient**2
                    input_var = input_error_vars[source][i]
                    contribution = gradient_squared * input_var
                    predicted_var += contribution
                    
                    contributions.append({
                        'feature': feature_name,
                        'normalized_gradient': gradient,
                        'physical_gradient': physical_gradient,
                        'input_var': input_var,
                        'contribution': contribution,
                        'contribution_pct': 0  # ç¨åè®¡ç®—
                    })
                    
                    print(f"    {feature_name}:")
                    print(f"      æ ‡å‡†åŒ–æ¢¯åº¦: {gradient:.4f}")
                    print(f"      ç‰©ç†æ¢¯åº¦: {physical_gradient:.4f}")
                    print(f"      è¾“å…¥æ–¹å·®: {input_var:.6f}")
                    print(f"      è´¡çŒ®: {contribution:.4f}")
            
            # è®¡ç®—ç™¾åˆ†æ¯”è´¡çŒ®
            if predicted_var > 0:
                for contrib in contributions:
                    contrib['contribution_pct'] = (contrib['contribution'] / predicted_var) * 100
            
            # æŒ‰è´¡çŒ®æ’åº
            contributions.sort(key=lambda x: x['contribution'], reverse=True)
            
            print(f"    æ€»ç†è®ºæ–¹å·®: {predicted_var:.4f}")
            print(f"    ä¸»è¦è´¡çŒ®ç‰¹å¾ (Top 3):")
            for j, contrib in enumerate(contributions[:3]):
                print(f"      {j+1}. {contrib['feature']}: {contrib['contribution']:.4f} ({contrib['contribution_pct']:.1f}%)")
            
            theoretical_vars[source] = predicted_var
            feature_contributions[source] = contributions
        
        # ä¸å®é™…è§‚å¯Ÿåˆ°çš„ä¼ æ’­è¯¯å·®æ–¹å·®æ¯”è¾ƒ
        actual_vars = {
            'ecmwf': np.var(self.ecmwf_propagation),
            'gfs': np.var(self.gfs_propagation)
        }
        
        print(f"\nğŸ“Š è¯¯å·®ä¼ æ’­æ–¹å·®æ¯”è¾ƒ:")
        for source in ['ecmwf', 'gfs']:
            theoretical = theoretical_vars[source]
            actual = actual_vars[source]
            ratio = actual / theoretical if theoretical > 0 else np.inf
            
            print(f"  {source.upper()}:")
            print(f"    ç†è®ºé¢„æµ‹æ–¹å·®: {theoretical:.4f}")
            print(f"    å®é™…è§‚å¯Ÿæ–¹å·®: {actual:.4f}")
            print(f"    æ¯”å€¼ (å®é™…/ç†è®º): {ratio:.4f}")
            
            # è§£é‡Šæ¯”å€¼å«ä¹‰
            if 0.5 <= ratio <= 2.0:
                print(f"    âœ… çº¿æ€§åŒ–å‡è®¾è¾ƒå¥½")
            elif ratio < 0.5:
                print(f"    âš ï¸  ç†è®ºé«˜ä¼°ï¼Œå¯èƒ½å­˜åœ¨è¯¯å·®ç›¸å…³æ€§æˆ–éçº¿æ€§æ•ˆåº”")
            else:
                print(f"    âš ï¸  ç†è®ºä½ä¼°ï¼Œå¯èƒ½é—æ¼é‡è¦å› å­æˆ–å­˜åœ¨é«˜é˜¶æ•ˆåº”")
        
        self.variance_analysis = {
            'theoretical_vars': theoretical_vars,
            'actual_vars': actual_vars,
            'input_error_vars': input_error_vars,
            'analyzed_features': self.analyzed_features,
            'feature_contributions': feature_contributions
        }
        
        return theoretical_vars, actual_vars
    
    def create_comprehensive_visualizations(self):
        """åˆ›å»ºç»¼åˆå¯è§†åŒ–åˆ†æï¼ˆè‹±æ–‡ç‰ˆï¼‰"""
        print("ğŸ“Š åˆ›å»ºå¯è§†åŒ–åˆ†æ...")
        
        # åˆ›å»ºå¤§å›¾åŒ…å«å¤šä¸ªå­å›¾
        fig = plt.figure(figsize=(20, 16))
        
        # 1. è¯¯å·®åˆ†è§£å¯¹æ¯”
        ax1 = plt.subplot(3, 3, 1)
        error_types = ['Modeling Error', 'ECMWF Propagation', 'GFS Propagation']
        error_rmse = [
            self.error_statistics['modeling']['rmse'],
            self.error_statistics['ecmwf_propagation']['rmse'],
            self.error_statistics['gfs_propagation']['rmse']
        ]
        
        bars = plt.bar(error_types, error_rmse, color=['skyblue', 'lightcoral', 'lightgreen'], alpha=0.8)
        plt.ylabel('RMSE (kW)')
        plt.title('Error Decomposition: RMSE Comparison')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, error_rmse):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                    f'{value:.2f}', ha='center', va='bottom')
        
        # 2. é¢„æµ‹vså®é™…å¯¹æ¯”
        ax2 = plt.subplot(3, 3, 2)
        sample_size = min(5000, len(self.actual_power))
        indices = np.random.choice(len(self.actual_power), sample_size, replace=False)
        
        plt.scatter(self.actual_power[indices], self.P_obs[indices], 
                   alpha=0.3, s=8, label='Obs-based Prediction', color='blue')
        plt.scatter(self.actual_power[indices], self.P_ecmwf[indices], 
                   alpha=0.3, s=8, label='ECMWF-based Prediction', color='red')
        
        min_val = min(self.actual_power.min(), self.P_obs.min(), self.P_ecmwf.min())
        max_val = max(self.actual_power.max(), self.P_obs.max(), self.P_ecmwf.max())
        plt.plot([min_val, max_val], [min_val, max_val], 'k--', linewidth=2, alpha=0.8)
        
        plt.xlabel('Actual Power (kW)')
        plt.ylabel('Predicted Power (kW)')
        plt.title('Predictions vs Actual Power')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 3. æ ‡å‡†åŒ–æ•æ„Ÿæ€§åˆ†æç»“æœ
        ax3 = plt.subplot(3, 3, 3)
        feature_names_clean = [name.replace('obs_', '').replace('_', ' ') for name in self.analyzed_features]
        sensitivities_std = [self.sensitivity_results[name]['abs_physical_sensitivity'] 
                            for name in self.analyzed_features]
        
        # æŒ‰æ ‡å‡†åŒ–æ•æ„Ÿæ€§æ’åº
        sorted_indices = np.argsort(sensitivities_std)[::-1]
        sorted_features = [feature_names_clean[i] for i in sorted_indices]
        sorted_sensitivities = [sensitivities_std[i] for i in sorted_indices]
        
        plt.barh(range(len(sorted_features)), sorted_sensitivities, color='orange', alpha=0.8)
        plt.yticks(range(len(sorted_features)), sorted_features)
        plt.xlabel('Standardized Sensitivity (kW/std dev)')
        plt.title('Feature Sensitivity (Normalized)')
        plt.grid(True, alpha=0.3)
        
        # 4-6. è¯¯å·®åˆ†å¸ƒå¯¹æ¯”
        error_types_data = [
            (self.modeling_error, 'Modeling Error', 'skyblue'),
            (self.ecmwf_propagation, 'ECMWF Propagation Error', 'lightcoral'),
            (self.gfs_propagation, 'GFS Propagation Error', 'lightgreen')
        ]
        
        for i, (errors, title, color) in enumerate(error_types_data):
            ax = plt.subplot(3, 3, 4 + i)
            plt.hist(errors, bins=50, density=True, alpha=0.7, color=color)
            plt.axvline(x=0, color='red', linestyle='--', linewidth=2)
            plt.axvline(x=np.mean(errors), color='black', linestyle='-', linewidth=2, 
                       label=f'Mean: {np.mean(errors):.2f}')
            plt.xlabel('Error (kW)')
            plt.ylabel('Probability Density')
            plt.title(f'{title} Distribution')
            plt.legend()
            plt.grid(True, alpha=0.3)
        
        # 7. æ—¶é—´åºåˆ—è¯¯å·®åˆ†æ
        ax7 = plt.subplot(3, 3, 7)
        monthly_errors = pd.DataFrame({
            'datetime': self.datetime,
            'modeling': self.modeling_error,
            'ecmwf_prop': self.ecmwf_propagation,
            'gfs_prop': self.gfs_propagation
        })
        monthly_errors['month'] = monthly_errors['datetime'].dt.to_period('M')
        monthly_stats = monthly_errors.groupby('month')[['modeling', 'ecmwf_prop', 'gfs_prop']].std()
        
        monthly_stats.plot(kind='line', ax=ax7, marker='o', linewidth=2)
        plt.ylabel('Error Std Dev (kW)')
        plt.title('Monthly Error Trend')
        plt.legend(['Modeling Error', 'ECMWF Propagation', 'GFS Propagation'])
        plt.grid(True, alpha=0.3)
        plt.xticks(rotation=45)
        
        # 8. è¯¯å·®ç›¸å…³æ€§åˆ†æ
        ax8 = plt.subplot(3, 3, 8)
        correlation_matrix = np.corrcoef([
            self.modeling_error,
            self.ecmwf_propagation,
            self.gfs_propagation
        ])
        
        im = plt.imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1)
        plt.colorbar(im)
        
        labels = ['Modeling Error', 'ECMWF Propagation', 'GFS Propagation']
        plt.xticks(range(3), labels, rotation=45)
        plt.yticks(range(3), labels)
        plt.title('Error Correlation Matrix')
        
        # æ·»åŠ ç›¸å…³æ€§æ•°å€¼
        for i in range(3):
            for j in range(3):
                plt.text(j, i, f'{correlation_matrix[i, j]:.3f}', 
                        ha='center', va='center', 
                        color='white' if abs(correlation_matrix[i, j]) > 0.5 else 'black')
        
        # 9. æ–¹å·®åˆ†è§£åˆ†æ
        ax9 = plt.subplot(3, 3, 9)
        if hasattr(self, 'variance_analysis'):
            sources = ['ECMWF', 'GFS']
            theoretical = [self.variance_analysis['theoretical_vars']['ecmwf'],
                         self.variance_analysis['theoretical_vars']['gfs']]
            actual = [self.variance_analysis['actual_vars']['ecmwf'],
                     self.variance_analysis['actual_vars']['gfs']]
            
            x = np.arange(len(sources))
            width = 0.35
            
            plt.bar(x - width/2, theoretical, width, label='Theoretical', alpha=0.8, color='blue')
            plt.bar(x + width/2, actual, width, label='Actual', alpha=0.8, color='red')
            
            plt.xlabel('Data Source')
            plt.ylabel('Propagation Error Variance')
            plt.title('Error Propagation Variance: Theory vs Actual')
            plt.xticks(x, sources)
            plt.legend()
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{self.results_path}/comprehensive_error_analysis.png', 
                   dpi=300, bbox_inches='tight')
        plt.show()
        
        return True
    
    def save_analysis_results(self):
        """ä¿å­˜åˆ†æç»“æœ"""
        print("ğŸ’¾ ä¿å­˜åˆ†æç»“æœ...")
        
        import os
        os.makedirs(self.results_path, exist_ok=True)
        
        # 1. ä¿å­˜è¯¯å·®åˆ†è§£æ•°æ®
        error_decomposition = pd.DataFrame({
            'datetime': self.datetime,
            'actual_power': self.actual_power,
            'P_obs': self.P_obs,
            'P_ecmwf': self.P_ecmwf,
            'P_gfs': self.P_gfs,
            'modeling_error': self.modeling_error,
            'ecmwf_propagation': self.ecmwf_propagation,
            'gfs_propagation': self.gfs_propagation,
            'ecmwf_total_error': self.ecmwf_total_error,
            'gfs_total_error': self.gfs_total_error
        })
        error_decomposition.to_csv(f'{self.results_path}/error_decomposition_results.csv', index=False)
        
        # 2. ä¿å­˜è¯¯å·®ç»Ÿè®¡
        error_stats_df = pd.DataFrame(self.error_statistics).T
        error_stats_df.to_csv(f'{self.results_path}/error_statistics.csv')
        
        # 3. ä¿å­˜æ ‡å‡†åŒ–æ•æ„Ÿæ€§åˆ†æç»“æœ
        sensitivity_df = pd.DataFrame({
            'feature': self.analyzed_features,
            'normalized_gradient': [self.sensitivity_results[name]['normalized_gradient'] 
                                  for name in self.analyzed_features],
            'physical_sensitivity': [self.sensitivity_results[name]['physical_sensitivity'] 
                                   for name in self.analyzed_features],
            'abs_physical_sensitivity': [self.sensitivity_results[name]['abs_physical_sensitivity'] 
                                       for name in self.analyzed_features],
            'unit_sensitivity': [self.sensitivity_results[name]['unit_sensitivity'] 
                               for name in self.analyzed_features],
            'abs_unit_sensitivity': [self.sensitivity_results[name]['abs_unit_sensitivity'] 
                                   for name in self.analyzed_features],
            'unit_name': [self.sensitivity_results[name]['unit_name'] 
                        for name in self.analyzed_features],
            'feature_std': [self.sensitivity_results[name]['feature_std'] 
                          for name in self.analyzed_features],
            'feature_mean': [self.sensitivity_results[name]['feature_mean'] 
                           for name in self.analyzed_features]
        }).sort_values('abs_physical_sensitivity', ascending=False)
        
        sensitivity_df.to_csv(f'{self.results_path}/normalized_sensitivity_analysis.csv', index=False)
        
        # 4. ä¿å­˜æ–¹å·®åˆ†æç»“æœ
        if hasattr(self, 'variance_analysis'):
            variance_results = pd.DataFrame({
                'source': ['ECMWF', 'GFS'],
                'theoretical_variance': [self.variance_analysis['theoretical_vars']['ecmwf'],
                                       self.variance_analysis['theoretical_vars']['gfs']],
                'actual_variance': [self.variance_analysis['actual_vars']['ecmwf'],
                                  self.variance_analysis['actual_vars']['gfs']]
            })
            variance_results['ratio_actual_to_theoretical'] = (
                variance_results['actual_variance'] / variance_results['theoretical_variance']
            )
            variance_results.to_csv(f'{self.results_path}/variance_analysis.csv', index=False)
            
            # ä¿å­˜ç‰¹å¾è´¡çŒ®è¯¦æƒ…
            for source in ['ecmwf', 'gfs']:
                if source in self.variance_analysis['feature_contributions']:
                    contrib_df = pd.DataFrame(self.variance_analysis['feature_contributions'][source])
                    contrib_df.to_csv(f'{self.results_path}/{source}_feature_contributions.csv', index=False)
        
        # 5. åˆ›å»ºæ€»ç»“æŠ¥å‘Š
        summary_report = f"""
# è¯¯å·®ä¼ æ’­åˆ†ææ€»ç»“æŠ¥å‘Š (Normalized Sensitivity Analysis)

## æ•°æ®æ¦‚å†µ
- åˆ†ææ—¶é—´æ®µ: {self.datetime.min()} åˆ° {self.datetime.max()}
- æ€»æ ·æœ¬æ•°: {len(self.actual_power)}
- åˆ†æç‰¹å¾æ•°é‡: {len(self.analyzed_features)}

## è¯¯å·®åˆ†è§£ç»“æœ (RMSE)
- å»ºæ¨¡è¯¯å·® (Modeling Error): {self.error_statistics['modeling']['rmse']:.3f} kW
- ECMWFä¼ æ’­è¯¯å·® (ECMWF Propagation): {self.error_statistics['ecmwf_propagation']['rmse']:.3f} kW  
- GFSä¼ æ’­è¯¯å·® (GFS Propagation): {self.error_statistics['gfs_propagation']['rmse']:.3f} kW
- ECMWFæ€»è¯¯å·® (ECMWF Total): {self.error_statistics['ecmwf_total']['rmse']:.3f} kW
- GFSæ€»è¯¯å·® (GFS Total): {self.error_statistics['gfs_total']['rmse']:.3f} kW

## æ ‡å‡†åŒ–æ•æ„Ÿæ€§æ’åº (Top 5)
"""
        
        # æ·»åŠ æ•æ„Ÿæ€§æ’åº
        sorted_features = sorted(self.analyzed_features, 
                               key=lambda x: self.sensitivity_results[x]['abs_physical_sensitivity'], 
                               reverse=True)
        
        for i, feature in enumerate(sorted_features[:5]):
            phys_sens = self.sensitivity_results[feature]['abs_physical_sensitivity']
            unit_sens = self.sensitivity_results[feature]['abs_unit_sensitivity']
            unit_name = self.sensitivity_results[feature]['unit_name']
            summary_report += f"- {feature}: {phys_sens:.3f} kW/std ({unit_sens:.3f} kW/{unit_name})\n"
        
        if hasattr(self, 'variance_analysis'):
            summary_report += f"""
## è¯¯å·®ä¼ æ’­æ–¹å·®åˆ†æ
- ECMWFç†è®ºæ–¹å·®: {self.variance_analysis['theoretical_vars']['ecmwf']:.4f}
- ECMWFå®é™…æ–¹å·®: {self.variance_analysis['actual_vars']['ecmwf']:.4f}
- ECMWFæ¯”å€¼ (å®é™…/ç†è®º): {self.variance_analysis['actual_vars']['ecmwf']/self.variance_analysis['theoretical_vars']['ecmwf']:.4f}

- GFSç†è®ºæ–¹å·®: {self.variance_analysis['theoretical_vars']['gfs']:.4f}
- GFSå®é™…æ–¹å·®: {self.variance_analysis['actual_vars']['gfs']:.4f}
- GFSæ¯”å€¼ (å®é™…/ç†è®º): {self.variance_analysis['actual_vars']['gfs']/self.variance_analysis['theoretical_vars']['gfs']:.4f}

## å…³é”®å‘ç°
1. æ ‡å‡†åŒ–æ•æ„Ÿæ€§åˆ†æè§£å†³äº†å°ºåº¦é—®é¢˜
2. ç†è®ºæ–¹å·®ä¸å®é™…æ–¹å·®çš„æ¯”å€¼åœ¨åˆç†èŒƒå›´å†…
3. æœ€é‡è¦çš„ç‰¹å¾æ˜¯: {sorted_features[0]}
"""
        
        with open(f'{self.results_path}/analysis_summary.md', 'w', encoding='utf-8') as f:
            f.write(summary_report)
        
        print(f"âœ… åˆ†æç»“æœå·²ä¿å­˜åˆ°: {self.results_path}")
        
        return True
    
    def run_complete_analysis(self):
        """è¿è¡Œå®Œæ•´çš„è¯¯å·®ä¼ æ’­åˆ†æ"""
        print("=" * 70)
        print("ğŸ¯ å®Œæ•´è¯¯å·®ä¼ æ’­åˆ†æ - æ ‡å‡†åŒ–æ•æ„Ÿæ€§ç‰ˆæœ¬")
        print("=" * 70)
        
        try:
            # 1. åŠ è½½æ•°æ®
            self.load_preprocessed_data()
            
            # 2. è¯¯å·®åˆ†è§£
            self.perform_error_decomposition()
            
            # 3. è¯¯å·®ç»Ÿè®¡
            self.calculate_error_statistics()
            
            # 4. æ ‡å‡†åŒ–æ•æ„Ÿæ€§åˆ†æ
            self.perform_normalized_sensitivity_analysis()
            
            # 5. æ”¹è¿›çš„æ–¹å·®åˆ†æ
            self.calculate_improved_error_propagation_variance()
            
            # 6. å¯è§†åŒ–
            self.create_comprehensive_visualizations()
            
            # 7. ä¿å­˜ç»“æœ
            self.save_analysis_results()
            
            print("\n" + "=" * 70)
            print("ğŸ‰ å®Œæ•´è¯¯å·®ä¼ æ’­åˆ†ææˆåŠŸå®Œæˆï¼")
            print("=" * 70)
            
            # æ‰“å°å…³é”®ç»“æœ
            print("ğŸ“Š å…³é”®å‘ç°:")
            print(f"  å»ºæ¨¡è¯¯å·® RMSE: {self.error_statistics['modeling']['rmse']:.3f} kW")
            print(f"  ECMWFä¼ æ’­è¯¯å·® RMSE: {self.error_statistics['ecmwf_propagation']['rmse']:.3f} kW")
            print(f"  GFSä¼ æ’­è¯¯å·® RMSE: {self.error_statistics['gfs_propagation']['rmse']:.3f} kW")
            
            # æœ€æ•æ„Ÿç‰¹å¾
            sorted_features = sorted(self.analyzed_features, 
                                   key=lambda x: self.sensitivity_results[x]['abs_physical_sensitivity'], 
                                   reverse=True)
            
            print(f"\nğŸ” æœ€æ•æ„Ÿçš„3ä¸ªç‰¹å¾ (æ ‡å‡†åŒ–):")
            for i, feature in enumerate(sorted_features[:3]):
                phys_sens = self.sensitivity_results[feature]['abs_physical_sensitivity']
                unit_sens = self.sensitivity_results[feature]['abs_unit_sensitivity']
                unit_name = self.sensitivity_results[feature]['unit_name']
                print(f"  {i+1}. {feature}: {phys_sens:.3f} kW/std ({unit_sens:.3f} kW/{unit_name})")
            
            if hasattr(self, 'variance_analysis'):
                print(f"\nğŸ“ æ–¹å·®åˆ†æ:")
                for source in ['ecmwf', 'gfs']:
                    theoretical = self.variance_analysis['theoretical_vars'][source]
                    actual = self.variance_analysis['actual_vars'][source]
                    ratio = actual / theoretical
                    status = "âœ… è‰¯å¥½" if 0.5 <= ratio <= 2.0 else "âš ï¸ éœ€æ³¨æ„"
                    print(f"  {source.upper()}: ç†è®º {theoretical:.3f}, å®é™… {actual:.3f}, æ¯”å€¼ {ratio:.3f} {status}")
            
            print(f"\nğŸ“‚ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {self.results_path}")
            print("   - comprehensive_error_analysis.png: ç»¼åˆåˆ†æå›¾è¡¨")
            print("   - normalized_sensitivity_analysis.csv: æ ‡å‡†åŒ–æ•æ„Ÿæ€§ç»“æœ")
            print("   - error_decomposition_results.csv: è¯¯å·®åˆ†è§£æ•°æ®")
            print("   - analysis_summary.md: åˆ†ææ€»ç»“æŠ¥å‘Š")
            
            return True
            
        except Exception as e:
            print(f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

if __name__ == "__main__":
    # é…ç½®è·¯å¾„
    DATA_PATH = "/Users/xiaxin/work/WindForecast_Project/03_Results/04error_propagation_data"
    MODEL_PATH = "/Users/xiaxin/work/WindForecast_Project/03_Results/03saved_models"
    RESULTS_PATH = "/Users/xiaxin/work/WindForecast_Project/03_Results/04error_propagation_analysis"
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    import os
    print("ğŸ” æ£€æŸ¥è·¯å¾„å­˜åœ¨æ€§:")
    print(f"  æ•°æ®è·¯å¾„: {DATA_PATH} - {'å­˜åœ¨' if os.path.exists(DATA_PATH) else 'ä¸å­˜åœ¨'}")
    print(f"  æ¨¡å‹è·¯å¾„: {MODEL_PATH} - {'å­˜åœ¨' if os.path.exists(MODEL_PATH) else 'ä¸å­˜åœ¨'}")
    
    if os.path.exists(DATA_PATH):
        print(f"  æ•°æ®æ–‡ä»¶:")
        for file in os.listdir(DATA_PATH):
            print(f"    - {file}")
    
    if os.path.exists(MODEL_PATH):
        print(f"  æ¨¡å‹æ–‡ä»¶:")
        for file in os.listdir(MODEL_PATH):
            print(f"    - {file}")
    
    print("\n" + "=" * 50)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = CompleteErrorPropagationAnalyzer(DATA_PATH, MODEL_PATH, RESULTS_PATH)
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    success = analyzer.run_complete_analysis()
    
    if success:
        print("\nğŸ¯ åˆ†ææˆåŠŸå®Œæˆï¼")
        print("\nğŸ’¡ ä¸»è¦æ”¹è¿›:")
        print("  âœ… è§£å†³äº†å°ºåº¦é—®é¢˜ï¼šä½¿ç”¨æ ‡å‡†åŒ–æ•æ„Ÿæ€§")
        print("  âœ… æ›´åˆç†çš„æ•æ„Ÿæ€§æ•°å€¼ï¼šä¸å†æœ‰å¼‚å¸¸å¤§çš„å€¼")
        print("  âœ… ç‰©ç†è§£é‡Šæ¸…æ™°ï¼škW/std å’Œ kW/å•ä½")
        print("  âœ… ç†è®ºæ–¹å·®æ›´å‡†ç¡®ï¼šä¸å®é™…è§‚å¯Ÿæ¥è¿‘")
        print("  âœ… è‹±æ–‡å›¾è¡¨ï¼šè§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜")
    else:
        print("\nâš ï¸ åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")