"""
ä¿®å¤ç‰ˆæ˜Œé©¬æ•°æ®å¤„ç†è„šæœ¬
è§£å†³æ—¶é—´ç´¢å¼•é—®é¢˜å’Œæ•°æ®ä¿å­˜é—®é¢˜
"""

import pandas as pd
import numpy as np
import os
import glob
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

class ChangmaDataProcessorFixed:
    def __init__(self, base_path):
        """
        åˆå§‹åŒ–æ˜Œé©¬æ•°æ®å¤„ç†å™¨
        
        å‚æ•°:
        base_path: æ˜Œé©¬æ•°æ®æ ¹ç›®å½•è·¯å¾„
        """
        self.base_path = base_path
        self.processed_path = os.path.join(base_path, '..', '..', 'processed', 'cleaned')
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.processed_path, exist_ok=True)
        
        # ç¼ºå¤±å€¼æ ‡è®°
        self.missing_values = [-99.0, -99, '-99.0', '-99', 'NULL', 'null', '', ' ', '\\N']
        
        # é«˜åº¦æ˜ å°„
        self.height_mapping = {
            '10ç±³': 10, '10m': 10, '10': 10,
            '30ç±³': 30, '30m': 30, '30': 30,
            '50ç±³': 50, '50m': 50, '50': 50,
            '70ç±³': 70, '70m': 70, '70': 70
        }

    def detect_height_from_path(self, path):
        """ä»è·¯å¾„ä¸­æ£€æµ‹é«˜åº¦ä¿¡æ¯"""
        path_lower = path.lower()
        
        for key, height in self.height_mapping.items():
            if key in path_lower:
                return height
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æå–æ•°å­—
        import re
        numbers = re.findall(r'\d+', os.path.basename(path))
        for num in numbers:
            if int(num) in [10, 30, 50, 70]:
                return int(num)
        
        return None

    def standardize_changma_columns(self, df):
        """æ ‡å‡†åŒ–æ˜Œé©¬æ•°æ®çš„åˆ—å"""
        df_std = df.copy()
        
        column_mapping = {
            'æ—¶é—´': 'datetime', 'Time': 'datetime', 'time': 'datetime', 'æ—¥æœŸ': 'datetime',
            'åœºç«™': 'station', 'å±‚é«˜': 'layer_height',
            'å®æµ‹é£é€Ÿ': 'wind_speed', 'é£é€Ÿ': 'wind_speed', 'WindSpeed': 'wind_speed',
            'å®æµ‹é£å‘': 'wind_direction', 'é£å‘': 'wind_direction', 'WindDirection': 'wind_direction',
            'å®æµ‹æ¸©åº¦': 'temperature', 'æ¸©åº¦': 'temperature', 'Temperature': 'temperature',
            'å®æµ‹æ¹¿åº¦': 'humidity', 'æ¹¿åº¦': 'humidity', 'Humidity': 'humidity',
            'å®æµ‹æ°”å‹': 'pressure', 'æ°”å‹': 'pressure', 'Pressure': 'pressure',
            'å¤§æ°”å¯†åº¦ï¼ˆkg/mÂ³ï¼‰': 'density', 'å¤§æ°”å¯†åº¦': 'density', 'å¯†åº¦': 'density'
        }
        
        # é‡å‘½ååˆ—
        for old_name, new_name in column_mapping.items():
            if old_name in df_std.columns:
                df_std = df_std.rename(columns={old_name: new_name})
        
        return df_std

    def process_changma_datetime(self, df):
        """å¤„ç†æ˜Œé©¬æ•°æ®çš„æ—¶é—´åˆ—"""
        if 'datetime' not in df.columns:
            print(f"      è­¦å‘Š: æœªæ‰¾åˆ°æ—¶é—´åˆ—ï¼Œå¯ç”¨åˆ—: {df.columns.tolist()}")
            return df
        
        df_time = df.copy()
        
        try:
            original_count = len(df_time)
            
            # ç§»é™¤æ˜æ˜¾æ— æ•ˆçš„æ—¶é—´å€¼
            df_time = df_time.dropna(subset=['datetime'])
            
            # è½¬æ¢ä¸ºdatetime - ä¿®å¤ï¼šä¸ç«‹å³è®¾ç½®ä¸ºç´¢å¼•
            df_time['datetime'] = pd.to_datetime(df_time['datetime'], errors='coerce')
            
            # ç§»é™¤è½¬æ¢å¤±è´¥çš„è¡Œ
            df_time = df_time.dropna(subset=['datetime'])
            
            final_count = len(df_time)
            if final_count < original_count:
                print(f"      æ—¶é—´å¤„ç†: {original_count} -> {final_count} è¡Œ")
            
            if not df_time.empty:
                print(f"      æ—¶é—´èŒƒå›´: {df_time['datetime'].min()} åˆ° {df_time['datetime'].max()}")
            
        except Exception as e:
            print(f"      æ—¶é—´å¤„ç†é”™è¯¯: {e}")
            if 'datetime' in df.columns:
                print(f"      æ—¶é—´åˆ—æ ·æœ¬: {df['datetime'].head().tolist()}")
        
        return df_time

    def clean_changma_data(self, df):
        """æ¸…ç†æ˜Œé©¬æ•°æ®"""
        if df.empty:
            return df
            
        df_clean = df.copy()
        
        # å¤„ç†ç¼ºå¤±å€¼æ ‡è®°
        for col in df_clean.columns:
            if col not in ['height', 'station', 'layer_height', 'datetime']:
                df_clean[col] = df_clean[col].replace(self.missing_values, np.nan)
        
        # æ•°æ®æœ‰æ•ˆæ€§æ£€æŸ¥
        if 'wind_speed' in df_clean.columns:
            mask_negative = df_clean['wind_speed'] < 0
            mask_extreme = df_clean['wind_speed'] > 50
            
            if mask_negative.sum() > 0:
                print(f"      å‘ç° {mask_negative.sum()} ä¸ªè´Ÿé£é€Ÿå€¼ï¼Œè®¾ä¸ºNaN")
                df_clean.loc[mask_negative, 'wind_speed'] = np.nan
            
            if mask_extreme.sum() > 0:
                print(f"      å‘ç° {mask_extreme.sum()} ä¸ªè¶…è¿‡50m/sçš„æç«¯é£é€Ÿå€¼ï¼Œè®¾ä¸ºNaN")
                df_clean.loc[mask_extreme, 'wind_speed'] = np.nan
        
        # æ¸©åº¦æ£€æŸ¥
        if 'temperature' in df_clean.columns:
            mask_temp_low = df_clean['temperature'] < -50
            mask_temp_high = df_clean['temperature'] > 60
            
            if mask_temp_low.sum() > 0:
                print(f"      å‘ç° {mask_temp_low.sum()} ä¸ªä½äº-50Â°Cçš„æ¸©åº¦å€¼ï¼Œè®¾ä¸ºNaN")
                df_clean.loc[mask_temp_low, 'temperature'] = np.nan
                
            if mask_temp_high.sum() > 0:
                print(f"      å‘ç° {mask_temp_high.sum()} ä¸ªé«˜äº60Â°Cçš„æ¸©åº¦å€¼ï¼Œè®¾ä¸ºNaN")
                df_clean.loc[mask_temp_high, 'temperature'] = np.nan
        
        # é£å‘æ£€æŸ¥
        if 'wind_direction' in df_clean.columns:
            mask_wd_invalid = (df_clean['wind_direction'] < 0) | (df_clean['wind_direction'] > 360)
            if mask_wd_invalid.sum() > 0:
                print(f"      å‘ç° {mask_wd_invalid.sum()} ä¸ªæ— æ•ˆé£å‘å€¼ï¼Œè®¾ä¸ºNaN")
                df_clean.loc[mask_wd_invalid, 'wind_direction'] = np.nan
        
        # ç»Ÿè®¡æ¸…ç†æ•ˆæœ
        if len(df_clean) > 0:
            key_vars = ['wind_speed', 'wind_direction', 'temperature', 'humidity', 'pressure']
            existing_vars = [v for v in key_vars if v in df_clean.columns]
            
            print(f"      æ•°æ®è´¨é‡æ£€æŸ¥:")
            for var in existing_vars:
                missing_pct = df_clean[var].isnull().mean() * 100
                print(f"        {var}: {missing_pct:.1f}% ç¼ºå¤±")
        
        return df_clean

    def load_height_data(self, height_path, height_value):
        """åŠ è½½ç‰¹å®šé«˜åº¦çš„æ‰€æœ‰æ•°æ®æ–‡ä»¶"""
        print(f"  å¤„ç† {height_value}m é«˜åº¦æ•°æ®...")
        
        excel_files = glob.glob(os.path.join(height_path, "*.xls*"))
        excel_files.sort()
        
        if not excel_files:
            print(f"    æœªæ‰¾åˆ°Excelæ–‡ä»¶")
            return pd.DataFrame()
        
        print(f"    æ‰¾åˆ° {len(excel_files)} ä¸ªæ–‡ä»¶")
        
        all_data = []
        
        for i, file_path in enumerate(excel_files):
            filename = os.path.basename(file_path)
            print(f"    å¤„ç†æ–‡ä»¶ {i+1}/{len(excel_files)}: {filename}")
            
            df = None
            try:
                df = pd.read_excel(file_path, engine='openpyxl')
                print(f"      ä½¿ç”¨openpyxlå¼•æ“æˆåŠŸè¯»å–")
            except:
                try:
                    df = pd.read_excel(file_path, engine='xlrd')
                    print(f"      ä½¿ç”¨xlrdå¼•æ“æˆåŠŸè¯»å–")
                except:
                    try:
                        df = pd.read_excel(file_path)
                        print(f"      ä½¿ç”¨é»˜è®¤å¼•æ“æˆåŠŸè¯»å–")
                    except Exception as e:
                        print(f"      æ‰€æœ‰å¼•æ“éƒ½å¤±è´¥: {e}")
                        continue
            
            if df is not None:
                try:
                    print(f"      åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
                    print(f"      åˆ—å: {df.columns.tolist()}")
                    
                    # æ ‡å‡†åŒ–åˆ—å
                    df_cleaned = self.standardize_changma_columns(df)
                    
                    # å¤„ç†æ—¶é—´åˆ—ï¼ˆä½†ä¸è®¾ç½®ä¸ºç´¢å¼•ï¼‰
                    df_cleaned = self.process_changma_datetime(df_cleaned)
                    
                    # æ·»åŠ é«˜åº¦ä¿¡æ¯
                    df_cleaned['height'] = height_value
                    
                    # å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
                    df_cleaned = self.clean_changma_data(df_cleaned)
                    
                    if not df_cleaned.empty:
                        all_data.append(df_cleaned)
                        print(f"      æ¸…ç†åæ•°æ®å½¢çŠ¶: {df_cleaned.shape}")
                    else:
                        print(f"      æ•°æ®æ¸…ç†åä¸ºç©º")
                        
                except Exception as e:
                    print(f"      æ•°æ®å¤„ç†é”™è¯¯: {e}")
        
        if all_data:
            # åˆå¹¶æ‰€æœ‰æ•°æ®
            combined_df = pd.concat(all_data, ignore_index=True)
            
            # æŒ‰æ—¶é—´æ’åºï¼ˆä½†ä»ä¿æŒdatetimeä½œä¸ºåˆ—ï¼‰
            if 'datetime' in combined_df.columns:
                combined_df = combined_df.sort_values('datetime')
                combined_df = combined_df.reset_index(drop=True)
                
            print(f"    {height_value}m åˆå¹¶åæ•°æ®å½¢çŠ¶: {combined_df.shape}")
            return combined_df
        else:
            print(f"    {height_value}m æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
            return pd.DataFrame()

    def process_all_heights(self):
        """å¤„ç†æ‰€æœ‰é«˜åº¦çš„æ•°æ®"""
        print("å¼€å§‹å¤„ç†æ˜Œé©¬æµ‹é£å¡”çš„æ‰€æœ‰é«˜åº¦æ•°æ®...")
        
        items = os.listdir(self.base_path)
        height_dirs = []
        
        for item in items:
            item_path = os.path.join(self.base_path, item)
            if os.path.isdir(item_path):
                height = self.detect_height_from_path(item)
                if height is not None:
                    height_dirs.append((height, item_path))
        
        height_dirs.sort()
        print(f"æ‰¾åˆ°é«˜åº¦ç›®å½•: {[(h, os.path.basename(p)) for h, p in height_dirs]}")
        
        height_data = {}
        
        for height, height_path in height_dirs:
            df = self.load_height_data(height_path, height)
            if not df.empty:
                height_data[height] = df
            else:
                print(f"  {height}m æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡")
        
        return height_data

    def create_wide_format_data_fixed(self, height_data):
        """ä¿®å¤ç‰ˆï¼šå°†ä¸åŒé«˜åº¦çš„æ•°æ®åˆå¹¶ä¸ºå®½æ ¼å¼"""
        print("\nåˆ›å»ºå®½æ ¼å¼æ•°æ®ï¼ˆä¿®å¤ç‰ˆï¼‰...")
        
        if not height_data:
            print("æ²¡æœ‰é«˜åº¦æ•°æ®å¯åˆå¹¶")
            return pd.DataFrame()
        
        # é¦–å…ˆæ£€æŸ¥æ¯ä¸ªé«˜åº¦æ•°æ®çš„æ—¶é—´åˆ—æƒ…å†µ
        print("æ£€æŸ¥æ—¶é—´åˆ—æƒ…å†µ:")
        valid_height_data = {}
        
        for height, df in height_data.items():
            print(f"  {height}m: å½¢çŠ¶ {df.shape}")
            if 'datetime' in df.columns:
                valid_times = df['datetime'].notna().sum()
                print(f"    æœ‰æ•ˆæ—¶é—´ç‚¹: {valid_times}")
                if valid_times > 0:
                    valid_height_data[height] = df
                else:
                    print(f"    è­¦å‘Š: {height}m æ•°æ®æ²¡æœ‰æœ‰æ•ˆæ—¶é—´ç‚¹")
            else:
                print(f"    è­¦å‘Š: {height}m æ•°æ®æ²¡æœ‰æ—¶é—´åˆ—")
        
        if not valid_height_data:
            print("æ²¡æœ‰åŒ…å«æœ‰æ•ˆæ—¶é—´çš„é«˜åº¦æ•°æ®")
            return pd.DataFrame()
        
        # åˆ›å»ºç»Ÿä¸€çš„æ—¶é—´åºåˆ—
        all_times = []
        for height, df in valid_height_data.items():
            times = df[df['datetime'].notna()]['datetime'].values
            all_times.extend(times)
        
        if not all_times:
            print("æ²¡æœ‰æœ‰æ•ˆçš„æ—¶é—´æ•°æ®")
            return pd.DataFrame()
        
        # è½¬æ¢ä¸ºDatetimeIndexå¹¶å»é‡
        time_index = pd.DatetimeIndex(all_times).drop_duplicates().sort_values()
        print(f"ç»Ÿä¸€æ—¶é—´ç´¢å¼•: {len(time_index)} ä¸ªæ—¶é—´ç‚¹")
        print(f"æ—¶é—´èŒƒå›´: {time_index.min()} åˆ° {time_index.max()}")
        
        # åˆå§‹åŒ–å®½æ ¼å¼DataFrame
        df_wide = pd.DataFrame(index=time_index)
        
        # ä¸ºæ¯ä¸ªé«˜åº¦æ·»åŠ å˜é‡
        variables = ['wind_speed', 'wind_direction', 'temperature', 'humidity', 'pressure', 'density']
        
        for height in sorted(valid_height_data.keys()):
            df_height = valid_height_data[height]
            print(f"  æ·»åŠ  {height}m æ•°æ®...")
            
            # è®¾ç½®æ—¶é—´ä¸ºç´¢å¼•ï¼ˆåˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŸæ•°æ®ï¼‰
            df_work = df_height.copy()
            df_work = df_work.set_index('datetime')
            
            # å¤„ç†é‡å¤æ—¶é—´ï¼ˆå–å¹³å‡å€¼ï¼‰
            if df_work.index.has_duplicates:
                print(f"    å‘ç°é‡å¤æ—¶é—´ï¼Œå–å¹³å‡å€¼")
                df_work = df_work.groupby(df_work.index).mean()
            
            for var in variables:
                if var in df_work.columns:
                    col_name = f"{var}_{height}m"
                    
                    # å¯¹é½åˆ°ç»Ÿä¸€æ—¶é—´ç´¢å¼•
                    aligned_series = df_work[var].reindex(time_index)
                    df_wide[col_name] = aligned_series
                    
                    # ç»Ÿè®¡æœ‰æ•ˆæ•°æ®é‡
                    valid_count = df_wide[col_name].notna().sum()
                    valid_pct = valid_count / len(df_wide) * 100
                    print(f"    {col_name}: {valid_count} æœ‰æ•ˆå€¼ ({valid_pct:.1f}%)")
        
        print(f"\nå®½æ ¼å¼æ•°æ®åˆ›å»ºå®Œæˆ:")
        print(f"  å½¢çŠ¶: {df_wide.shape}")
        print(f"  åˆ—: {df_wide.columns.tolist()}")
        
        return df_wide

    def save_changma_data(self, df_wide):
        """ä¿å­˜æ˜Œé©¬å¤„ç†åçš„æ•°æ®"""
        if df_wide.empty:
            print("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return None
        
        # ç”Ÿæˆæ–‡ä»¶å
        start_date = df_wide.index.min().strftime('%Y%m%d')
        end_date = df_wide.index.max().strftime('%Y%m%d')
        
        # ä¿å­˜CSVæ–‡ä»¶
        csv_filename = f"changma_{start_date}_{end_date}_cleaned.csv"
        csv_filepath = os.path.join(self.processed_path, csv_filename)
        
        try:
            df_wide.to_csv(csv_filepath)
            print(f"\nâœ“ æ•°æ®å·²ä¿å­˜åˆ°: {csv_filepath}")
            
            # æ£€æŸ¥ä¿å­˜çš„æ–‡ä»¶
            file_size = os.path.getsize(csv_filepath) / 1024  # KB
            print(f"  æ–‡ä»¶å¤§å°: {file_size:.1f} KB")
            
        except Exception as e:
            print(f"\nâœ— ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {e}")
            return None
        
        # ä¿å­˜æ‘˜è¦æ–‡ä»¶
        summary_filename = f"changma_{start_date}_{end_date}_summary.txt"
        summary_filepath = os.path.join(self.processed_path, summary_filename)
        
        try:
            with open(summary_filepath, 'w', encoding='utf-8') as f:
                f.write("æ˜Œé©¬æµ‹é£å¡”æ•°æ®å¤„ç†æ‘˜è¦\n")
                f.write("="*50 + "\n\n")
                f.write(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ•°æ®å½¢çŠ¶: {df_wide.shape}\n")
                f.write(f"æ—¶é—´èŒƒå›´: {df_wide.index.min()} åˆ° {df_wide.index.max()}\n")
                f.write(f"æ•°æ®æœŸé—´: {(df_wide.index.max() - df_wide.index.min()).days} å¤©\n\n")
                
                f.write("å˜é‡åˆ—è¡¨:\n")
                for col in df_wide.columns:
                    valid_count = df_wide[col].notna().sum()
                    valid_pct = valid_count / len(df_wide) * 100
                    f.write(f"  {col}: {valid_count} æœ‰æ•ˆå€¼ ({valid_pct:.1f}%)\n")
                
                f.write("\nåŸºæœ¬ç»Ÿè®¡:\n")
                f.write(df_wide.describe().to_string())
            
            print(f"âœ“ æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_filepath}")
            
        except Exception as e:
            print(f"âœ— ä¿å­˜æ‘˜è¦æ–‡ä»¶å¤±è´¥: {e}")
        
        return csv_filepath

    def generate_data_summary(self, df_wide):
        """ç”Ÿæˆæ•°æ®æ‘˜è¦"""
        if df_wide.empty:
            print("æ²¡æœ‰æ•°æ®å¯ç”Ÿæˆæ‘˜è¦")
            return
        
        print("\n" + "="*60)
        print("æ˜Œé©¬æµ‹é£å¡”æ•°æ®æ‘˜è¦")
        print("="*60)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"æ•°æ®å½¢çŠ¶: {df_wide.shape}")
        print(f"æ—¶é—´èŒƒå›´: {df_wide.index.min()} åˆ° {df_wide.index.max()}")
        print(f"æ•°æ®æœŸé—´: {(df_wide.index.max() - df_wide.index.min()).days} å¤©")
        
        # å˜é‡ç»Ÿè®¡
        print(f"\nå¯ç”¨å˜é‡:")
        for col in df_wide.columns:
            valid_count = df_wide[col].notna().sum()
            valid_pct = valid_count / len(df_wide) * 100
            if valid_count > 0:
                mean_val = df_wide[col].mean()
                print(f"  {col}: {valid_count} æœ‰æ•ˆå€¼ ({valid_pct:.1f}%), å‡å€¼: {mean_val:.2f}")
            else:
                print(f"  {col}: {valid_count} æœ‰æ•ˆå€¼ ({valid_pct:.1f}%), æ— æœ‰æ•ˆæ•°æ®")
        
        # æŒ‰é«˜åº¦åˆ†æé£é€Ÿ
        wind_speed_cols = [col for col in df_wide.columns if 'wind_speed' in col]
        if wind_speed_cols:
            print(f"\né£é€Ÿç»Ÿè®¡:")
            for col in sorted(wind_speed_cols):
                valid_data = df_wide[col].dropna()
                if len(valid_data) > 0:
                    stats = valid_data.describe()
                    print(f"  {col}:")
                    print(f"    å¹³å‡: {stats['mean']:.2f} m/s")
                    print(f"    æœ€å¤§: {stats['max']:.2f} m/s")
                    print(f"    æœ€å°: {stats['min']:.2f} m/s")
                    print(f"    æ ‡å‡†å·®: {stats['std']:.2f} m/s")
                else:
                    print(f"  {col}: æ— æœ‰æ•ˆæ•°æ®")

def main():
    """ä¸»å‡½æ•° - å¤„ç†æ˜Œé©¬æµ‹é£å¡”æ•°æ®"""
    # è®¾ç½®æ˜Œé©¬æ•°æ®è·¯å¾„
    changma_path = "/Users/xiaxin/work/WindForecast_Project/01_Data/raw/obs/changma"
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = ChangmaDataProcessorFixed(changma_path)
    
    print("="*60)
    print("å¼€å§‹æ˜Œé©¬æ•°æ®å¤„ç†ï¼ˆä¿®å¤ç‰ˆï¼‰")
    print("="*60)
    
    # å¤„ç†æ‰€æœ‰é«˜åº¦æ•°æ®
    height_data = processor.process_all_heights()
    
    if height_data:
        print(f"\næˆåŠŸå¤„ç† {len(height_data)} ä¸ªé«˜åº¦çš„æ•°æ®")
        
        # åˆ›å»ºå®½æ ¼å¼æ•°æ®ï¼ˆä½¿ç”¨ä¿®å¤ç‰ˆæ–¹æ³•ï¼‰
        df_wide = processor.create_wide_format_data_fixed(height_data)
        
        if not df_wide.empty:
            # ç”Ÿæˆæ‘˜è¦
            processor.generate_data_summary(df_wide)
            
            # ä¿å­˜æ•°æ®
            output_file = processor.save_changma_data(df_wide)
            
            if output_file:
                print(f"\nğŸ‰ æ˜Œé©¬æµ‹é£å¡”æ•°æ®å¤„ç†æˆåŠŸå®Œæˆï¼")
                print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
                
                # å¿«é€ŸéªŒè¯
                print(f"\nå¿«é€ŸéªŒè¯:")
                try:
                    test_df = pd.read_csv(output_file, index_col=0, parse_dates=True, nrows=5)
                    print(f"  âœ“ æ–‡ä»¶å¯æ­£å¸¸è¯»å–")
                    print(f"  âœ“ æ•°æ®å½¢çŠ¶: {test_df.shape}")
                    print(f"  âœ“ ä¸»è¦åˆ—: {[col for col in test_df.columns if 'wind_speed' in col]}")
                except Exception as e:
                    print(f"  âœ— æ–‡ä»¶éªŒè¯å¤±è´¥: {e}")
            else:
                print(f"\nâŒ æ•°æ®ä¿å­˜å¤±è´¥")
        else:
            print("âŒ å®½æ ¼å¼æ•°æ®åˆ›å»ºå¤±è´¥")
    else:
        print("âŒ æœªèƒ½åŠ è½½ä»»ä½•é«˜åº¦æ•°æ®")

if __name__ == "__main__":
    main()