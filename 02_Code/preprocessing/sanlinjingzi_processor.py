"""
ä¸‰åé‡Œäº•å­é£ç”µåœºæ•°æ®å¤„ç†è„šæœ¬ï¼ˆä¿®å¤ç‰ˆï¼‰
åŸºäºæ˜Œé©¬æ•°æ®å¤„ç†è„šæœ¬ä¿®æ”¹ï¼Œå¤„ç†æ—¥æœŸæ—¶é—´åˆ†åˆ—çš„æ ¼å¼
"""

import pandas as pd
import numpy as np
import os
import glob
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

class SanlijijingziDataProcessor:
    def __init__(self, base_path):
        """
        åˆå§‹åŒ–ä¸‰åé‡Œäº•å­é£ç”µåœºæ•°æ®å¤„ç†å™¨
        
        å‚æ•°:
        base_path: ä¸‰åé‡Œäº•å­æ•°æ®æ ¹ç›®å½•è·¯å¾„
        """
        self.base_path = base_path
        self.processed_path = '/Users/xiaxin/work/WindForecast_Project/01_Data/raw/processed/cleaned'
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.processed_path, exist_ok=True)
        
        # ç¼ºå¤±å€¼æ ‡è®°
        self.missing_values = [-99.0, -99, '-99.0', '-99', 'NULL', 'null', '', ' ', '\\N']
        
        # é«˜åº¦æ˜ å°„
        self.height_mapping = {
            '10ç±³': 10, '10m': 10, '10': 10,
            '30ç±³': 30, '30m': 30, '30': 30,
            '50ç±³': 50, '50m': 50, '50': 50,
            '70ç±³': 70, '70m': 70, '70': 70,
            '100ç±³': 100, '100m': 100, '100': 100
        }

    def detect_height_from_path(self, path):
        """ä»è·¯å¾„ä¸­æ£€æµ‹é«˜åº¦ä¿¡æ¯"""
        path_lower = path.lower()
        
        for key, height in self.height_mapping.items():
            if key in path_lower:
                return height
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æå–æ•°å­—
        import re
        numbers = re.findall(r'\d+', os.path.basename(path))
        for num in numbers:
            if int(num) in [10, 30, 50, 70, 100]:
                return int(num)
        
        return None

    def standardize_sanlijijingzi_columns(self, df):
        """æ ‡å‡†åŒ–ä¸‰åé‡Œäº•å­æ•°æ®çš„åˆ—å"""
        df_std = df.copy()
        
        # ä¸‰åé‡Œäº•å­çš„ç‰¹å®šåˆ—åæ˜ å°„
        column_mapping = {
            'æ—¥æœŸ': 'date',
            'æ—¶é—´': 'time', 
            'é£é€Ÿæœ€å¤§å€¼(m/s)': 'wind_speed_max',
            'é£é€Ÿæœ€å°å€¼(m/s)': 'wind_speed_min',
            'é£é€Ÿå¹³å‡å€¼(m/s)': 'wind_speed',
            'é£é€Ÿæ ‡å‡†åå·®(m/s)': 'wind_speed_std',
            'ç¬æ—¶é£é€Ÿ(m/s)': 'wind_speed_instant',
            'å¹³å‡é£å‘(Â°)': 'wind_direction',
            'ç¬æ—¶é£å‘(Â°)': 'wind_direction_instant',
            # å¯èƒ½çš„å…¶ä»–å˜ä½“
            'å®æµ‹é£é€Ÿ': 'wind_speed', 
            'é£é€Ÿ': 'wind_speed', 
            'å®æµ‹é£å‘': 'wind_direction', 
            'é£å‘': 'wind_direction',
            'å®æµ‹æ¸©åº¦': 'temperature', 
            'æ¸©åº¦': 'temperature',
            'å®æµ‹æ¹¿åº¦': 'humidity', 
            'æ¹¿åº¦': 'humidity',
            'å®æµ‹æ°”å‹': 'pressure', 
            'æ°”å‹': 'pressure',
            'å¤§æ°”å¯†åº¦': 'density', 
            'å¯†åº¦': 'density'
        }
        
        # é‡å‘½ååˆ—
        for old_name, new_name in column_mapping.items():
            if old_name in df_std.columns:
                df_std = df_std.rename(columns={old_name: new_name})
        
        return df_std

    def process_sanlijijingzi_datetime(self, df):
        """å¤„ç†ä¸‰åé‡Œäº•å­æ•°æ®çš„æ—¶é—´åˆ—ï¼ˆæ—¥æœŸå’Œæ—¶é—´åˆ†åˆ—ï¼‰"""
        if 'date' not in df.columns or 'time' not in df.columns:
            print(f"      è­¦å‘Š: æœªæ‰¾åˆ°æ—¥æœŸæ—¶é—´åˆ—ï¼Œå¯ç”¨åˆ—: {list(df.columns)}")
            return df
        
        df_time = df.copy()
        
        try:
            original_count = len(df_time)
            
            # ç§»é™¤æ˜æ˜¾æ— æ•ˆçš„æ—¶é—´å€¼
            df_time = df_time.dropna(subset=['date', 'time'])
            
            # åˆå¹¶æ—¥æœŸå’Œæ—¶é—´åˆ—ï¼Œåˆ›å»ºdatetimeåˆ—
            # å°†æ—¥æœŸå’Œæ—¶é—´è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œç„¶ååˆå¹¶
            df_time['date_str'] = df_time['date'].astype(str)
            df_time['time_str'] = df_time['time'].astype(str)
            
            # åˆå¹¶æ—¥æœŸå’Œæ—¶é—´
            df_time['datetime_str'] = df_time['date_str'] + ' ' + df_time['time_str']
            
            # è½¬æ¢ä¸ºdatetime
            df_time['datetime'] = pd.to_datetime(df_time['datetime_str'], errors='coerce')
            
            # ç§»é™¤è½¬æ¢å¤±è´¥çš„è¡Œ
            df_time = df_time.dropna(subset=['datetime'])
            
            # åˆ é™¤ä¸´æ—¶åˆ—
            df_time = df_time.drop(['date_str', 'time_str', 'datetime_str'], axis=1)
            
            final_count = len(df_time)
            if final_count < original_count:
                print(f"      æ—¶é—´å¤„ç†: {original_count} -> {final_count} è¡Œ")
            
            if not df_time.empty:
                print(f"      æ—¶é—´èŒƒå›´: {df_time['datetime'].min()} åˆ° {df_time['datetime'].max()}")
            
        except Exception as e:
            print(f"      æ—¶é—´å¤„ç†é”™è¯¯: {e}")
            if 'date' in df.columns and 'time' in df.columns:
                print(f"      æ—¥æœŸåˆ—æ ·æœ¬: {df['date'].head().tolist()}")
                print(f"      æ—¶é—´åˆ—æ ·æœ¬: {df['time'].head().tolist()}")
        
        return df_time

    def detect_stuck_values(self, df, min_consecutive=5):
        """
        æ£€æµ‹å’Œæ¸…ç†åƒµå€¼ï¼ˆè¿ç»­ç›¸åŒçš„æ•°å€¼ï¼‰
        
        å‚æ•°:
        df: æ•°æ®æ¡†
        min_consecutive: è®¤ä¸ºæ˜¯åƒµå€¼çš„æœ€å°è¿ç»­ç›¸åŒå€¼æ•°é‡ï¼Œé»˜è®¤ä¸º5
        
        è¿”å›:
        æ¸…ç†åçš„æ•°æ®æ¡†
        """
        df_cleaned = df.copy()
        
        # éœ€è¦æ£€æŸ¥åƒµå€¼çš„æ•°å€¼åˆ—
        numeric_columns = df_cleaned.select_dtypes(include=[np.number]).columns
        # æ’é™¤ä¸éœ€è¦æ£€æŸ¥çš„åˆ—
        exclude_cols = ['height']
        check_columns = [col for col in numeric_columns if col not in exclude_cols]
        
        total_stuck_removed = 0
        
        for col in check_columns:
            if col in df_cleaned.columns and df_cleaned[col].notna().sum() > 0:
                # åˆ›å»ºä¸€ä¸ªç”¨äºæ ‡è®°è¿ç»­ç›¸åŒå€¼çš„ç³»åˆ—
                series = df_cleaned[col].copy()
                
                # æ‰¾å‡ºéNaNå€¼çš„ä½ç½®
                non_nan_mask = series.notna()
                
                if non_nan_mask.sum() < min_consecutive:
                    continue  # å¦‚æœæœ‰æ•ˆæ•°æ®ä¸è¶³min_consecutiveä¸ªï¼Œè·³è¿‡
                
                # æ£€æµ‹è¿ç»­ç›¸åŒå€¼
                # æ–¹æ³•ï¼šæ¯”è¾ƒå½“å‰å€¼ä¸å‰ä¸€ä¸ªå€¼æ˜¯å¦ç›¸åŒ
                is_same_as_previous = series == series.shift(1)
                
                # åˆ›å»ºç»„IDï¼šæ¯å½“å€¼å‘ç”Ÿå˜åŒ–æ—¶ï¼Œç»„IDå°±å¢åŠ 
                group_id = (~is_same_as_previous).cumsum()
                
                # å¯¹æ¯ä¸ªç»„è®¡ç®—å¤§å°
                group_sizes = series.groupby(group_id).transform('size')
                
                # æ ‡è®°è¿ç»­ç›¸åŒå€¼>=min_consecutiveçš„ä½ç½®ä¸ºåƒµå€¼
                stuck_mask = (group_sizes >= min_consecutive) & non_nan_mask
                
                if stuck_mask.sum() > 0:
                    # å°†åƒµå€¼è®¾ä¸ºNaN
                    df_cleaned.loc[stuck_mask, col] = np.nan
                    stuck_count = stuck_mask.sum()
                    total_stuck_removed += stuck_count
                    print(f"      {col}: æ£€æµ‹åˆ° {stuck_count} ä¸ªåƒµå€¼ï¼Œå·²è®¾ä¸ºNaN")
        
        if total_stuck_removed > 0:
            print(f"      åƒµå€¼æ£€æµ‹: æ€»è®¡æ¸…ç† {total_stuck_removed} ä¸ªåƒµå€¼")
        else:
            print(f"      åƒµå€¼æ£€æµ‹: æœªå‘ç°åƒµå€¼")
            
        return df_cleaned

    def clean_sanlijijingzi_data(self, df):
        """æ¸…ç†ä¸‰åé‡Œäº•å­æ•°æ®"""
        if df.empty:
            return df
            
        df_clean = df.copy()
        
        # å¤„ç†ç¼ºå¤±å€¼æ ‡è®°
        for col in df_clean.columns:
            if col not in ['height', 'date', 'time', 'datetime']:
                df_clean[col] = df_clean[col].replace(self.missing_values, np.nan)
        
        # æ•°æ®æœ‰æ•ˆæ€§æ£€æŸ¥ - æ£€æŸ¥æ‰€æœ‰é£é€Ÿç›¸å…³å˜é‡
        wind_speed_vars = ['wind_speed', 'wind_speed_max', 'wind_speed_min', 'wind_speed_instant']
        for var in wind_speed_vars:
            if var in df_clean.columns:
                mask_negative = df_clean[var] < 0
                mask_extreme = df_clean[var] > 50
                
                if mask_negative.sum() > 0:
                    print(f"      å‘ç° {mask_negative.sum()} ä¸ªè´Ÿ{var}å€¼ï¼Œè®¾ä¸ºNaN")
                    df_clean.loc[mask_negative, var] = np.nan
                
                if mask_extreme.sum() > 0:
                    print(f"      å‘ç° {mask_extreme.sum()} ä¸ªè¶…è¿‡50m/sçš„æç«¯{var}å€¼ï¼Œè®¾ä¸ºNaN")
                    df_clean.loc[mask_extreme, var] = np.nan
        
        # æ¸©åº¦æ£€æŸ¥
        if 'temperature' in df_clean.columns:
            mask_temp_low = df_clean['temperature'] < -50
            mask_temp_high = df_clean['temperature'] > 60
            
            if mask_temp_low.sum() > 0:
                print(f"      å‘ç° {mask_temp_low.sum()} ä¸ªä½äº-50Â°Cçš„æ¸©åº¦å€¼ï¼Œè®¾ä¸ºNaN")
                df_clean.loc[mask_temp_low, 'temperature'] = np.nan
                
            if mask_temp_high.sum() > 0:
                print(f"      å‘ç° {mask_temp_high.sum()} ä¸ªé«˜äº60Â°Cçš„æ¸©åº¦å€¼ï¼Œè®¾ä¸ºNaN")
                df_clean.loc[mask_temp_high, 'temperature'] = np.nan
        
        # é£å‘æ£€æŸ¥
        wind_direction_vars = ['wind_direction', 'wind_direction_instant']
        for var in wind_direction_vars:
            if var in df_clean.columns:
                mask_wd_invalid = (df_clean[var] < 0) | (df_clean[var] > 360)
                if mask_wd_invalid.sum() > 0:
                    print(f"      å‘ç° {mask_wd_invalid.sum()} ä¸ªæ— æ•ˆ{var}å€¼ï¼Œè®¾ä¸ºNaN")
                    df_clean.loc[mask_wd_invalid, var] = np.nan
        
        # åƒµå€¼æ£€æµ‹å’Œæ¸…ç†
        print(f"      æ‰§è¡Œåƒµå€¼æ£€æµ‹ï¼ˆè¿ç»­5ä¸ªç›¸åŒå€¼ï¼‰...")
        df_clean = self.detect_stuck_values(df_clean, min_consecutive=5)
        
        # ç»Ÿè®¡æ¸…ç†æ•ˆæœ
        if len(df_clean) > 0:
            key_vars = ['wind_speed', 'wind_speed_max', 'wind_speed_min', 'wind_speed_instant', 
                       'wind_direction', 'wind_direction_instant', 'temperature', 'humidity', 'pressure']
            existing_vars = [v for v in key_vars if v in df_clean.columns]
            
            print(f"      æ•°æ®è´¨é‡æ£€æŸ¥:")
            for var in existing_vars:
                missing_pct = df_clean[var].isnull().mean() * 100
                print(f"        {var}: {missing_pct:.1f}% ç¼ºå¤±")
        
        return df_clean

    def load_height_data(self, height_path, height_value):
        """åŠ è½½ç‰¹å®šé«˜åº¦çš„æ‰€æœ‰æ•°æ®æ–‡ä»¶"""
        print(f"  å¤„ç† {height_value}m é«˜åº¦æ•°æ®...")
        
        excel_files = glob.glob(os.path.join(height_path, "*.xls*"))
        excel_files.sort()
        
        if not excel_files:
            print(f"    æœªæ‰¾åˆ°Excelæ–‡ä»¶")
            return pd.DataFrame()
        
        print(f"    æ‰¾åˆ° {len(excel_files)} ä¸ªæ–‡ä»¶")
        
        all_data = []
        
        for i, file_path in enumerate(excel_files):
            filename = os.path.basename(file_path)
            print(f"    å¤„ç†æ–‡ä»¶ {i+1}/{len(excel_files)}: {filename}")
            
            # è·³è¿‡15åˆ†é’ŸæŠ¥è¡¨æ–‡ä»¶
            if '15åˆ†é’ŸæŠ¥è¡¨' in filename:
                print(f"      è·³è¿‡15åˆ†é’ŸæŠ¥è¡¨æ–‡ä»¶ï¼Œä¿æŒæ•°æ®æ ¼å¼ä¸€è‡´")
                continue
            
            df = None
            try:
                df = pd.read_excel(file_path, engine='openpyxl')
                print(f"      ä½¿ç”¨openpyxlå¼•æ“æˆåŠŸè¯»å–")
            except:
                try:
                    df = pd.read_excel(file_path, engine='xlrd')
                    print(f"      ä½¿ç”¨xlrdå¼•æ“æˆåŠŸè¯»å–")
                except:
                    try:
                        df = pd.read_excel(file_path)
                        print(f"      ä½¿ç”¨é»˜è®¤å¼•æ“æˆåŠŸè¯»å–")
                    except Exception as e:
                        print(f"      æ‰€æœ‰å¼•æ“éƒ½å¤±è´¥: {e}")
                        continue
            
            if df is not None:
                try:
                    print(f"      åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
                    print(f"      åˆ—å: {list(df.columns)}")
                    
                    # æ ‡å‡†åŒ–åˆ—å
                    df_cleaned = self.standardize_sanlijijingzi_columns(df)
                    
                    # å¤„ç†æ—¶é—´åˆ—
                    df_cleaned = self.process_sanlijijingzi_datetime(df_cleaned)
                    
                    # æ·»åŠ é«˜åº¦ä¿¡æ¯
                    df_cleaned['height'] = height_value
                    
                    # å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
                    df_cleaned = self.clean_sanlijijingzi_data(df_cleaned)
                    
                    if not df_cleaned.empty:
                        all_data.append(df_cleaned)
                        print(f"      æ¸…ç†åæ•°æ®å½¢çŠ¶: {df_cleaned.shape}")
                    else:
                        print(f"      æ•°æ®æ¸…ç†åä¸ºç©º")
                        
                except Exception as e:
                    print(f"      æ•°æ®å¤„ç†é”™è¯¯: {e}")
                    import traceback
                    traceback.print_exc()
        
        if all_data:
            # åˆå¹¶æ‰€æœ‰æ•°æ®
            combined_df = pd.concat(all_data, ignore_index=True)
            
            # æŒ‰æ—¶é—´æ’åº
            if 'datetime' in combined_df.columns:
                combined_df = combined_df.sort_values('datetime')
                combined_df = combined_df.reset_index(drop=True)
                
            print(f"    {height_value}m åˆå¹¶åæ•°æ®å½¢çŠ¶: {combined_df.shape}")
            return combined_df
        else:
            print(f"    {height_value}m æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
            return pd.DataFrame()

    def process_all_heights(self):
        """å¤„ç†æ‰€æœ‰é«˜åº¦çš„æ•°æ®"""
        print("å¼€å§‹å¤„ç†ä¸‰åé‡Œäº•å­é£ç”µåœºçš„æ‰€æœ‰é«˜åº¦æ•°æ®...")
        
        items = os.listdir(self.base_path)
        height_dirs = []
        
        for item in items:
            item_path = os.path.join(self.base_path, item)
            if os.path.isdir(item_path):
                height = self.detect_height_from_path(item)
                if height is not None:
                    height_dirs.append((height, item_path))
        
        height_dirs.sort()
        print(f"æ‰¾åˆ°é«˜åº¦ç›®å½•: {[(h, os.path.basename(p)) for h, p in height_dirs]}")
        
        height_data = {}
        
        for height, height_path in height_dirs:
            df = self.load_height_data(height_path, height)
            if not df.empty:
                height_data[height] = df
            else:
                print(f"  {height}m æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡")
        
        return height_data

    def create_wide_format_data(self, height_data):
        """å°†ä¸åŒé«˜åº¦çš„æ•°æ®åˆå¹¶ä¸ºå®½æ ¼å¼"""
        print("\nåˆ›å»ºå®½æ ¼å¼æ•°æ®...")
        
        if not height_data:
            print("æ²¡æœ‰é«˜åº¦æ•°æ®å¯åˆå¹¶")
            return pd.DataFrame()
        
        # é¦–å…ˆæ£€æŸ¥æ¯ä¸ªé«˜åº¦æ•°æ®çš„æ—¶é—´åˆ—æƒ…å†µ
        print("æ£€æŸ¥æ—¶é—´åˆ—æƒ…å†µ:")
        valid_height_data = {}
        
        for height, df in height_data.items():
            print(f"  {height}m: å½¢çŠ¶ {df.shape}")
            if 'datetime' in df.columns:
                valid_times = df['datetime'].notna().sum()
                print(f"    æœ‰æ•ˆæ—¶é—´ç‚¹: {valid_times}")
                if valid_times > 0:
                    valid_height_data[height] = df
                else:
                    print(f"    è­¦å‘Š: {height}m æ•°æ®æ²¡æœ‰æœ‰æ•ˆæ—¶é—´ç‚¹")
            else:
                print(f"    è­¦å‘Š: {height}m æ•°æ®æ²¡æœ‰æ—¶é—´åˆ—")
        
        if not valid_height_data:
            print("æ²¡æœ‰åŒ…å«æœ‰æ•ˆæ—¶é—´çš„é«˜åº¦æ•°æ®")
            return pd.DataFrame()
        
        # åˆ›å»ºç»Ÿä¸€çš„æ—¶é—´åºåˆ—
        all_times = []
        for height, df in valid_height_data.items():
            times = df[df['datetime'].notna()]['datetime'].values
            all_times.extend(times)
        
        if not all_times:
            print("æ²¡æœ‰æœ‰æ•ˆçš„æ—¶é—´æ•°æ®")
            return pd.DataFrame()
        
        # è½¬æ¢ä¸ºDatetimeIndexå¹¶å»é‡
        time_index = pd.DatetimeIndex(all_times).drop_duplicates().sort_values()
        print(f"ç»Ÿä¸€æ—¶é—´ç´¢å¼•: {len(time_index)} ä¸ªæ—¶é—´ç‚¹")
        print(f"æ—¶é—´èŒƒå›´: {time_index.min()} åˆ° {time_index.max()}")
        
        # åˆå§‹åŒ–å®½æ ¼å¼DataFrame
        df_wide = pd.DataFrame(index=time_index)
        
        # ä¸ºæ¯ä¸ªé«˜åº¦æ·»åŠ å˜é‡
        variables = ['wind_speed', 'wind_speed_max', 'wind_speed_min', 'wind_speed_instant', 
                    'wind_direction', 'wind_direction_instant', 'wind_speed_std',
                    'temperature', 'humidity', 'pressure', 'density']
        
        for height in sorted(valid_height_data.keys()):
            df_height = valid_height_data[height]
            print(f"  æ·»åŠ  {height}m æ•°æ®...")
            
            # è®¾ç½®æ—¶é—´ä¸ºç´¢å¼•ï¼ˆåˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŸæ•°æ®ï¼‰
            df_work = df_height.copy()
            df_work = df_work.set_index('datetime')
            
            # å¤„ç†é‡å¤æ—¶é—´ï¼ˆå–å¹³å‡å€¼ï¼‰
            if df_work.index.has_duplicates:
                print(f"    å‘ç°é‡å¤æ—¶é—´ï¼Œå–å¹³å‡å€¼")
                numeric_columns = df_work.select_dtypes(include=[np.number]).columns
                df_work = df_work[numeric_columns].groupby(df_work.index).mean()
            
            for var in variables:
                if var in df_work.columns:
                    col_name = f"{var}_{height}m"
                    
                    # å¯¹é½åˆ°ç»Ÿä¸€æ—¶é—´ç´¢å¼•
                    aligned_series = df_work[var].reindex(time_index)
                    df_wide[col_name] = aligned_series
                    
                    # ç»Ÿè®¡æœ‰æ•ˆæ•°æ®é‡
                    valid_count = df_wide[col_name].notna().sum()
                    valid_pct = valid_count / len(df_wide) * 100
                    print(f"    {col_name}: {valid_count} æœ‰æ•ˆå€¼ ({valid_pct:.1f}%)")
        
        print(f"\nå®½æ ¼å¼æ•°æ®åˆ›å»ºå®Œæˆ:")
        print(f"  å½¢çŠ¶: {df_wide.shape}")
        print(f"  åˆ—: {list(df_wide.columns)}")
        
        return df_wide

    def save_sanlijijingzi_data(self, df_wide):
        """ä¿å­˜ä¸‰åé‡Œäº•å­å¤„ç†åçš„æ•°æ®"""
        if df_wide.empty:
            print("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return None
        
        # ç”Ÿæˆæ–‡ä»¶å
        start_date = df_wide.index.min().strftime('%Y%m%d')
        end_date = df_wide.index.max().strftime('%Y%m%d')
        
        # ä¿å­˜CSVæ–‡ä»¶
        csv_filename = f"sanlijijingzi_{start_date}_{end_date}_cleaned.csv"
        csv_filepath = os.path.join(self.processed_path, csv_filename)
        
        try:
            df_wide.to_csv(csv_filepath)
            print(f"\nâœ“ æ•°æ®å·²ä¿å­˜åˆ°: {csv_filepath}")
            
            # æ£€æŸ¥ä¿å­˜çš„æ–‡ä»¶
            file_size = os.path.getsize(csv_filepath) / 1024  # KB
            print(f"  æ–‡ä»¶å¤§å°: {file_size:.1f} KB")
            
        except Exception as e:
            print(f"\nâœ— ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {e}")
            return None
        
        # ä¿å­˜æ‘˜è¦æ–‡ä»¶
        summary_filename = f"sanlijijingzi_{start_date}_{end_date}_summary.txt"
        summary_filepath = os.path.join(self.processed_path, summary_filename)
        
        try:
            with open(summary_filepath, 'w', encoding='utf-8') as f:
                f.write("ä¸‰åé‡Œäº•å­é£ç”µåœºæ•°æ®å¤„ç†æ‘˜è¦\n")
                f.write("="*50 + "\n\n")
                f.write(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ•°æ®å½¢çŠ¶: {df_wide.shape}\n")
                f.write(f"æ—¶é—´èŒƒå›´: {df_wide.index.min()} åˆ° {df_wide.index.max()}\n")
                f.write(f"æ•°æ®æœŸé—´: {(df_wide.index.max() - df_wide.index.min()).days} å¤©\n\n")
                
                f.write("å˜é‡åˆ—è¡¨:\n")
                for col in df_wide.columns:
                    valid_count = df_wide[col].notna().sum()
                    valid_pct = valid_count / len(df_wide) * 100
                    f.write(f"  {col}: {valid_count} æœ‰æ•ˆå€¼ ({valid_pct:.1f}%)\n")
                
                f.write("\nåŸºæœ¬ç»Ÿè®¡:\n")
                f.write(df_wide.describe().to_string())
            
            print(f"âœ“ æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_filepath}")
            
        except Exception as e:
            print(f"âœ— ä¿å­˜æ‘˜è¦æ–‡ä»¶å¤±è´¥: {e}")
        
        return csv_filepath

    def generate_data_summary(self, df_wide):
        """ç”Ÿæˆæ•°æ®æ‘˜è¦"""
        if df_wide.empty:
            print("æ²¡æœ‰æ•°æ®å¯ç”Ÿæˆæ‘˜è¦")
            return
        
        print("\n" + "="*60)
        print("ä¸‰åé‡Œäº•å­é£ç”µåœºæ•°æ®æ‘˜è¦")
        print("="*60)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"æ•°æ®å½¢çŠ¶: {df_wide.shape}")
        print(f"æ—¶é—´èŒƒå›´: {df_wide.index.min()} åˆ° {df_wide.index.max()}")
        print(f"æ•°æ®æœŸé—´: {(df_wide.index.max() - df_wide.index.min()).days} å¤©")
        
        # å˜é‡ç»Ÿè®¡
        print(f"\nå¯ç”¨å˜é‡:")
        for col in df_wide.columns:
            valid_count = df_wide[col].notna().sum()
            valid_pct = valid_count / len(df_wide) * 100
            if valid_count > 0:
                mean_val = df_wide[col].mean()
                print(f"  {col}: {valid_count} æœ‰æ•ˆå€¼ ({valid_pct:.1f}%), å‡å€¼: {mean_val:.2f}")
            else:
                print(f"  {col}: {valid_count} æœ‰æ•ˆå€¼ ({valid_pct:.1f}%), æ— æœ‰æ•ˆæ•°æ®")
        
        # æŒ‰é«˜åº¦åˆ†æé£é€Ÿ
        wind_speed_cols = [col for col in df_wide.columns if 'wind_speed' in col and not any(x in col for x in ['max', 'min', 'std', 'instant'])]
        if wind_speed_cols:
            print(f"\nå¹³å‡é£é€Ÿç»Ÿè®¡:")
            for col in sorted(wind_speed_cols):
                valid_data = df_wide[col].dropna()
                if len(valid_data) > 0:
                    stats = valid_data.describe()
                    print(f"  {col}:")
                    print(f"    å¹³å‡: {stats['mean']:.2f} m/s")
                    print(f"    æœ€å¤§: {stats['max']:.2f} m/s")
                    print(f"    æœ€å°: {stats['min']:.2f} m/s")
                    print(f"    æ ‡å‡†å·®: {stats['std']:.2f} m/s")
                else:
                    print(f"  {col}: æ— æœ‰æ•ˆæ•°æ®")

def main():
    """ä¸»å‡½æ•° - å¤„ç†ä¸‰åé‡Œäº•å­é£ç”µåœºæ•°æ®"""
    # è®¾ç½®ä¸‰åé‡Œäº•å­æ•°æ®è·¯å¾„
    sanlijijingzi_path = "/Users/xiaxin/work/WindForecast_Project/01_Data/raw/obs/sanlijijingzi"
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = SanlijijingziDataProcessor(sanlijijingzi_path)
    
    print("="*60)
    print("å¼€å§‹ä¸‰åé‡Œäº•å­é£ç”µåœºæ•°æ®å¤„ç†ï¼ˆä¿®å¤ç‰ˆï¼‰")
    print("="*60)
    
    # å¤„ç†æ‰€æœ‰é«˜åº¦æ•°æ®
    height_data = processor.process_all_heights()
    
    if height_data:
        print(f"\næˆåŠŸå¤„ç† {len(height_data)} ä¸ªé«˜åº¦çš„æ•°æ®")
        
        # åˆ›å»ºå®½æ ¼å¼æ•°æ®
        df_wide = processor.create_wide_format_data(height_data)
        
        if not df_wide.empty:
            # ç”Ÿæˆæ‘˜è¦
            processor.generate_data_summary(df_wide)
            
            # ä¿å­˜æ•°æ®
            output_file = processor.save_sanlijijingzi_data(df_wide)
            
            if output_file:
                print(f"\nğŸ‰ ä¸‰åé‡Œäº•å­é£ç”µåœºæ•°æ®å¤„ç†æˆåŠŸå®Œæˆï¼")
                print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
                
                # å¿«é€ŸéªŒè¯
                print(f"\nå¿«é€ŸéªŒè¯:")
                try:
                    test_df = pd.read_csv(output_file, index_col=0, parse_dates=True, nrows=5)
                    print(f"  âœ“ æ–‡ä»¶å¯æ­£å¸¸è¯»å–")
                    print(f"  âœ“ æ•°æ®å½¢çŠ¶: {test_df.shape}")
                    wind_cols = [col for col in test_df.columns if 'wind_speed' in col]
                    print(f"  âœ“ ä¸»è¦åˆ—: {wind_cols}")
                except Exception as e:
                    print(f"  âœ— æ–‡ä»¶éªŒè¯å¤±è´¥: {e}")
            else:
                print(f"\nâŒ æ•°æ®ä¿å­˜å¤±è´¥")
        else:
            print("âŒ å®½æ ¼å¼æ•°æ®åˆ›å»ºå¤±è´¥")
    else:
        print("âŒ æœªèƒ½åŠ è½½ä»»ä½•é«˜åº¦æ•°æ®")

if __name__ == "__main__":
    main()