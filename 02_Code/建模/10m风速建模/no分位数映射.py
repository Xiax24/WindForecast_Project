"""
Distribution Matching åˆ†å¸ƒåŒ¹é…æ–¹æ³•è¯¦è§£
é€šè¿‡åˆ†ä½æ•°æ˜ å°„è®©é¢„æµ‹åˆ†å¸ƒåŒ¹é…è§‚æµ‹åˆ†å¸ƒ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
from scipy.interpolate import interp1d
import seaborn as sns

def distribution_matching_detailed(y_test, y_pred_original, y_train, method='quantile'):
    """
    åˆ†å¸ƒåŒ¹é…çš„è¯¦ç»†å®ç°å’Œè§£é‡Š
    
    Parameters:
    -----------
    y_test : array
        æµ‹è¯•é›†è§‚æµ‹å€¼
    y_pred_original : array  
        åŸå§‹é¢„æµ‹å€¼
    y_train : array
        è®­ç»ƒé›†è§‚æµ‹å€¼ï¼ˆç”¨äºæ„å»ºç›®æ ‡åˆ†å¸ƒï¼‰
    method : str
        æ–¹æ³•é€‰æ‹©ï¼š'quantile', 'cdf', 'histogram'
    """
    
    print("ğŸ¯ åˆ†å¸ƒåŒ¹é…æ–¹æ³•è¯¦è§£")
    print("=" * 50)
    
    if method == 'quantile':
        return quantile_mapping(y_test, y_pred_original, y_train)
    elif method == 'cdf':
        return cdf_mapping(y_test, y_pred_original, y_train)
    elif method == 'histogram':
        return histogram_matching(y_test, y_pred_original, y_train)

def quantile_mapping(y_test, y_pred_original, y_train):
    """
    æ–¹æ³•1: åˆ†ä½æ•°æ˜ å°„ (Quantile Mapping)
    è¿™æ˜¯æ˜¨å¤©ä»£ç ä¸­ä½¿ç”¨çš„æ–¹æ³•
    """
    print("\nğŸ“Š æ–¹æ³•1: åˆ†ä½æ•°æ˜ å°„")
    print("-" * 30)
    
    # æ­¥éª¤1: è®¡ç®—è§‚æµ‹æ•°æ®çš„åˆ†ä½æ•°
    print("æ­¥éª¤1: è®¡ç®—ç›®æ ‡åˆ†å¸ƒï¼ˆè§‚æµ‹æ•°æ®ï¼‰çš„åˆ†ä½æ•°")
    percentiles = np.arange(0, 101, 1)  # 0%, 1%, 2%, ..., 100%
    obs_quantiles = np.percentile(y_train, percentiles)
    
    print(f"  è§‚æµ‹æ•°æ®èŒƒå›´: {y_train.min():.2f} - {y_train.max():.2f}")
    print(f"  åˆ†ä½æ•°æ•°é‡: {len(obs_quantiles)}")
    
    # æ­¥éª¤2: å¯¹æ¯ä¸ªé¢„æµ‹å€¼è¿›è¡Œæ˜ å°„
    print("\næ­¥éª¤2: é¢„æµ‹å€¼ â†’ åˆ†ä½æ•° â†’ è§‚æµ‹åˆ†å¸ƒå¯¹åº”å€¼")
    y_pred_corrected = np.zeros_like(y_pred_original)
    
    mapping_examples = []
    
    for i, pred in enumerate(y_pred_original):
        # 2.1: è®¡ç®—é¢„æµ‹å€¼åœ¨é¢„æµ‹åˆ†å¸ƒä¸­çš„åˆ†ä½æ•°ä½ç½®
        pred_percentile = stats.percentileofscore(y_pred_original, pred)
        
        # 2.2: æ˜ å°„åˆ°è§‚æµ‹åˆ†å¸ƒçš„å¯¹åº”åˆ†ä½æ•°å€¼
        if pred_percentile <= 0:
            corrected_value = obs_quantiles[0]
        elif pred_percentile >= 100:
            corrected_value = obs_quantiles[100]
        else:
            # çº¿æ€§æ’å€¼
            lower_idx = int(pred_percentile)
            upper_idx = min(lower_idx + 1, 100)
            weight = pred_percentile - lower_idx
            
            corrected_value = (obs_quantiles[lower_idx] * (1 - weight) + 
                             obs_quantiles[upper_idx] * weight)
        
        y_pred_corrected[i] = corrected_value
        
        # æ”¶é›†ä¸€äº›ç¤ºä¾‹ç”¨äºè¯´æ˜
        if i < 5:
            mapping_examples.append({
                'original_pred': pred,
                'percentile': pred_percentile,
                'corrected_pred': corrected_value
            })
    
    print("  æ˜ å°„ç¤ºä¾‹:")
    for ex in mapping_examples:
        print(f"    {ex['original_pred']:.2f} â†’ {ex['percentile']:.1f}% â†’ {ex['corrected_pred']:.2f}")
    
    print(f"\nç»“æœ: åŸå§‹é¢„æµ‹èŒƒå›´ {y_pred_original.min():.2f}-{y_pred_original.max():.2f}")
    print(f"      æ ¡æ­£åèŒƒå›´   {y_pred_corrected.min():.2f}-{y_pred_corrected.max():.2f}")
    print(f"      è§‚æµ‹èŒƒå›´     {y_test.min():.2f}-{y_test.max():.2f}")
    
    return y_pred_corrected

def cdf_mapping(y_test, y_pred_original, y_train):
    """
    æ–¹æ³•2: ç´¯ç§¯åˆ†å¸ƒå‡½æ•°æ˜ å°„ (CDF Mapping)
    æ›´å¹³æ»‘çš„åˆ†å¸ƒåŒ¹é…æ–¹æ³•
    """
    print("\nğŸ“Š æ–¹æ³•2: CDFæ˜ å°„")
    print("-" * 30)
    
    # æ„å»ºè§‚æµ‹æ•°æ®çš„ç»éªŒCDF
    print("æ­¥éª¤1: æ„å»ºç›®æ ‡åˆ†å¸ƒçš„ç»éªŒCDF")
    obs_sorted = np.sort(y_train)
    obs_cdf = np.arange(1, len(obs_sorted) + 1) / len(obs_sorted)
    
    # æ„å»ºé¢„æµ‹æ•°æ®çš„ç»éªŒCDF
    pred_sorted = np.sort(y_pred_original)
    pred_cdf = np.arange(1, len(pred_sorted) + 1) / len(pred_sorted)
    
    # åˆ›å»ºæ’å€¼å‡½æ•°
    print("æ­¥éª¤2: åˆ›å»ºCDFæ’å€¼å‡½æ•°")
    # é¢„æµ‹å€¼ â†’ CDFå€¼çš„æ˜ å°„
    pred_to_cdf = interp1d(pred_sorted, pred_cdf, 
                          bounds_error=False, fill_value=(0, 1))
    
    # CDFå€¼ â†’ è§‚æµ‹å€¼çš„æ˜ å°„
    cdf_to_obs = interp1d(obs_cdf, obs_sorted, 
                         bounds_error=False, 
                         fill_value=(obs_sorted[0], obs_sorted[-1]))
    
    # æ‰§è¡Œæ˜ å°„
    print("æ­¥éª¤3: æ‰§è¡ŒCDFæ˜ å°„")
    pred_cdf_values = pred_to_cdf(y_pred_original)
    y_pred_corrected = cdf_to_obs(pred_cdf_values)
    
    print(f"CDFæ˜ å°„å®Œæˆï¼Œå¤„ç†äº† {len(y_pred_original)} ä¸ªé¢„æµ‹å€¼")
    
    return y_pred_corrected

def histogram_matching(y_test, y_pred_original, y_train):
    """
    æ–¹æ³•3: ç›´æ–¹å›¾åŒ¹é… (Histogram Matching)
    åŸºäºç›´æ–¹å›¾çš„åˆ†å¸ƒæ ¡æ­£
    """
    print("\nğŸ“Š æ–¹æ³•3: ç›´æ–¹å›¾åŒ¹é…")
    print("-" * 30)
    
    # å®šä¹‰ç»Ÿä¸€çš„åˆ†ç®±
    bins = np.linspace(min(y_train.min(), y_pred_original.min()), 
                      max(y_train.max(), y_pred_original.max()), 50)
    
    # è®¡ç®—è§‚æµ‹å’Œé¢„æµ‹çš„ç›´æ–¹å›¾
    obs_hist, _ = np.histogram(y_train, bins=bins, density=True)
    pred_hist, _ = np.histogram(y_pred_original, bins=bins, density=True)
    
    # è®¡ç®—æ ¡æ­£å› å­
    correction_factors = np.divide(obs_hist, pred_hist, 
                                  out=np.ones_like(obs_hist), 
                                  where=pred_hist!=0)
    
    # åº”ç”¨æ ¡æ­£
    y_pred_corrected = y_pred_original.copy()
    bin_indices = np.digitize(y_pred_original, bins) - 1
    bin_indices = np.clip(bin_indices, 0, len(correction_factors) - 1)
    
    # è¿™é‡Œç®€åŒ–ä¸ºè°ƒæ•´é¢„æµ‹å€¼çš„æ–¹å·®
    pred_mean = np.mean(y_pred_original)
    pred_std = np.std(y_pred_original)
    obs_std = np.std(y_train)
    
    # è°ƒæ•´æ–¹å·®åŒ¹é…è§‚æµ‹æ•°æ®
    y_pred_corrected = pred_mean + (y_pred_original - pred_mean) * (obs_std / pred_std)
    
    print(f"ç›´æ–¹å›¾åŒ¹é…å®Œæˆ")
    print(f"  é¢„æµ‹æ•°æ®æ ‡å‡†å·®: {pred_std:.3f} â†’ {np.std(y_pred_corrected):.3f}")
    print(f"  è§‚æµ‹æ•°æ®æ ‡å‡†å·®: {obs_std:.3f}")
    
    return y_pred_corrected

def visualize_distribution_matching(y_test, y_pred_original, y_train, methods=['quantile', 'cdf']):
    """å¯è§†åŒ–åˆ†å¸ƒåŒ¹é…çš„æ•ˆæœ"""
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # åº”ç”¨ä¸åŒæ–¹æ³•
    results = {}
    for method in methods:
        results[method] = distribution_matching_detailed(y_test, y_pred_original, y_train, method)
    
    # 1. åŸå§‹åˆ†å¸ƒå¯¹æ¯”
    ax1 = axes[0, 0]
    ax1.hist(y_train, bins=30, alpha=0.7, density=True, label='è§‚æµ‹(è®­ç»ƒ)', color='black')
    ax1.hist(y_pred_original, bins=30, alpha=0.7, density=True, label='åŸå§‹é¢„æµ‹', color='blue')
    ax1.set_xlabel('é£é€Ÿ (m/s)')
    ax1.set_ylabel('å¯†åº¦')
    ax1.set_title('åŸå§‹åˆ†å¸ƒå¯¹æ¯”')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. æ ¡æ­£ååˆ†å¸ƒå¯¹æ¯”
    ax2 = axes[0, 1]
    ax2.hist(y_test, bins=30, alpha=0.7, density=True, label='è§‚æµ‹(æµ‹è¯•)', color='black')
    colors = ['red', 'green', 'orange']
    for i, (method, corrected) in enumerate(results.items()):
        ax2.hist(corrected, bins=30, alpha=0.6, density=True, 
                label=f'{method}æ ¡æ­£', color=colors[i])
    ax2.set_xlabel('é£é€Ÿ (m/s)')
    ax2.set_ylabel('å¯†åº¦')
    ax2.set_title('æ ¡æ­£ååˆ†å¸ƒå¯¹æ¯”')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. Q-Qå›¾
    ax3 = axes[0, 2]
    
    # è®¡ç®—åˆ†ä½æ•°
    quantiles = np.linspace(0.01, 0.99, 50)
    obs_quantiles = np.percentile(y_test, quantiles * 100)
    pred_quantiles = np.percentile(y_pred_original, quantiles * 100)
    
    ax3.scatter(obs_quantiles, pred_quantiles, alpha=0.6, label='åŸå§‹é¢„æµ‹', color='blue')
    
    for i, (method, corrected) in enumerate(results.items()):
        corr_quantiles = np.percentile(corrected, quantiles * 100)
        ax3.scatter(obs_quantiles, corr_quantiles, alpha=0.6, 
                   label=f'{method}æ ¡æ­£', color=colors[i], s=20)
    
    ax3.plot([obs_quantiles.min(), obs_quantiles.max()], 
            [obs_quantiles.min(), obs_quantiles.max()], 'k--', label='ç†æƒ³çº¿')
    ax3.set_xlabel('è§‚æµ‹åˆ†ä½æ•°')
    ax3.set_ylabel('é¢„æµ‹åˆ†ä½æ•°')
    ax3.set_title('Q-Qå›¾å¯¹æ¯”')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. æ•£ç‚¹å›¾å¯¹æ¯” - åŸå§‹
    ax4 = axes[1, 0]
    ax4.scatter(y_test, y_pred_original, alpha=0.5, s=2, color='blue')
    ax4.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
    ax4.set_xlabel('è§‚æµ‹é£é€Ÿ')
    ax4.set_ylabel('åŸå§‹é¢„æµ‹é£é€Ÿ')
    ax4.set_title('åŸå§‹é¢„æµ‹æ•£ç‚¹å›¾')
    ax4.grid(True, alpha=0.3)
    
    # 5. æ•£ç‚¹å›¾å¯¹æ¯” - æ ¡æ­£å
    ax5 = axes[1, 1]
    for i, (method, corrected) in enumerate(results.items()):
        ax5.scatter(y_test, corrected, alpha=0.6, s=2, 
                   label=f'{method}æ ¡æ­£', color=colors[i])
    ax5.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')
    ax5.set_xlabel('è§‚æµ‹é£é€Ÿ')
    ax5.set_ylabel('æ ¡æ­£åé¢„æµ‹é£é€Ÿ')
    ax5.set_title('æ ¡æ­£åé¢„æµ‹æ•£ç‚¹å›¾')
    ax5.legend()
    ax5.grid(True, alpha=0.3)
    
    # 6. 3-4m/såŒºé—´ç‰¹åˆ«å…³æ³¨
    ax6 = axes[1, 2]
    target_mask = (y_test >= 3) & (y_test < 4)
    
    if target_mask.sum() > 0:
        ax6.scatter(y_test[target_mask], y_pred_original[target_mask], 
                   alpha=0.8, s=30, label='åŸå§‹é¢„æµ‹', color='blue')
        
        for i, (method, corrected) in enumerate(results.items()):
            ax6.scatter(y_test[target_mask], corrected[target_mask], 
                       alpha=0.8, s=30, label=f'{method}æ ¡æ­£', color=colors[i])
        
        ax6.plot([3, 4], [3, 4], 'k--')
        ax6.set_xlim(2.8, 4.2)
        ax6.set_ylim(2.8, 4.2)
        ax6.set_xlabel('è§‚æµ‹é£é€Ÿ')
        ax6.set_ylabel('é¢„æµ‹é£é€Ÿ')
        ax6.set_title('3-4m/såŒºé—´å¯¹æ¯”')
        ax6.legend()
        ax6.grid(True, alpha=0.3)
    else:
        ax6.text(0.5, 0.5, '3-4m/såŒºé—´æ— æ•°æ®', ha='center', va='center', transform=ax6.transAxes)
    
    plt.tight_layout()
    plt.show()
    
    return results

def explain_distribution_matching_theory():
    """è§£é‡Šåˆ†å¸ƒåŒ¹é…çš„ç†è®ºåŸºç¡€"""
    
    print("ğŸ“ åˆ†å¸ƒåŒ¹é…ç†è®ºåŸºç¡€")
    print("=" * 50)
    
    print("\n1. æ ¸å¿ƒå‡è®¾:")
    print("   âœ“ æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ­£ç¡®çš„ç›¸å¯¹å…³ç³»ï¼ˆæ’åºï¼‰")
    print("   âœ“ ä½†é¢„æµ‹åˆ†å¸ƒçš„å½¢çŠ¶å¯èƒ½ä¸è§‚æµ‹ä¸åŒ")
    print("   âœ“ é€šè¿‡é‡æ–°æ˜ å°„å¯ä»¥æ ¡æ­£åˆ†å¸ƒåå·®")
    
    print("\n2. æ•°å­¦åŸç†:")
    print("   è®¾ F_obs ä¸ºè§‚æµ‹æ•°æ®çš„CDFï¼ŒF_pred ä¸ºé¢„æµ‹æ•°æ®çš„CDF")
    print("   å¯¹äºé¢„æµ‹å€¼ y_predï¼Œå…¶åˆ†ä½æ•°ä¸º: p = F_pred(y_pred)")
    print("   æ ¡æ­£å€¼ä¸º: y_corrected = F_obs^(-1)(p)")
    
    print("\n3. é€‚ç”¨åœºæ™¯:")
    print("   âœ“ é¢„æµ‹è¶‹åŠ¿æ­£ç¡®ä½†åˆ†å¸ƒåç§»")
    print("   âœ“ ç³»ç»Ÿæ€§çš„åå·®ï¼ˆå¦‚æ•´ä½“åé«˜æˆ–åä½ï¼‰")
    print("   âœ“ åˆ†å¸ƒå½¢çŠ¶å·®å¼‚ï¼ˆå¦‚æ–¹å·®ä¸åŒ¹é…ï¼‰")
    
    print("\n4. ä¼˜åŠ¿:")
    print("   âœ“ ä¿æŒé¢„æµ‹å€¼çš„ç›¸å¯¹æ’åº")
    print("   âœ“ è‡ªåŠ¨åŒ¹é…ç›®æ ‡åˆ†å¸ƒçš„ç»Ÿè®¡ç‰¹æ€§")
    print("   âœ“ ä¸éœ€è¦é¢å¤–çš„ç‰¹å¾å·¥ç¨‹")
    
    print("\n5. å±€é™æ€§:")
    print("   âš  å‡è®¾è®­ç»ƒé›†åˆ†å¸ƒä»£è¡¨çœŸå®åˆ†å¸ƒ")
    print("   âš  å¯èƒ½è¿‡åº¦ä¾èµ–å†å²æ•°æ®")
    print("   âš  å¯¹æå€¼çš„å¤„ç†å¯èƒ½ä¸å¤Ÿç¨³å¥")

def demo_distribution_matching():
    """æ¼”ç¤ºåˆ†å¸ƒåŒ¹é…çš„æ•ˆæœ"""
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    
    # æ¨¡æ‹Ÿè§‚æµ‹æ•°æ®ï¼ˆè®­ç»ƒé›†ï¼‰
    y_train = np.random.gamma(2, 2, 1000)  # ä¼½é©¬åˆ†å¸ƒ
    
    # æ¨¡æ‹Ÿæµ‹è¯•é›†è§‚æµ‹
    y_test = np.random.gamma(2, 2, 300)
    
    # æ¨¡æ‹Ÿæœ‰åå·®çš„é¢„æµ‹ï¼ˆæ­£æ€åˆ†å¸ƒï¼Œå‡å€¼åé«˜ï¼‰
    y_pred_original = np.random.normal(y_test.mean() + 1, y_test.std() * 0.8, len(y_test))
    y_pred_original = np.maximum(y_pred_original, 0)  # ç¡®ä¿éè´Ÿ
    
    print("ğŸ§ª åˆ†å¸ƒåŒ¹é…æ¼”ç¤º")
    print("=" * 50)
    print(f"è§‚æµ‹æ•°æ®ç»Ÿè®¡:")
    print(f"  è®­ç»ƒé›†: å‡å€¼={y_train.mean():.2f}, æ ‡å‡†å·®={y_train.std():.2f}")
    print(f"  æµ‹è¯•é›†: å‡å€¼={y_test.mean():.2f}, æ ‡å‡†å·®={y_test.std():.2f}")
    print(f"åŸå§‹é¢„æµ‹ç»Ÿè®¡:")
    print(f"  å‡å€¼={y_pred_original.mean():.2f}, æ ‡å‡†å·®={y_pred_original.std():.2f}")
    
    # åº”ç”¨åˆ†å¸ƒåŒ¹é…
    results = visualize_distribution_matching(y_test, y_pred_original, y_train)
    
    # è¯„ä¼°æ”¹å–„æ•ˆæœ
    print(f"\nğŸ“Š æ”¹å–„æ•ˆæœ:")
    from scipy.stats import pearsonr
    from sklearn.metrics import mean_squared_error
    
    orig_corr, _ = pearsonr(y_test, y_pred_original)
    orig_rmse = np.sqrt(mean_squared_error(y_test, y_pred_original))
    
    print(f"åŸå§‹é¢„æµ‹: ç›¸å…³ç³»æ•°={orig_corr:.4f}, RMSE={orig_rmse:.4f}")
    
    for method, corrected in results.items():
        corr, _ = pearsonr(y_test, corrected)
        rmse = np.sqrt(mean_squared_error(y_test, corrected))
        print(f"{method}æ ¡æ­£: ç›¸å…³ç³»æ•°={corr:.4f}, RMSE={rmse:.4f}")

if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    explain_distribution_matching_theory()
    demo_distribution_matching()