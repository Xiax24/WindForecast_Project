"""
å®Œæ•´çš„æ··åˆä¼˜åŒ–æµ‹è¯•
å¯»æ‰¾ç²¾åº¦ä¸ä½é£é€Ÿè¦†ç›–çš„æœ€ä½³å¹³è¡¡ç‚¹
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from scipy.stats import pearsonr
import lightgbm as lgb
import warnings
import os
from datetime import datetime
warnings.filterwarnings('ignore')

# è®¾ç½®matplotlibä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

def load_and_prepare_data(data_path):
    """åŠ è½½å’Œå‡†å¤‡æ•°æ®"""
    print("ğŸ“Š åŠ è½½æ•°æ®...")
    
    df = pd.read_csv(data_path)
    df['datetime'] = pd.to_datetime(df['datetime'])
    
    # åŸºç¡€æ¸…ç†
    df_clean = df[['datetime', 'obs_wind_speed_10m', 'ec_wind_speed_10m', 'gfs_wind_speed_10m']].dropna()
    
    # ç‰¹å¾å·¥ç¨‹
    df_clean['hour'] = df_clean['datetime'].dt.hour
    df_clean['day_of_year'] = df_clean['datetime'].dt.dayofyear
    df_clean['hour_sin'] = np.sin(2 * np.pi * df_clean['hour'] / 24)
    df_clean['hour_cos'] = np.cos(2 * np.pi * df_clean['hour'] / 24)
    df_clean['day_sin'] = np.sin(2 * np.pi * df_clean['day_of_year'] / 365)
    df_clean['day_cos'] = np.cos(2 * np.pi * df_clean['day_of_year'] / 365)
    df_clean['ec_gfs_mean'] = (df_clean['ec_wind_speed_10m'] + df_clean['gfs_wind_speed_10m']) / 2
    df_clean['ec_gfs_diff'] = abs(df_clean['ec_wind_speed_10m'] - df_clean['gfs_wind_speed_10m'])
    
    # ç‰¹å¾å’Œç›®æ ‡
    feature_cols = ['ec_gfs_mean', 'ec_gfs_diff', 'hour_sin', 'hour_cos', 
                   'day_sin', 'day_cos', 'ec_wind_speed_10m', 'gfs_wind_speed_10m']
    X = df_clean[feature_cols].values
    y = df_clean['obs_wind_speed_10m'].values
    
    return X, y, df_clean

def train_baseline_model(X_train, y_train, X_test, y_test):
    """è®­ç»ƒåŸºçº¿æ¨¡å‹"""
    print("ğŸ¯ è®­ç»ƒåŸºçº¿æ¨¡å‹...")
    
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'random_state': 42
    }
    
    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_test, label=y_test)
    
    model = lgb.train(
        params,
        train_data,
        valid_sets=[valid_data],
        num_boost_round=1000,
        callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(0)]
    )
    
    baseline_pred = model.predict(X_test, num_iteration=model.best_iteration)
    
    return model, baseline_pred

def order_preserving_mapping(y_pred_original, y_train, y_test):
    """Order_Preservingæ–¹æ³•ï¼ˆå·²çŸ¥æ•ˆæœå¥½çš„æ–¹æ¡ˆï¼‰"""
    print("ğŸ¯ Order_Preservingæ˜ å°„")
    
    sorted_indices = np.argsort(y_pred_original)
    n_samples = len(y_pred_original)
    target_percentiles = np.linspace(0, 100, n_samples)
    target_values = np.percentile(y_train, target_percentiles)
    
    y_pred_corrected = y_pred_original.copy()
    cutoff_idx = int(n_samples * 0.3)
    
    for i in range(cutoff_idx):
        original_idx = sorted_indices[i]
        mapping_strength = 1 - (i / cutoff_idx)
        
        y_pred_corrected[original_idx] = (
            mapping_strength * target_values[i] + 
            (1 - mapping_strength) * y_pred_original[original_idx]
        )
    
    return y_pred_corrected

def hybrid_conservative(y_pred_original, y_train, y_test, precision_weight=0.8):
    """æ··åˆæ–¹æ¡ˆ1: ä¿å®ˆè°ƒæ•´ï¼Œä¼˜å…ˆä¿æŒç²¾åº¦"""
    print(f"ğŸ¯ æ··åˆä¿å®ˆæ–¹æ¡ˆ (ç²¾åº¦æƒé‡={precision_weight})")
    
    y_pred_corrected = y_pred_original.copy()
    
    # è®¡ç®—ä½é£é€Ÿè¦†ç›–ç¼ºå£
    obs_low_ratio = (y_train < 3).mean()
    pred_low_ratio = (y_pred_original < 3).mean()
    coverage_deficit = max(0, obs_low_ratio - pred_low_ratio)
    
    print(f"  ä½é£é€Ÿè¦†ç›–ç¼ºå£: {coverage_deficit*100:.1f}%")
    
    if coverage_deficit > 0.05:  # åªæœ‰ç¼ºå£>5%æ‰ä¿®æ­£
        # æœ€å°å¿…è¦è°ƒæ•´é‡
        needed_samples = int(len(y_pred_original) * coverage_deficit * 0.5)  # åªä¿®æ­£ä¸€åŠç¼ºå£
        
        # é€‰æ‹©å½±å“æœ€å°çš„å€™é€‰æ ·æœ¬ï¼š3-5m/såŒºé—´
        candidates_mask = (y_pred_original >= 3) & (y_pred_original <= 5)
        candidates_indices = np.where(candidates_mask)[0]
        
        if len(candidates_indices) >= needed_samples:
            # éšæœºé€‰æ‹©ï¼Œé¿å…ç³»ç»Ÿæ€§åå·®
            selected_indices = np.random.choice(candidates_indices, needed_samples, replace=False)
            
            # ä¿å®ˆè°ƒæ•´ï¼šå‘ä½é£é€Ÿè½»å¾®æ¨è¿›
            obs_low_values = y_train[y_train < 3]
            
            for idx in selected_indices:
                original_pred = y_pred_original[idx]
                target_low = np.random.choice(obs_low_values)
                
                # è°ƒæ•´å¼ºåº¦åŸºäºç²¾åº¦æƒé‡
                adjustment_strength = (1 - precision_weight) * 0.5  # æœ€å¤š50%è°ƒæ•´
                
                y_pred_corrected[idx] = (
                    (1 - adjustment_strength) * original_pred + 
                    adjustment_strength * target_low
                )
        
        print(f"  ä¿å®ˆè°ƒæ•´äº† {needed_samples} ä¸ªæ ·æœ¬")
    
    return y_pred_corrected

def hybrid_segmented(y_pred_original, y_train, y_test):
    """æ··åˆæ–¹æ¡ˆ2: åˆ†æ®µä¼˜åŒ–"""
    print("ğŸ¯ æ··åˆåˆ†æ®µæ–¹æ¡ˆ")
    
    y_pred_corrected = y_pred_original.copy()
    
    # åªå¯¹0-4m/såŒºé—´è¿›è¡Œè½»å¾®è°ƒæ•´
    low_wind_mask = y_pred_original < 4
    
    if low_wind_mask.sum() > 0:
        low_indices = np.where(low_wind_mask)[0]
        low_pred = y_pred_original[low_indices]
        
        # è®¡ç®—è¿™ä¸ªåŒºé—´çš„ç›®æ ‡åˆ†å¸ƒ
        obs_low = y_train[y_train < 4]
        if len(obs_low) > 0:
            target_percentiles = np.linspace(0, 100, len(low_indices))
            target_values = np.percentile(obs_low, target_percentiles)
            
            # æ’åºå¹¶è½»å¾®æ˜ å°„
            sorted_low_indices = np.argsort(low_pred)
            
            for i, orig_idx in enumerate(sorted_low_indices):
                actual_idx = low_indices[orig_idx]
                # æ¸è¿›æ˜ å°„ï¼Œä½†å¼ºåº¦å¾ˆä½
                mapping_strength = 0.2 * (1 - i / len(sorted_low_indices))  # æœ€å¤š20%è°ƒæ•´
                
                y_pred_corrected[actual_idx] = (
                    (1 - mapping_strength) * y_pred_original[actual_idx] + 
                    mapping_strength * target_values[i]
                )
        
        print(f"  è½»å¾®è°ƒæ•´äº†ä½é£é€ŸåŒºé—´: {low_wind_mask.sum()} æ ·æœ¬")
    
    return y_pred_corrected

def hybrid_multi_objective(y_pred_original, y_train, y_test):
    """æ··åˆæ–¹æ¡ˆ3: å¤šç›®æ ‡ä¼˜åŒ–"""
    print("ğŸ¯ æ··åˆå¤šç›®æ ‡ä¼˜åŒ–")
    
    def multi_objective_score(y_pred, y_true, y_train, alpha=0.75):
        """å¤šç›®æ ‡è¯„åˆ†å‡½æ•°"""
        # ç²¾åº¦å¾—åˆ†
        corr, _ = pearsonr(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        precision_score = corr - rmse/15  # å½’ä¸€åŒ–ç²¾åº¦å¾—åˆ†
        
        # åˆ†å¸ƒå®Œæ•´æ€§å¾—åˆ†
        obs_low_ratio = (y_train < 3).mean()
        pred_low_ratio = (y_pred < 3).mean()
        coverage_score = min(pred_low_ratio / obs_low_ratio, 1.0) if obs_low_ratio > 0 else 1.0
        
        # è¿ç»­æ€§å¾—åˆ†
        continuity_samples = ((y_pred >= 3) & (y_pred < 4)).sum()
        continuity_score = min(continuity_samples / 200, 1.0)  # ç›®æ ‡200ä¸ªæ ·æœ¬
        
        distribution_score = (coverage_score + continuity_score) / 2
        
        # ç»¼åˆå¾—åˆ† (75%ç²¾åº¦ + 25%åˆ†å¸ƒ)
        total_score = alpha * precision_score + (1 - alpha) * distribution_score
        
        return total_score, precision_score, distribution_score
    
    # å‚æ•°æœç´¢å¯»æ‰¾æœ€ä¼˜å¹³è¡¡
    best_score = -999
    best_pred = y_pred_original.copy()
    best_params = None
    
    print("  æœç´¢æœ€ä¼˜å‚æ•°...")
    
    # ç²¾ç®€æœç´¢ç©ºé—´ï¼Œé¿å…è¿‡åº¦ä¼˜åŒ–
    for cutoff_ratio in [0.15, 0.20, 0.25]:
        for mapping_strength in [0.2, 0.3, 0.4]:
            y_pred_candidate = y_pred_original.copy()
            
            sorted_indices = np.argsort(y_pred_original)
            cutoff_idx = int(len(y_pred_original) * cutoff_ratio)
            
            target_percentiles = np.linspace(0, 100, len(y_pred_original))
            target_values = np.percentile(y_train, target_percentiles)
            
            for i in range(cutoff_idx):
                original_idx = sorted_indices[i]
                strength = mapping_strength * (1 - i / cutoff_idx)
                
                y_pred_candidate[original_idx] = (
                    strength * target_values[i] + 
                    (1 - strength) * y_pred_original[original_idx]
                )
            
            # è¯„ä¼°å€™é€‰æ–¹æ¡ˆ
            score, prec_score, dist_score = multi_objective_score(
                y_pred_candidate, y_test, y_train
            )
            
            if score > best_score:
                best_score = score
                best_pred = y_pred_candidate.copy()
                best_params = (cutoff_ratio, mapping_strength, prec_score, dist_score)
    
    if best_params:
        cutoff, strength, prec, dist = best_params
        print(f"  æœ€ä¼˜å‚æ•°: cutoff={cutoff:.2f}, strength={strength:.1f}")
        print(f"  ç²¾åº¦å¾—åˆ†: {prec:.3f}, åˆ†å¸ƒå¾—åˆ†: {dist:.3f}")
    
    return best_pred

def evaluate_method(y_pred, y_test, y_train, method_name):
    """è¯„ä¼°å•ä¸ªæ–¹æ³•"""
    corr, _ = pearsonr(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    
    # ä½é£é€Ÿç»Ÿè®¡
    obs_low_1 = (y_test < 1).sum()
    obs_low_2 = (y_test < 2).sum()
    obs_low_3 = (y_test < 3).sum()
    obs_low_4 = (y_test < 4).sum()
    
    pred_low_1 = (y_pred < 1).sum()
    pred_low_2 = (y_pred < 2).sum()
    pred_low_3 = (y_pred < 3).sum()
    pred_low_4 = (y_pred < 4).sum()
    
    # è¿ç»­æ€§è¯„ä¼°
    continuity_34 = ((y_pred >= 3) & (y_pred < 4)).sum()
    
    # è¦†ç›–ç‡
    coverage_3 = pred_low_3 / obs_low_3 if obs_low_3 > 0 else 0
    
    # ç»¼åˆè¯„åˆ† (ç²¾åº¦æƒé‡70%)
    comprehensive_score = 0.7 * (corr - rmse/15) + 0.3 * min(coverage_3, 1.0)
    
    return {
        'method': method_name,
        'corr': corr,
        'rmse': rmse,
        'pred_low_1': pred_low_1,
        'pred_low_2': pred_low_2,
        'pred_low_3': pred_low_3,
        'pred_low_4': pred_low_4,
        'continuity_34': continuity_34,
        'coverage_3': coverage_3,
        'comp_score': comprehensive_score,
        'pred': y_pred
    }

def compare_all_hybrid_methods(y_pred_original, y_train, y_test):
    """æ¯”è¾ƒæ‰€æœ‰æ··åˆæ–¹æ³•"""
    print("\nğŸ”„ æ¯”è¾ƒæ‰€æœ‰æ··åˆä¼˜åŒ–æ–¹æ³•")
    print("="*80)
    
    # è¿è¡Œæ‰€æœ‰æ–¹æ³•
    methods_results = []
    
    # åŸºçº¿
    methods_results.append(evaluate_method(y_pred_original, y_test, y_train, "Original_Baseline"))
    
    # Order_Preserving (å·²çŸ¥å¥½æ–¹æ¡ˆ)
    order_pred = order_preserving_mapping(y_pred_original.copy(), y_train, y_test)
    methods_results.append(evaluate_method(order_pred, y_test, y_train, "Order_Preserving"))
    
    # æ··åˆæ–¹æ¡ˆ
    hybrid_cons_pred = hybrid_conservative(y_pred_original.copy(), y_train, y_test, 0.85)
    methods_results.append(evaluate_method(hybrid_cons_pred, y_test, y_train, "Hybrid_Conservative"))
    
    hybrid_moderate_pred = hybrid_conservative(y_pred_original.copy(), y_train, y_test, 0.7)
    methods_results.append(evaluate_method(hybrid_moderate_pred, y_test, y_train, "Hybrid_Moderate"))
    
    hybrid_seg_pred = hybrid_segmented(y_pred_original.copy(), y_train, y_test)
    methods_results.append(evaluate_method(hybrid_seg_pred, y_test, y_train, "Hybrid_Segmented"))
    
    hybrid_multi_pred = hybrid_multi_objective(y_pred_original.copy(), y_train, y_test)
    methods_results.append(evaluate_method(hybrid_multi_pred, y_test, y_train, "Hybrid_MultiObj"))
    
    # æ˜¾ç¤ºç»“æœè¡¨æ ¼
    print(f"\nğŸ“Š è¯¦ç»†å¯¹æ¯”ç»“æœ:")
    print(f"{'æ–¹æ³•':<18} {'ç›¸å…³ç³»æ•°':<8} {'RMSE':<8} {'<3m/s':<8} {'3-4m/s':<8} {'è¦†ç›–ç‡':<8} {'ç»¼åˆåˆ†':<8}")
    print("-"*85)
    
    # è§‚æµ‹åŸºå‡†
    obs_low_3 = (y_test < 3).sum()
    print(f"{'è§‚æµ‹åŸºå‡†':<18} {'--':<8} {'--':<8} {obs_low_3:<8} {'--':<8} {'1.00':<8} {'--':<8}")
    print("-"*85)
    
    for result in methods_results:
        print(f"{result['method']:<18} {result['corr']:<8.4f} {result['rmse']:<8.4f} "
              f"{result['pred_low_3']:<8} {result['continuity_34']:<8} "
              f"{result['coverage_3']:<8.2f} {result['comp_score']:<8.3f}")
    
    return methods_results

def visualize_hybrid_comparison(methods_results, y_test, output_dir):
    """å¯è§†åŒ–æ··åˆæ–¹æ³•å¯¹æ¯”"""
    
    fig, axes = plt.subplots(2, 3, figsize=(20, 12))
    
    # é€‰æ‹©è¦æ˜¾ç¤ºçš„æ–¹æ³•ï¼ˆè·³è¿‡åŸºçº¿ï¼‰
    display_methods = [r for r in methods_results if r['method'] != 'Original_Baseline']
    colors = ['red', 'green', 'orange', 'purple', 'brown']
    
    for i, result in enumerate(display_methods[:5]):
        row = i // 3
        col = i % 3
        ax = axes[row, col]
        
        pred = result['pred']
        
        # ç»˜åˆ¶æ•£ç‚¹å›¾
        ax.scatter(y_test, pred, alpha=0.4, s=1, color=colors[i])
        ax.plot([0, 12], [0, 12], 'k--', linewidth=2)
        
        # æ ‡æ³¨å…³æ³¨åŒºé—´
        ax.axvspan(3, 4, alpha=0.1, color='red')
        ax.axhspan(3, 4, alpha=0.1, color='red')
        
        # æ˜¾ç¤ºæŒ‡æ ‡
        method_name = result['method'].replace('_', '\n')
        ax.set_title(f"{method_name}\nCorr={result['corr']:.3f}, RMSE={result['rmse']:.3f}\n"
                    f"<3m/s: {result['pred_low_3']}, 3-4m/s: {result['continuity_34']}")
        ax.set_xlabel('è§‚æµ‹é£é€Ÿ (m/s)')
        ax.set_ylabel('é¢„æµ‹é£é€Ÿ (m/s)')
        ax.grid(True, alpha=0.3)
        ax.set_xlim(0, 10)
        ax.set_ylim(0, 10)
    
    # æœ€åä¸€ä¸ªå›¾ï¼šæ€§èƒ½é›·è¾¾å›¾
    if len(display_methods) <= 5:
        ax_radar = axes[1, 2] if len(display_methods) <= 2 else axes[1, len(display_methods) % 3]
        
        # æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
        methods_names = [r['method'].replace('_', '\n') for r in display_methods]
        corr_scores = [r['corr'] for r in display_methods]
        coverage_scores = [r['coverage_3'] for r in display_methods]
        comp_scores = [r['comp_score'] for r in display_methods]
        
        x = np.arange(len(methods_names))
        width = 0.25
        
        ax_radar.bar(x - width, corr_scores, width, label='ç›¸å…³ç³»æ•°', alpha=0.7)
        ax_radar.bar(x, coverage_scores, width, label='è¦†ç›–ç‡', alpha=0.7)
        ax_radar.bar(x + width, comp_scores, width, label='ç»¼åˆè¯„åˆ†', alpha=0.7)
        
        ax_radar.set_xlabel('æ–¹æ³•')
        ax_radar.set_ylabel('å¾—åˆ†')
        ax_radar.set_title('æ€§èƒ½å¯¹æ¯”')
        ax_radar.set_xticks(x)
        ax_radar.set_xticklabels(methods_names, rotation=45, ha='right')
        ax_radar.legend()
        ax_radar.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    plot_path = os.path.join(output_dir, f'hybrid_methods_comparison_{timestamp}.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {plot_path}")
    
    plt.show()
    return fig

def generate_recommendations(methods_results):
    """ç”Ÿæˆæ¨èå»ºè®®"""
    print("\nğŸ¯ æ–¹æ¡ˆæ¨è")
    print("="*50)
    
    # æ‰¾å‡ºå„æ–¹é¢çš„æœ€ä½³æ–¹æ¡ˆ
    best_precision = max(methods_results, key=lambda x: x['corr'])
    best_coverage = max(methods_results, key=lambda x: x['coverage_3'])
    best_comprehensive = max(methods_results, key=lambda x: x['comp_score'])
    
    print(f"ğŸ† å„æŒ‡æ ‡æœ€ä½³æ–¹æ¡ˆ:")
    print(f"  ç²¾åº¦æœ€ä½³: {best_precision['method']} (Corr={best_precision['corr']:.4f})")
    print(f"  è¦†ç›–æœ€ä½³: {best_coverage['method']} (è¦†ç›–ç‡={best_coverage['coverage_3']:.2f})")
    print(f"  ç»¼åˆæœ€ä½³: {best_comprehensive['method']} (ç»¼åˆåˆ†={best_comprehensive['comp_score']:.3f})")
    
    print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    
    # åŸºäºä¸åŒéœ€æ±‚ç»™å‡ºå»ºè®®
    baseline = next(r for r in methods_results if r['method'] == 'Original_Baseline')
    
    for result in methods_results:
        if result['method'] == 'Original_Baseline':
            continue
            
        corr_loss = baseline['corr'] - result['corr']
        coverage_gain = result['coverage_3'] - baseline['coverage_3']
        
        print(f"\nğŸ“Œ {result['method']}:")
        
        if corr_loss < 0.005 and coverage_gain > 0.3:
            print(f"  âœ… å¼ºçƒˆæ¨è: ç²¾åº¦æŸå¤±æå°({corr_loss:.4f})ï¼Œè¦†ç›–å¤§å¹…æå‡({coverage_gain:.2f})")
        elif corr_loss < 0.01 and coverage_gain > 0.2:
            print(f"  ğŸŒŸ æ¨è: ç²¾åº¦è½»å¾®æŸå¤±({corr_loss:.4f})ï¼Œè¦†ç›–æ˜¾è‘—æå‡({coverage_gain:.2f})")
        elif corr_loss < 0.02:
            print(f"  âš–ï¸ å¹³è¡¡é€‰æ‹©: å¯æ¥å—çš„ç²¾åº¦æŸå¤±({corr_loss:.4f})æ¢å–è¦†ç›–æ”¹å–„({coverage_gain:.2f})")
        else:
            print(f"  âš ï¸ æƒè¡¡é€‰æ‹©: éœ€è¦è¯„ä¼°ç²¾åº¦æŸå¤±({corr_loss:.4f})æ˜¯å¦å¯æ¥å—")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ··åˆä¼˜åŒ–æ–¹æ¡ˆå®Œæ•´æµ‹è¯•")
    print("å¯»æ‰¾ç²¾åº¦ä¸ä½é£é€Ÿè¦†ç›–çš„æœ€ä½³å¹³è¡¡")
    print("="*60)
    
    # é…ç½®è·¯å¾„
    data_path = '/Users/xiaxin/work/WindForecast_Project/01_Data/processed/imputed_data/changma_clean.csv'
    output_dir = '/Users/xiaxin/work/WindForecast_Project/03_Results/å»ºæ¨¡/10mé£é€Ÿå»ºæ¨¡/æ··åˆä¼˜åŒ–æ–¹æ¡ˆ'
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # 1. æ•°æ®å‡†å¤‡
        X, y, df_clean = load_and_prepare_data(data_path)
        
        split_idx = int(len(X) * 0.8)
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        print(f"æ•°æ®è§„æ¨¡: è®­ç»ƒ{len(X_train)}, æµ‹è¯•{len(X_test)}")
        
        # æ˜¾ç¤ºè§‚æµ‹æ•°æ®çš„ä½é£é€Ÿåˆ†å¸ƒ
        for threshold in [1, 2, 3, 4]:
            count = (y_test < threshold).sum()
            pct = count / len(y_test) * 100
            print(f"æµ‹è¯•é›†<{threshold}m/s: {count} ({pct:.1f}%)")
        
        # 2. è®­ç»ƒåŸºçº¿æ¨¡å‹
        baseline_model, baseline_pred = train_baseline_model(X_train, y_train, X_test, y_test)
        
        # 3. æ¯”è¾ƒæ‰€æœ‰æ··åˆæ–¹æ³•
        methods_results = compare_all_hybrid_methods(baseline_pred, y_train, y_test)
        
        # 4. å¯è§†åŒ–å¯¹æ¯”
        fig = visualize_hybrid_comparison(methods_results, y_test, output_dir)
        
        # 5. ç”Ÿæˆæ¨è
        generate_recommendations(methods_results)
        
        # 6. ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        pred_data = {result['method']: result['pred'] for result in methods_results}
        pred_df = pd.DataFrame(pred_data)
        pred_path = os.path.join(output_dir, f'hybrid_predictions_{timestamp}.csv')
        pred_df.to_csv(pred_path, index=False)
        
        # ä¿å­˜æ€§èƒ½å¯¹æ¯”
        perf_data = []
        for result in methods_results:
            perf_data.append({
                'Method': result['method'],
                'Correlation': result['corr'],
                'RMSE': result['rmse'],
                'Pred_Low_3ms': result['pred_low_3'],
                'Continuity_3_4ms': result['continuity_34'],
                'Coverage_Rate': result['coverage_3'],
                'Comprehensive_Score': result['comp_score']
            })
        
        perf_df = pd.DataFrame(perf_data)
        perf_path = os.path.join(output_dir, f'hybrid_performance_{timestamp}.csv')
        perf_df.to_csv(perf_path, index=False)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        print(f"  é¢„æµ‹æ•°æ®: {os.path.basename(pred_path)}")
        print(f"  æ€§èƒ½å¯¹æ¯”: {os.path.basename(perf_path)}")
        
        print("\nğŸ‰ æ··åˆä¼˜åŒ–æµ‹è¯•å®Œæˆï¼")
        print("ç°åœ¨æ‚¨å¯ä»¥æ ¹æ®ä¸šåŠ¡éœ€æ±‚é€‰æ‹©æœ€åˆé€‚çš„æ–¹æ¡ˆäº†ï¼")
        
        return methods_results, output_dir
        
    except FileNotFoundError:
        print(f"âŒ æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {data_path}")
        return None
        
    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    methods_results, output_dir = main()
    
    if methods_results:
        print("\nâœ… æ··åˆä¼˜åŒ–æµ‹è¯•æˆåŠŸå®Œæˆï¼")
        print("æŸ¥çœ‹ç”Ÿæˆçš„å›¾è¡¨å’Œæ€§èƒ½å¯¹æ¯”ï¼Œé€‰æ‹©æœ€é€‚åˆæ‚¨éœ€æ±‚çš„æ–¹æ¡ˆã€‚")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„å’Œç¯å¢ƒé…ç½®ã€‚")