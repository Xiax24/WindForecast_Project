"""
Enhanced Low Wind Speed Correction Strategies
å¢å¼ºå‹ä½é£é€Ÿè®¢æ­£ç­–ç•¥ - è§£å†³3-4m/sæ•°æ®ç¼ºå¤±é—®é¢˜
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy.stats import pearsonr
import lightgbm as lgb
from scipy import stats
from scipy.interpolate import interp1d
import warnings
import os
warnings.filterwarnings('ignore')

def ensure_output_directory(base_path):
    """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
    output_dir = os.path.join(base_path, "ä½é£é€Ÿè®¢æ­£åˆ†æ")
    os.makedirs(output_dir, exist_ok=True)
    return output_dir

def load_data_with_analysis(file_path):
    """åŠ è½½æ•°æ®å¹¶åˆ†æä½é£é€Ÿåˆ†å¸ƒ"""
    print("ğŸ“Š åŠ è½½æ•°æ®å¹¶åˆ†æä½é£é€Ÿåˆ†å¸ƒ...")
    
    df = pd.read_csv(file_path)
    df['datetime'] = pd.to_datetime(df['datetime'])
    
    # åŸºç¡€æ¸…ç†
    df_clean = df[['datetime', 'obs_wind_speed_10m', 'ec_wind_speed_10m', 'gfs_wind_speed_10m']].dropna()
    
    # åˆ†æå„é£é€ŸåŒºé—´åˆ†å¸ƒ
    wind_ranges = [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 8), (8, 15)]
    print("\nğŸŒªï¸ è¯¦ç»†é£é€ŸåŒºé—´æ ·æœ¬åˆ†å¸ƒ:")
    print("=" * 70)
    print(f"{'åŒºé—´(m/s)':<12} {'æ ·æœ¬æ•°':<8} {'å æ¯”%':<8} {'è§‚æµ‹å‡å€¼':<10} {'ECå‡å€¼':<10} {'GFSå‡å€¼':<10}")
    print("-" * 70)
    
    distribution_info = {}
    for low, high in wind_ranges:
        mask = (df_clean['obs_wind_speed_10m'] >= low) & (df_clean['obs_wind_speed_10m'] < high)
        count = mask.sum()
        pct = count / len(df_clean) * 100
        obs_mean = df_clean.loc[mask, 'obs_wind_speed_10m'].mean() if count > 0 else 0
        ec_mean = df_clean.loc[mask, 'ec_wind_speed_10m'].mean() if count > 0 else 0
        gfs_mean = df_clean.loc[mask, 'gfs_wind_speed_10m'].mean() if count > 0 else 0
        
        distribution_info[(low, high)] = {
            'count': count, 'pct': pct, 'obs_mean': obs_mean, 
            'ec_mean': ec_mean, 'gfs_mean': gfs_mean
        }
        
        print(f"{low}-{high:<8} {count:<8} {pct:<8.1f} {obs_mean:<10.2f} {ec_mean:<10.2f} {gfs_mean:<10.2f}")
    
    # è¯†åˆ«æ•°æ®ç¨€å°‘åŒºé—´
    sparse_ranges = [(low, high) for (low, high), info in distribution_info.items() 
                    if info['count'] < len(df_clean) * 0.01]  # å°‘äº1%çš„åŒºé—´
    
    if sparse_ranges:
        print(f"\nâš ï¸  æ•°æ®ç¨€å°‘åŒºé—´ (<1%): {sparse_ranges}")
    
    return df_clean, distribution_info

def strategy_6_data_augmentation(X_train, y_train, distribution_info):
    """ç­–ç•¥6: æ•°æ®å¢å¼º - é’ˆå¯¹ç¨€å°‘åŒºé—´ç”Ÿæˆåˆæˆæ•°æ®"""
    print("\nğŸ¯ ç­–ç•¥6: æ•°æ®å¢å¼ºï¼ˆè§£å†³3-4m/sæ•°æ®ç¼ºå¤±ï¼‰")
    print("-" * 50)
    
    # è¯†åˆ«éœ€è¦å¢å¼ºçš„åŒºé—´
    target_range = (3, 4)
    range_mask = (y_train >= target_range[0]) & (y_train < target_range[1])
    current_samples = range_mask.sum()
    
    print(f"å½“å‰3-4m/sæ ·æœ¬æ•°: {current_samples}")
    
    if current_samples < 100:  # å¦‚æœæ ·æœ¬æ•°å°‘äº100ï¼Œè¿›è¡Œæ•°æ®å¢å¼º
        print("å¼€å§‹æ•°æ®å¢å¼º...")
        
        # æ–¹æ³•1: åŸºäºç›¸é‚»åŒºé—´çš„æ’å€¼ç”Ÿæˆ
        lower_range_mask = (y_train >= 2) & (y_train < 3)
        upper_range_mask = (y_train >= 4) & (y_train < 5)
        
        if lower_range_mask.sum() > 0 and upper_range_mask.sum() > 0:
            # è·å–ç›¸é‚»åŒºé—´çš„æ ·æœ¬
            X_lower = X_train[lower_range_mask]
            y_lower = y_train[lower_range_mask]
            X_upper = X_train[upper_range_mask]
            y_upper = y_train[upper_range_mask]
            
            # ç”Ÿæˆç›®æ ‡æ•°é‡çš„åˆæˆæ ·æœ¬
            target_samples = max(200, len(X_train) // 50)  # è‡³å°‘200ä¸ªæ ·æœ¬
            
            synthetic_X = []
            synthetic_y = []
            
            for _ in range(target_samples):
                # éšæœºé€‰æ‹©ä¸Šä¸‹åŒºé—´çš„æ ·æœ¬
                if len(X_lower) > 0 and len(X_upper) > 0:
                    idx_lower = np.random.randint(0, len(X_lower))
                    idx_upper = np.random.randint(0, len(X_upper))
                    
                    # çº¿æ€§æ’å€¼ç”Ÿæˆæ–°æ ·æœ¬
                    alpha = np.random.random()  # æ’å€¼æƒé‡
                    
                    new_x = alpha * X_lower[idx_lower] + (1 - alpha) * X_upper[idx_upper]
                    new_y = alpha * y_lower[idx_lower] + (1 - alpha) * y_upper[idx_upper]
                    
                    # ç¡®ä¿ç”Ÿæˆçš„yåœ¨ç›®æ ‡åŒºé—´å†…
                    new_y = np.clip(new_y, target_range[0], target_range[1])
                    
                    synthetic_X.append(new_x)
                    synthetic_y.append(new_y)
            
            if synthetic_X:
                synthetic_X = np.array(synthetic_X)
                synthetic_y = np.array(synthetic_y)
                
                # åˆå¹¶åŸå§‹æ•°æ®å’Œåˆæˆæ•°æ®
                X_augmented = np.vstack([X_train, synthetic_X])
                y_augmented = np.hstack([y_train, synthetic_y])
                
                print(f"ç”Ÿæˆåˆæˆæ•°æ®: {len(synthetic_X)} ä¸ªæ ·æœ¬")
                print(f"å¢å¼ºå3-4m/sæ ·æœ¬æ•°: {((y_augmented >= 3) & (y_augmented < 4)).sum()}")
                
                return X_augmented, y_augmented
    
    print("æ— éœ€æ•°æ®å¢å¼ºæˆ–å¢å¼ºå¤±è´¥ï¼Œè¿”å›åŸå§‹æ•°æ®")
    return X_train, y_train

def strategy_7_smooth_interpolation(y_test, y_pred_original, smoothing_window=0.5):
    """ç­–ç•¥7: å¹³æ»‘æ’å€¼æ ¡æ­£ - è§£å†³é¢„æµ‹å€¼åœ¨ç‰¹å®šåŒºé—´çš„è·³è·ƒ"""
    print("\nğŸ¯ ç­–ç•¥7: å¹³æ»‘æ’å€¼æ ¡æ­£")
    print("-" * 40)
    
    # åˆ›å»ºé£é€Ÿ-é¢„æµ‹å€¼çš„æ˜ å°„å…³ç³»
    wind_bins = np.arange(0, 15, 0.1)
    bin_centers = (wind_bins[:-1] + wind_bins[1:]) / 2
    
    # è®¡ç®—æ¯ä¸ªåŒºé—´çš„å¹³å‡é¢„æµ‹åå·®
    digitized = np.digitize(y_test, wind_bins)
    
    bias_correction = np.zeros_like(bin_centers)
    for i in range(1, len(wind_bins)):
        mask = digitized == i
        if mask.sum() > 5:  # è‡³å°‘5ä¸ªæ ·æœ¬
            bias_correction[i-1] = np.mean(y_test[mask] - y_pred_original[mask])
    
    # å¹³æ»‘åå·®æ ¡æ­£æ›²çº¿
    from scipy.ndimage import gaussian_filter1d
    smooth_bias = gaussian_filter1d(bias_correction, sigma=2)
    
    # åº”ç”¨æ ¡æ­£
    y_pred_corrected = y_pred_original.copy()
    
    for i, pred in enumerate(y_pred_original):
        # æ‰¾åˆ°å¯¹åº”çš„åŒºé—´
        bin_idx = np.digitize(pred, wind_bins) - 1
        bin_idx = np.clip(bin_idx, 0, len(smooth_bias) - 1)
        
        # åº”ç”¨å¹³æ»‘æ ¡æ­£
        y_pred_corrected[i] = pred + smooth_bias[bin_idx]
    
    # ç¡®ä¿é¢„æµ‹å€¼ä¸ºæ­£
    y_pred_corrected = np.maximum(y_pred_corrected, 0)
    
    print(f"3-4m/såŒºé—´æ ¡æ­£å‰åå¯¹æ¯”:")
    range_mask = (y_test >= 3) & (y_test < 4)
    if range_mask.sum() > 0:
        before_rmse = np.sqrt(mean_squared_error(y_test[range_mask], y_pred_original[range_mask]))
        after_rmse = np.sqrt(mean_squared_error(y_test[range_mask], y_pred_corrected[range_mask]))
        print(f"æ ¡æ­£å‰RMSE: {before_rmse:.4f}")
        print(f"æ ¡æ­£åRMSE: {after_rmse:.4f}")
    
    return y_pred_corrected

def strategy_8_hybrid_ensemble(strategies_results, y_test, target_range=(3, 4)):
    """ç­–ç•¥8: æ··åˆé›†æˆ - é’ˆå¯¹ç‰¹å®šåŒºé—´é€‰æ‹©æœ€ä¼˜ç­–ç•¥"""
    print("\nğŸ¯ ç­–ç•¥8: æ··åˆé›†æˆï¼ˆé’ˆå¯¹3-4m/sä¼˜åŒ–ï¼‰")
    print("-" * 50)
    
    # è¯„ä¼°å„ç­–ç•¥åœ¨ç›®æ ‡åŒºé—´çš„è¡¨ç°
    range_mask = (y_test >= target_range[0]) & (y_test < target_range[1])
    
    if range_mask.sum() == 0:
        print("ç›®æ ‡åŒºé—´æ— è§‚æµ‹æ•°æ®ï¼Œä½¿ç”¨å…¨å±€æœ€ä¼˜ç­–ç•¥")
        # é€‰æ‹©å…¨å±€ç›¸å…³ç³»æ•°æœ€é«˜çš„ç­–ç•¥
        best_strategy = None
        best_corr = -1
        
        for strategy_name, y_pred in strategies_results.items():
            corr, _ = pearsonr(y_test, y_pred)
            if corr > best_corr:
                best_corr = corr
                best_strategy = strategy_name
        
        return strategies_results[best_strategy], best_strategy
    
    strategy_performance = {}
    
    print("å„ç­–ç•¥åœ¨3-4m/såŒºé—´çš„è¡¨ç°:")
    for strategy_name, y_pred in strategies_results.items():
        if range_mask.sum() > 0:
            range_rmse = np.sqrt(mean_squared_error(y_test[range_mask], y_pred[range_mask]))
            range_mae = mean_absolute_error(y_test[range_mask], y_pred[range_mask])
            
            strategy_performance[strategy_name] = {
                'rmse': range_rmse,
                'mae': range_mae,
                'score': 1 / (range_rmse + 0.001)  # ç»¼åˆè¯„åˆ†
            }
            
            print(f"{strategy_name}: RMSE={range_rmse:.4f}, MAE={range_mae:.4f}")
    
    # é€‰æ‹©åœ¨ç›®æ ‡åŒºé—´è¡¨ç°æœ€å¥½çš„ç­–ç•¥
    best_strategy = max(strategy_performance.keys(), 
                       key=lambda x: strategy_performance[x]['score'])
    
    print(f"é€‰æ‹©ç­–ç•¥: {best_strategy}")
    
    # åˆ›å»ºæ··åˆé¢„æµ‹ï¼šåœ¨ç›®æ ‡åŒºé—´ä½¿ç”¨æœ€ä¼˜ç­–ç•¥ï¼Œå…¶ä»–åŒºé—´ä½¿ç”¨åŠ æƒå¹³å‡
    y_pred_hybrid = np.zeros_like(y_test)
    
    # ç›®æ ‡åŒºé—´ä½¿ç”¨æœ€ä¼˜ç­–ç•¥
    y_pred_hybrid[range_mask] = strategies_results[best_strategy][range_mask]
    
    # å…¶ä»–åŒºé—´ä½¿ç”¨åŠ æƒé›†æˆ
    other_mask = ~range_mask
    if other_mask.sum() > 0:
        # è®¡ç®—æƒé‡ï¼ˆåŸºäºå…¨å±€è¡¨ç°ï¼‰
        weights = {}
        total_score = 0
        
        for strategy_name, y_pred in strategies_results.items():
            corr, _ = pearsonr(y_test[other_mask], y_pred[other_mask])
            rmse = np.sqrt(mean_squared_error(y_test[other_mask], y_pred[other_mask]))
            score = corr / (rmse + 0.001)
            weights[strategy_name] = max(0, score)
            total_score += weights[strategy_name]
        
        # å½’ä¸€åŒ–æƒé‡
        if total_score > 0:
            for strategy_name in weights:
                weights[strategy_name] /= total_score
        
        # åŠ æƒå¹³å‡
        for strategy_name, y_pred in strategies_results.items():
            y_pred_hybrid[other_mask] += weights[strategy_name] * y_pred[other_mask]
    
    return y_pred_hybrid, f"Hybrid_{best_strategy}"

def save_results_and_models(strategies_results, results_df, models_dict, output_dir):
    """ä¿å­˜ç»“æœå’Œæ¨¡å‹"""
    print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_dir}")
    
    # 1. ä¿å­˜é¢„æµ‹ç»“æœ
    predictions_df = pd.DataFrame(strategies_results)
    predictions_df.to_csv(os.path.join(output_dir, 'all_predictions.csv'), index=False)
    
    # 2. ä¿å­˜æ€§èƒ½è¯„ä¼°ç»“æœ
    results_df.to_csv(os.path.join(output_dir, 'performance_comparison.csv'), index=False)
    
    # 3. ä¿å­˜æ¨¡å‹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    import pickle
    if models_dict:
        with open(os.path.join(output_dir, 'trained_models.pkl'), 'wb') as f:
            pickle.dump(models_dict, f)
    
    # 4. ç”Ÿæˆåˆ†ææŠ¥å‘Š
    create_analysis_report(strategies_results, results_df, output_dir)
    
    print("âœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜")

def create_analysis_report(strategies_results, results_df, output_dir):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    report_path = os.path.join(output_dir, 'analysis_report.md')
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# ä½é£é€Ÿè®¢æ­£ç­–ç•¥åˆ†ææŠ¥å‘Š\n\n")
        f.write("## æ‰§è¡Œæ—¶é—´\n")
        f.write(f"- åˆ†ææ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("## ç­–ç•¥æ¦‚è¿°\n")
        f.write("æœ¬åˆ†æé’ˆå¯¹é£é€Ÿé¢„æµ‹ä¸­3-4m/såŒºé—´æ•°æ®ç¨€å°‘çš„é—®é¢˜ï¼Œå®æ–½äº†8ç§ä¸åŒçš„è®¢æ­£ç­–ç•¥ï¼š\n\n")
        f.write("1. **åŠ æƒè®­ç»ƒ**: å¢åŠ ä½é£é€Ÿæ ·æœ¬æƒé‡\n")
        f.write("2. **åˆ†å±‚å»ºæ¨¡**: ä½é£é€Ÿå’Œé«˜é£é€Ÿåˆ†åˆ«å»ºæ¨¡\n")
        f.write("3. **åˆ†å¸ƒåŒ¹é…**: è°ƒæ•´é¢„æµ‹åˆ†å¸ƒåŒ¹é…è§‚æµ‹åˆ†å¸ƒ\n")
        f.write("4. **åˆ†ä½æ•°å›å½’**: ä¼˜åŒ–ä½åˆ†ä½æ•°é¢„æµ‹\n")
        f.write("5. **æ®‹å·®æ ¡æ­£**: å­¦ä¹ ç³»ç»Ÿæ€§åå·®\n")
        f.write("6. **æ•°æ®å¢å¼º**: ç”Ÿæˆåˆæˆæ•°æ®å¡«è¡¥ç¨€å°‘åŒºé—´\n")
        f.write("7. **å¹³æ»‘æ’å€¼**: æ¶ˆé™¤é¢„æµ‹è·³è·ƒ\n")
        f.write("8. **æ··åˆé›†æˆ**: é’ˆå¯¹ä¸åŒåŒºé—´é€‰æ‹©æœ€ä¼˜ç­–ç•¥\n\n")
        
        f.write("## æ€§èƒ½å¯¹æ¯”\n\n")
        f.write("| ç­–ç•¥ | ç›¸å…³ç³»æ•° | æ€»RMSE | <4m/sæ ·æœ¬æ•° | <4m/så æ¯”% | ä½é£é€ŸRMSE |\n")
        f.write("|------|----------|---------|-------------|------------|------------|\n")
        
        for _, row in results_df.iterrows():
            f.write(f"| {row['Strategy']} | {row['Correlation']:.4f} | {row['RMSE']:.4f} | "
                   f"{row['Low_Wind_Samples(<4)']:.0f} | {row['Low_Wind_Pct(%)']:.1f} | "
                   f"{row['Low_Wind_RMSE']:.4f} |\n")
        
        f.write("\n## ä¸»è¦å‘ç°\n\n")
        
        # æ‰¾å‡ºæœ€ä½³ç­–ç•¥
        best_low_wind = results_df.loc[results_df['Strategy'] != 'EC_Baseline'].nsmallest(1, 'Low_Wind_RMSE')
        best_overall = results_df.loc[results_df['Strategy'] != 'EC_Baseline'].nlargest(1, 'Correlation')
        
        f.write(f"- **ä½é£é€Ÿè¡¨ç°æœ€ä½³**: {best_low_wind.iloc[0]['Strategy']}\n")
        f.write(f"- **ç»¼åˆè¡¨ç°æœ€ä½³**: {best_overall.iloc[0]['Strategy']}\n")
        
        f.write("\n## å»ºè®®\n\n")
        f.write("åŸºäºåˆ†æç»“æœï¼Œå»ºè®®ï¼š\n")
        f.write("1. åœ¨å®é™…åº”ç”¨ä¸­ä¼˜å…ˆä½¿ç”¨æ··åˆé›†æˆç­–ç•¥\n")
        f.write("2. ç»§ç»­æ”¶é›†3-4m/såŒºé—´çš„è§‚æµ‹æ•°æ®\n")
        f.write("3. è€ƒè™‘ä½¿ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯æ”¹å–„ç¨€å°‘åŒºé—´çš„é¢„æµ‹\n")
        f.write("4. å®šæœŸé‡æ–°è®­ç»ƒæ¨¡å‹ä»¥é€‚åº”æ•°æ®åˆ†å¸ƒå˜åŒ–\n")

def create_enhanced_comparison_plots(y_test, strategies_results, ec_baseline, output_dir):
    """åˆ›å»ºå¢å¼ºçš„å¯¹æ¯”å›¾è¡¨ï¼Œç‰¹åˆ«å…³æ³¨3-4m/såŒºé—´"""
    print("\nğŸ“ˆ åˆ›å»ºå¢å¼ºå¯¹æ¯”å›¾è¡¨...")
    
    # åˆ›å»ºæ›´å¤§çš„å›¾è¡¨å¸ƒå±€
    fig, axes = plt.subplots(4, 3, figsize=(24, 20))
    
    # 1. 3-4m/såŒºé—´ç‰¹åˆ«å…³æ³¨çš„æ•£ç‚¹å›¾
    ax1 = axes[0, 0]
    target_mask = (y_test >= 3) & (y_test <= 4)
    
    colors = ['orange', 'blue', 'green', 'red', 'purple', 'brown', 'pink', 'gray']
    
    if target_mask.sum() > 0:
        for i, (strategy_name, y_pred) in enumerate(strategies_results.items()):
            ax1.scatter(y_test[target_mask], y_pred[target_mask], 
                       alpha=0.7, s=20, label=strategy_name, color=colors[i % len(colors)])
        
        ax1.plot([3, 4], [3, 4], 'r--', linewidth=2, label='Perfect')
        ax1.set_xlabel('Observed Wind Speed (m/s)')
        ax1.set_ylabel('Predicted Wind Speed (m/s)')
        ax1.set_title('Focus on 3-4 m/s Range')
        ax1.legend(fontsize=8)
        ax1.grid(True, alpha=0.3)
        ax1.set_xlim(2.8, 4.2)
        ax1.set_ylim(2.8, 4.2)
    else:
        ax1.text(0.5, 0.5, 'No data in 3-4 m/s range', 
                ha='center', va='center', transform=ax1.transAxes)
        ax1.set_title('3-4 m/s Range (No Data)')
    
    # 2. ä½é£é€ŸåŒºé—´è¯¦ç»†åˆ†å¸ƒ
    ax2 = axes[0, 1]
    
    wind_ranges = np.arange(0, 8, 0.5)
    obs_counts, _ = np.histogram(y_test, bins=wind_ranges)
    
    # å„ç­–ç•¥åœ¨ä¸åŒåŒºé—´çš„é¢„æµ‹æ•°é‡
    bar_width = 0.1
    x_pos = wind_ranges[:-1]
    
    for i, (strategy_name, y_pred) in enumerate(strategies_results.items()):
        pred_counts, _ = np.histogram(y_pred, bins=wind_ranges)
        ax2.bar(x_pos + i * bar_width, pred_counts, bar_width, 
               label=strategy_name, alpha=0.7, color=colors[i % len(colors)])
    
    # è§‚æµ‹æ•°æ®
    ax2.bar(x_pos - bar_width, obs_counts, bar_width, 
           label='Observed', alpha=0.8, color='black')
    
    ax2.set_xlabel('Wind Speed (m/s)')
    ax2.set_ylabel('Sample Count')
    ax2.set_title('Detailed Distribution in Low Wind Speed Range')
    ax2.legend(fontsize=8)
    ax2.grid(True, alpha=0.3)
    ax2.axvspan(3, 4, alpha=0.2, color='red', label='Target Range')
    
    # 3. æ®‹å·®åˆ†æ
    ax3 = axes[0, 2]
    
    # é€‰æ‹©ä¸€ä¸ªä»£è¡¨æ€§ç­–ç•¥è¿›è¡Œæ®‹å·®åˆ†æ
    if 'Hybrid_' in next(iter(strategies_results.keys()), ''):
        representative_pred = next(iter(strategies_results.values()))
    else:
        representative_pred = strategies_results[list(strategies_results.keys())[0]]
    
    residuals = y_test - representative_pred
    
    ax3.scatter(y_test, residuals, alpha=0.5, s=2)
    ax3.axhline(y=0, color='r', linestyle='--')
    ax3.set_xlabel('Observed Wind Speed (m/s)')
    ax3.set_ylabel('Residuals (Obs - Pred)')
    ax3.set_title('Residual Analysis')
    ax3.grid(True, alpha=0.3)
    
    # æ·»åŠ 3-4m/såŒºé—´çš„æ®‹å·®ç»Ÿè®¡
    if target_mask.sum() > 0:
        target_residuals = residuals[target_mask]
        ax3.axvspan(3, 4, alpha=0.2, color='red')
        ax3.text(0.02, 0.98, f'3-4m/s residual std: {np.std(target_residuals):.3f}', 
                transform=ax3.transAxes, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    # 4-11. å„ç­–ç•¥çš„è¯¦ç»†æ•£ç‚¹å›¾ï¼ˆç¬¬äºŒã€ä¸‰ã€å››è¡Œï¼‰
    strategy_names = list(strategies_results.keys())
    positions = [(1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2), (3, 0), (3, 1)]
    
    for i, strategy_name in enumerate(strategy_names[:8]):  # æœ€å¤š8ä¸ªç­–ç•¥
        if i < len(positions):
            row, col = positions[i]
            ax = axes[row, col]
            
            y_pred = strategies_results[strategy_name]
            
            # å…¨éƒ¨æ•°æ®çš„æ•£ç‚¹å›¾
            ax.scatter(y_test, y_pred, alpha=0.3, s=1, color=colors[i])
            ax.plot([0, 15], [0, 15], 'r--', linewidth=2)
            
            # ç‰¹åˆ«æ ‡æ³¨3-4m/såŒºé—´
            if target_mask.sum() > 0:
                ax.scatter(y_test[target_mask], y_pred[target_mask], 
                          alpha=0.8, s=10, color='red', edgecolors='black', linewidth=0.5)
            
            # è®¡ç®—æŒ‡æ ‡
            corr, _ = pearsonr(y_test, y_pred)
            rmse = np.sqrt(mean_squared_error(y_test, y_pred))
            
            # 3-4m/såŒºé—´ç‰¹åˆ«è¯„ä¼°
            if target_mask.sum() > 0:
                target_rmse = np.sqrt(mean_squared_error(y_test[target_mask], y_pred[target_mask]))
                target_samples = ((y_pred >= 3) & (y_pred <= 4)).sum()
                
                ax.set_title(f'{strategy_name}\nCorr={corr:.3f}, RMSE={rmse:.3f}\n'
                           f'Target RMSE={target_rmse:.3f}, Pred Samples={target_samples}')
            else:
                ax.set_title(f'{strategy_name}\nCorr={corr:.3f}, RMSE={rmse:.3f}')
            
            ax.set_xlabel('Observed Wind Speed (m/s)')
            ax.set_ylabel('Predicted Wind Speed (m/s)')
            ax.grid(True, alpha=0.3)
            
            # æ·»åŠ 3-4m/såŒºåŸŸçš„çŸ©å½¢æ¡†
            ax.add_patch(plt.Rectangle((3, 3), 1, 1, fill=False, edgecolor='red', 
                                     linewidth=2, linestyle='--', alpha=0.7))
    
    # 12. æ€§èƒ½é›·è¾¾å›¾
    ax12 = axes[3, 2]
    
    # å‡†å¤‡é›·è¾¾å›¾æ•°æ®
    metrics = ['Correlation', 'Low_RMSE_Inv', 'Sample_Coverage', 'Overall_RMSE_Inv']
    
    # è®¡ç®—å„ç­–ç•¥çš„æ ‡å‡†åŒ–æŒ‡æ ‡
    strategy_metrics = {}
    for strategy_name, y_pred in strategies_results.items():
        corr, _ = pearsonr(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        
        # ä½é£é€ŸRMSE
        low_mask = y_test < 4
        low_rmse = np.sqrt(mean_squared_error(y_test[low_mask], y_pred[low_mask])) if low_mask.sum() > 0 else rmse
        
        # æ ·æœ¬è¦†ç›–åº¦ï¼ˆé¢„æµ‹çš„ä½é£é€Ÿæ ·æœ¬æ¯”ä¾‹ï¼‰
        pred_low_ratio = (y_pred < 4).mean()
        obs_low_ratio = (y_test < 4).mean()
        coverage = 1 - abs(pred_low_ratio - obs_low_ratio)
        
        strategy_metrics[strategy_name] = [
            corr,                                    # ç›¸å…³ç³»æ•°
            1/(low_rmse + 0.001),                   # ä½é£é€ŸRMSEå€’æ•°
            coverage,                                # æ ·æœ¬è¦†ç›–åº¦
            1/(rmse + 0.001)                        # æ€»RMSEå€’æ•°
        ]
    
    # ç®€åŒ–æ˜¾ç¤ºï¼šåªæ˜¾ç¤ºå‰3ä¸ªæœ€å¥½çš„ç­–ç•¥
    if len(strategy_metrics) > 3:
        # æ ¹æ®ç»¼åˆæ€§èƒ½æ’åº
        sorted_strategies = sorted(strategy_metrics.items(), 
                                 key=lambda x: sum(x[1]), reverse=True)[:3]
        strategy_metrics = dict(sorted_strategies)
    
    # ç»˜åˆ¶é›·è¾¾å›¾
    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆ
    
    for i, (strategy_name, values) in enumerate(strategy_metrics.items()):
        values += values[:1]  # é—­åˆ
        ax12.plot(angles, values, 'o-', linewidth=2, label=strategy_name, color=colors[i])
        ax12.fill(angles, values, alpha=0.25, color=colors[i])
    
    ax12.set_xticks(angles[:-1])
    ax12.set_xticklabels(metrics)
    ax12.set_title('Performance Radar Chart\n(Top 3 Strategies)')
    ax12.legend(fontsize=8)
    ax12.grid(True)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    plt.savefig(os.path.join(output_dir, 'comprehensive_analysis.png'), 
                dpi=300, bbox_inches='tight')
    plt.show()
    
    return fig

def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰å¢å¼ºçš„ä½é£é€Ÿè®¢æ­£ç­–ç•¥"""
    # æ•°æ®è·¯å¾„å’Œè¾“å‡ºè·¯å¾„
    data_path = '/Users/xiaxin/work/WindForecast_Project/01_Data/processed/imputed_data/changma_clean.csv'
    base_output_path = '/Users/xiaxin/work/WindForecast_Project/03_Results/å»ºæ¨¡/10mé£é€Ÿå»ºæ¨¡'
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = ensure_output_directory(base_output_path)
    
    print("ğŸš€ å¼€å§‹å¢å¼ºå‹ä½é£é€Ÿè®¢æ­£ç­–ç•¥åˆ†æ...")
    
    # 1. åŠ è½½å’Œåˆ†ææ•°æ®
    df, distribution_info = load_data_with_analysis(data_path)
    
    # 2. å‡†å¤‡ç‰¹å¾
    df['hour'] = df['datetime'].dt.hour
    df['day_of_year'] = df['datetime'].dt.dayofyear
    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
    df['day_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)
    df['day_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)
    df['ec_gfs_mean'] = (df['ec_wind_speed_10m'] + df['gfs_wind_speed_10m']) / 2
    df['ec_gfs_diff'] = abs(df['ec_wind_speed_10m'] - df['gfs_wind_speed_10m'])
    
    # ç‰¹å¾å’Œç›®æ ‡
    feature_cols = ['ec_gfs_mean', 'ec_gfs_diff', 'hour_sin', 'hour_cos', 
                   'day_sin', 'day_cos', 'ec_wind_speed_10m', 'gfs_wind_speed_10m']
    X = df[feature_cols].values
    y = df['obs_wind_speed_10m'].values
    
    # 3. æ•°æ®åˆ†å‰²ï¼ˆæ—¶é—´åºåˆ—åˆ†å‰²ï¼‰
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    ec_baseline = X_test[:, -2]  # ec_wind_speed_10m
    
    print(f"\nè®­ç»ƒé›†å¤§å°: {len(X_train)}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(X_test)}")
    
    # æ£€æŸ¥3-4m/såŒºé—´çš„æ•°æ®æƒ…å†µ
    target_range_train = ((y_train >= 3) & (y_train < 4)).sum()
    target_range_test = ((y_test >= 3) & (y_test < 4)).sum()
    print(f"è®­ç»ƒé›†3-4m/sæ ·æœ¬: {target_range_train}")
    print(f"æµ‹è¯•é›†3-4m/sæ ·æœ¬: {target_range_test}")
    
    # 4. æ•°æ®å¢å¼ºï¼ˆå¦‚æœéœ€è¦ï¼‰
    X_train_aug, y_train_aug = strategy_6_data_augmentation(X_train, y_train, distribution_info)
    
    # 5. æ‰§è¡ŒåŸºç¡€ç­–ç•¥
    strategies_results = {}
    models_dict = {}
    
    # ç­–ç•¥1: åŠ æƒè®­ç»ƒ
    model_weighted, y_pred_weighted = strategy_1_weighted_training(X_train_aug, y_train_aug, X_test, y_test)
    strategies_results['Weighted_Training'] = y_pred_weighted
    models_dict['weighted'] = model_weighted
    
    # ç­–ç•¥2: åˆ†å±‚å»ºæ¨¡
    models_ensemble, y_pred_ensemble = strategy_2_ensemble_modeling(X_train_aug, y_train_aug, X_test, y_test)
    strategies_results['Ensemble_Modeling'] = y_pred_ensemble
    models_dict['ensemble'] = models_ensemble
    
    # ç­–ç•¥3: åˆ†å¸ƒåŒ¹é…
    y_pred_distribution = strategy_3_distribution_matching(y_test, y_pred_weighted, y_train_aug)
    strategies_results['Distribution_Matching'] = y_pred_distribution
    
    # ç­–ç•¥4: åˆ†ä½æ•°å›å½’
    models_quantile, y_pred_quantile, y_pred_quantile_adj = strategy_4_quantile_regression(X_train_aug, y_train_aug, X_test, y_test)
    strategies_results['Quantile_Regression'] = y_pred_quantile_adj
    models_dict['quantile'] = models_quantile
    
    # ç­–ç•¥5: æ®‹å·®æ ¡æ­£
    y_pred_residual = strategy_5_residual_correction(y_test, y_pred_weighted, ec_baseline)
    strategies_results['Residual_Correction'] = y_pred_residual
    
    # ç­–ç•¥7: å¹³æ»‘æ’å€¼æ ¡æ­£
    y_pred_smooth = strategy_7_smooth_interpolation(y_test, y_pred_weighted)
    strategies_results['Smooth_Interpolation'] = y_pred_smooth
    
    # ç­–ç•¥8: æ··åˆé›†æˆ
    y_pred_hybrid, hybrid_name = strategy_8_hybrid_ensemble(strategies_results, y_test)
    strategies_results[hybrid_name] = y_pred_hybrid
    
    # 6. è¯„ä¼°æ‰€æœ‰ç­–ç•¥
    results_df = evaluate_all_strategies(strategies_results, y_test, ec_baseline)
    
    # 7. åˆ›å»ºå¢å¼ºçš„å¯¹æ¯”å›¾è¡¨
    fig = create_enhanced_comparison_plots(y_test, strategies_results, ec_baseline, output_dir)
    
    # 8. ä¿å­˜æ‰€æœ‰ç»“æœ
    save_results_and_models(strategies_results, results_df, models_dict, output_dir)
    
    # 9. ç‰¹åˆ«åˆ†æ3-4m/såŒºé—´
    analyze_target_range(y_test, strategies_results, output_dir)
    
    # 10. ç”Ÿæˆæœ€ç»ˆå»ºè®®
    print("\nğŸ† æœ€ç»ˆå»ºè®®:")
    print("=" * 60)
    
    # æ‰¾å‡ºå„æ–¹é¢æœ€ä½³ç­–ç•¥
    best_low_wind = results_df.loc[results_df['Strategy'] != 'EC_Baseline'].nsmallest(1, 'Low_Wind_RMSE')
    best_overall = results_df.loc[results_df['Strategy'] != 'EC_Baseline'].nlargest(1, 'Correlation')
    best_coverage = results_df.loc[results_df['Strategy'] != 'EC_Baseline'].iloc[
        (results_df.loc[results_df['Strategy'] != 'EC_Baseline', 'Low_Wind_Pct(%)'] - 
         (y_test < 4).mean() * 100).abs().idxmin() - results_df.index[0]
    ]
    
    print(f"ğŸ¯ ä½é£é€Ÿç²¾åº¦æœ€ä½³: {best_low_wind.iloc[0]['Strategy']}")
    print(f"ğŸ“Š ç»¼åˆè¡¨ç°æœ€ä½³: {best_overall.iloc[0]['Strategy']}")
    print(f"ğŸª æ ·æœ¬è¦†ç›–æœ€ä½³: {best_coverage['Strategy']}")
    
    # æ£€æŸ¥3-4m/sé—®é¢˜æ˜¯å¦å¾—åˆ°æ”¹å–„
    original_34_samples = (ec_baseline >= 3) & (ec_baseline < 4)
    print(f"\nğŸ“ˆ 3-4m/såŒºé—´æ”¹å–„æƒ…å†µ:")
    print(f"åŸå§‹ECé¢„æµ‹è¯¥åŒºé—´æ ·æœ¬æ•°: {original_34_samples.sum()}")
    
    for strategy_name, y_pred in strategies_results.items():
        improved_34_samples = (y_pred >= 3) & (y_pred < 4)
        improvement = improved_34_samples.sum() - original_34_samples.sum()
        print(f"{strategy_name}: {improved_34_samples.sum()} (+{improvement})")
    
    return strategies_results, results_df, output_dir

def analyze_target_range(y_test, strategies_results, output_dir):
    """ç‰¹åˆ«åˆ†æ3-4m/såŒºé—´çš„æ”¹å–„æƒ…å†µ"""
    print("\nğŸ” 3-4m/såŒºé—´è¯¦ç»†åˆ†æ")
    print("=" * 50)
    
    target_mask = (y_test >= 3) & (y_test < 4)
    
    if target_mask.sum() == 0:
        print("âš ï¸ æµ‹è¯•é›†ä¸­æ— 3-4m/sè§‚æµ‹æ•°æ®")
        
        # åˆ†æé¢„æµ‹åˆ†å¸ƒçš„æ”¹å–„
        analysis_results = []
        for strategy_name, y_pred in strategies_results.items():
            pred_34_count = ((y_pred >= 3) & (y_pred < 4)).sum()
            pred_34_ratio = pred_34_count / len(y_pred) * 100
            
            analysis_results.append({
                'Strategy': strategy_name,
                'Pred_3-4_Count': pred_34_count,
                'Pred_3-4_Ratio(%)': pred_34_ratio
            })
        
        analysis_df = pd.DataFrame(analysis_results)
        analysis_df.to_csv(os.path.join(output_dir, 'target_range_analysis.csv'), index=False)
        
        print("é¢„æµ‹åˆ†å¸ƒåœ¨3-4m/såŒºé—´çš„æ”¹å–„:")
        for _, row in analysis_df.iterrows():
            print(f"{row['Strategy']}: {row['Pred_3-4_Count']} æ ·æœ¬ ({row['Pred_3-4_Ratio(%)']:.1f}%)")
    
    else:
        print(f"æµ‹è¯•é›†3-4m/sè§‚æµ‹æ ·æœ¬: {target_mask.sum()}")
        
        # è¯¦ç»†åˆ†æå„ç­–ç•¥åœ¨è¯¥åŒºé—´çš„è¡¨ç°
        target_analysis = []
        for strategy_name, y_pred in strategies_results.items():
            target_pred = y_pred[target_mask]
            target_obs = y_test[target_mask]
            
            rmse = np.sqrt(mean_squared_error(target_obs, target_pred))
            mae = mean_absolute_error(target_obs, target_pred)
            bias = np.mean(target_pred - target_obs)
            
            target_analysis.append({
                'Strategy': strategy_name,
                'RMSE': rmse,
                'MAE': mae,
                'Bias': bias,
                'Pred_Mean': np.mean(target_pred),
                'Obs_Mean': np.mean(target_obs)
            })
        
        target_df = pd.DataFrame(target_analysis)
        target_df.to_csv(os.path.join(output_dir, 'target_range_detailed_analysis.csv'), index=False)
        
        print("\n3-4m/såŒºé—´è¯¦ç»†è¡¨ç°:")
        print(f"{'ç­–ç•¥':<20} {'RMSE':<8} {'MAE':<8} {'åå·®':<8} {'é¢„æµ‹å‡å€¼':<10} {'è§‚æµ‹å‡å€¼':<10}")
        print("-" * 70)
        
        for _, row in target_df.iterrows():
            print(f"{row['Strategy']:<20} {row['RMSE']:<8.4f} {row['MAE']:<8.4f} "
                  f"{row['Bias']:<8.4f} {row['Pred_Mean']:<10.4f} {row['Obs_Mean']:<10.4f}")

# åŸæœ‰çš„ç­–ç•¥å‡½æ•°ä¿æŒä¸å˜
def strategy_1_weighted_training(X_train, y_train, X_test, y_test):
    """ç­–ç•¥1: åŠ æƒè®­ç»ƒ - å¢åŠ ä½é£é€Ÿæ ·æœ¬æƒé‡"""
    print("\nğŸ¯ ç­–ç•¥1: åŠ æƒè®­ç»ƒ")
    print("-" * 40)
    
    # åˆ›å»ºæ ·æœ¬æƒé‡ï¼šä½é£é€Ÿæ ·æœ¬æƒé‡æ›´é«˜
    def create_sample_weights(y, low_threshold=4.0, weight_multiplier=3.0):
        weights = np.ones(len(y))
        weights[y < low_threshold] = weight_multiplier
        # å¯¹3-4m/såŒºé—´ç»™äºˆé¢å¤–æƒé‡
        weights[(y >= 3) & (y < 4)] = weight_multiplier * 1.5
        return weights
    
    sample_weights = create_sample_weights(y_train)
    
    print(f"ä½é£é€Ÿæ ·æœ¬(<4m/s)æƒé‡: {sample_weights[y_train < 4][0]:.1f}")
    print(f"3-4m/sæ ·æœ¬æƒé‡: {sample_weights[(y_train >= 3) & (y_train < 4)][0]:.1f}" if ((y_train >= 3) & (y_train < 4)).sum() > 0 else "3-4m/sæ ·æœ¬æƒé‡: æ— æ ·æœ¬")
    print(f"å…¶ä»–æ ·æœ¬æƒé‡: {sample_weights[y_train >= 4][0]:.1f}")
    
    # LightGBMå‚æ•°
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'random_state': 42
    }
    
    # è®­ç»ƒåŠ æƒæ¨¡å‹
    train_data = lgb.Dataset(X_train, label=y_train, weight=sample_weights)
    valid_data = lgb.Dataset(X_test, label=y_test)
    
    model_weighted = lgb.train(
        params,
        train_data,
        valid_sets=[valid_data],
        num_boost_round=1000,
        callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(0)]
    )
    
    # é¢„æµ‹
    y_pred_weighted = model_weighted.predict(X_test, num_iteration=model_weighted.best_iteration)
    
    return model_weighted, y_pred_weighted

def strategy_2_ensemble_modeling(X_train, y_train, X_test, y_test):
    """ç­–ç•¥2: åˆ†å±‚é›†æˆå»ºæ¨¡ - ä½é£é€Ÿå’Œé«˜é£é€Ÿåˆ†åˆ«å»ºæ¨¡"""
    print("\nğŸ¯ ç­–ç•¥2: åˆ†å±‚é›†æˆå»ºæ¨¡")
    print("-" * 40)
    
    threshold = 4.0
    
    # åˆ†ç¦»è®­ç»ƒæ•°æ®
    low_wind_mask_train = y_train < threshold
    high_wind_mask_train = y_train >= threshold
    
    X_train_low = X_train[low_wind_mask_train]
    y_train_low = y_train[low_wind_mask_train]
    X_train_high = X_train[high_wind_mask_train]
    y_train_high = y_train[high_wind_mask_train]
    
    print(f"ä½é£é€Ÿè®­ç»ƒæ ·æœ¬: {len(X_train_low)}")
    print(f"é«˜é£é€Ÿè®­ç»ƒæ ·æœ¬: {len(X_train_high)}")
    
    # LightGBMå‚æ•°
    params_low = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 15,  # ä½é£é€Ÿæ¨¡å‹æ›´ç®€å•
        'learning_rate': 0.1,
        'feature_fraction': 0.8,
        'verbose': -1,
        'random_state': 42
    }
    
    params_high = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'verbose': -1,
        'random_state': 42
    }
    
    # è®­ç»ƒä½é£é€Ÿæ¨¡å‹
    train_data_low = lgb.Dataset(X_train_low, label=y_train_low)
    model_low = lgb.train(
        params_low,
        train_data_low,
        num_boost_round=500,
        callbacks=[lgb.log_evaluation(0)]
    )
    
    # è®­ç»ƒé«˜é£é€Ÿæ¨¡å‹
    train_data_high = lgb.Dataset(X_train_high, label=y_train_high)
    model_high = lgb.train(
        params_high,
        train_data_high,
        num_boost_round=1000,
        callbacks=[lgb.log_evaluation(0)]
    )
    
    # é¢„æµ‹ï¼šæ ¹æ®EC/GFSå¹³å‡å€¼é€‰æ‹©æ¨¡å‹
    ec_gfs_mean_test = X_test[:, 0]  # å‡è®¾ç¬¬ä¸€ä¸ªç‰¹å¾æ˜¯ec_gfs_mean
    
    y_pred_ensemble = np.zeros(len(X_test))
    low_wind_mask_test = ec_gfs_mean_test < threshold
    high_wind_mask_test = ec_gfs_mean_test >= threshold
    
    if low_wind_mask_test.sum() > 0:
        y_pred_ensemble[low_wind_mask_test] = model_low.predict(X_test[low_wind_mask_test])
    
    if high_wind_mask_test.sum() > 0:
        y_pred_ensemble[high_wind_mask_test] = model_high.predict(X_test[high_wind_mask_test])
    
    print(f"ä½¿ç”¨ä½é£é€Ÿæ¨¡å‹é¢„æµ‹çš„æ ·æœ¬æ•°: {low_wind_mask_test.sum()}")
    print(f"ä½¿ç”¨é«˜é£é€Ÿæ¨¡å‹é¢„æµ‹çš„æ ·æœ¬æ•°: {high_wind_mask_test.sum()}")
    
    return (model_low, model_high), y_pred_ensemble

def strategy_3_distribution_matching(y_test, y_pred_original, y_train):
    """ç­–ç•¥3: åˆ†å¸ƒåŒ¹é…æ ¡æ­£"""
    print("\nğŸ¯ ç­–ç•¥3: åˆ†å¸ƒåŒ¹é…æ ¡æ­£")
    print("-" * 40)
    
    # è®¡ç®—è§‚æµ‹æ•°æ®çš„åˆ†ä½æ•°
    obs_percentiles = np.percentile(y_train, np.arange(0, 101, 1))
    
    # å¯¹é¢„æµ‹å€¼è¿›è¡Œåˆ†ä½æ•°æ˜ å°„
    y_pred_corrected = np.zeros_like(y_pred_original)
    
    for i, pred in enumerate(y_pred_original):
        # æ‰¾åˆ°é¢„æµ‹å€¼åœ¨é¢„æµ‹åˆ†å¸ƒä¸­çš„åˆ†ä½æ•°
        pred_percentile = stats.percentileofscore(y_pred_original, pred)
        
        # æ˜ å°„åˆ°è§‚æµ‹åˆ†å¸ƒçš„å¯¹åº”åˆ†ä½æ•°
        if pred_percentile <= 0:
            y_pred_corrected[i] = obs_percentiles[0]
        elif pred_percentile >= 100:
            y_pred_corrected[i] = obs_percentiles[100]
        else:
            # çº¿æ€§æ’å€¼
            lower_idx = int(pred_percentile)
            upper_idx = min(lower_idx + 1, 100)
            weight = pred_percentile - lower_idx
            
            y_pred_corrected[i] = (obs_percentiles[lower_idx] * (1 - weight) + 
                                  obs_percentiles[upper_idx] * weight)
    
    print(f"åŸå§‹é¢„æµ‹æœ€å°å€¼: {y_pred_original.min():.2f}")
    print(f"æ ¡æ­£åé¢„æµ‹æœ€å°å€¼: {y_pred_corrected.min():.2f}")
    print(f"è§‚æµ‹æœ€å°å€¼: {y_test.min():.2f}")
    
    return y_pred_corrected

def strategy_4_quantile_regression(X_train, y_train, X_test, y_test):
    """ç­–ç•¥4: åˆ†ä½æ•°å›å½’ - ç›´æ¥ä¼˜åŒ–ä½åˆ†ä½æ•°é¢„æµ‹"""
    print("\nğŸ¯ ç­–ç•¥4: åˆ†ä½æ•°å›å½’")
    print("-" * 40)
    
    # ä½¿ç”¨å¤šä¸ªåˆ†ä½æ•°è®­ç»ƒ
    quantiles = [0.1, 0.3, 0.5, 0.7, 0.9]
    models = {}
    predictions = {}
    
    for q in quantiles:
        print(f"è®­ç»ƒ {q:.1f} åˆ†ä½æ•°æ¨¡å‹...")
        
        # åˆ†ä½æ•°æŸå¤±å‡½æ•°å‚æ•°
        params = {
            'objective': 'quantile',
            'alpha': q,
            'metric': 'quantile',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'verbose': -1,
            'random_state': 42
        }
        
        train_data = lgb.Dataset(X_train, label=y_train)
        
        model = lgb.train(
            params,
            train_data,
            num_boost_round=500,
            callbacks=[lgb.log_evaluation(0)]
        )
        
        models[q] = model
        predictions[q] = model.predict(X_test)
    
    # ä½¿ç”¨ä¸­ä½æ•°ä½œä¸ºä¸»è¦é¢„æµ‹
    y_pred_quantile = predictions[0.5]
    
    # ä½†å¯¹äºä½é£é€ŸåŒºé—´ï¼Œä½¿ç”¨æ›´ä½çš„åˆ†ä½æ•°
    low_wind_adjustment = np.where(
        (predictions[0.5] < 4) & (y_test < 4),
        predictions[0.3],  # ä½¿ç”¨30%åˆ†ä½æ•°
        predictions[0.5]   # ä½¿ç”¨50%åˆ†ä½æ•°ï¼ˆä¸­ä½æ•°ï¼‰
    )
    
    print(f"ä½¿ç”¨30%åˆ†ä½æ•°è°ƒæ•´çš„æ ·æœ¬æ•°: {((predictions[0.5] < 4) & (y_test < 4)).sum()}")
    
    return models, y_pred_quantile, low_wind_adjustment

def strategy_5_residual_correction(y_test, y_pred_original, ec_baseline):
    """ç­–ç•¥5: æ®‹å·®å­¦ä¹ æ ¡æ­£"""
    print("\nğŸ¯ ç­–ç•¥5: æ®‹å·®å­¦ä¹ æ ¡æ­£")
    print("-" * 40)
    
    # è®¡ç®—å„é£é€ŸåŒºé—´çš„ç³»ç»Ÿæ€§åå·®
    wind_ranges = [(0, 2), (2, 3), (3, 4), (4, 6), (6, 8), (8, 15)]
    corrections = {}
    
    for low, high in wind_ranges:
        mask = (y_test >= low) & (y_test < high)
        if mask.sum() > 10:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ ·æœ¬
            obs_mean = y_test[mask].mean()
            pred_mean = y_pred_original[mask].mean()
            correction = obs_mean - pred_mean
            corrections[(low, high)] = correction
            print(f"{low}-{high}m/s: åå·®={correction:.3f}")
    
    # åº”ç”¨åˆ†åŒºé—´æ ¡æ­£
    y_pred_corrected = y_pred_original.copy()
    
    for (low, high), correction in corrections.items():
        mask = (y_pred_original >= low) & (y_pred_original < high)
        y_pred_corrected[mask] += correction
    
    # ç¡®ä¿é¢„æµ‹å€¼ä¸ä¸ºè´Ÿ
    y_pred_corrected = np.maximum(y_pred_corrected, 0)
    
    return y_pred_corrected

def evaluate_all_strategies(strategies_results, y_test, ec_baseline):
    """è¯„ä¼°æ‰€æœ‰ç­–ç•¥çš„æ•ˆæœ"""
    print("\nğŸ“Š æ‰€æœ‰ç­–ç•¥æ•ˆæœå¯¹æ¯”")
    print("=" * 80)
    
    results_summary = []
    
    # æ·»åŠ åŸºçº¿
    corr_ec, _ = pearsonr(y_test, ec_baseline)
    rmse_ec = np.sqrt(mean_squared_error(y_test, ec_baseline))
    
    # ä½é£é€ŸåŒºé—´è¯„ä¼°
    low_wind_mask_obs = y_test < 4
    if low_wind_mask_obs.sum() > 0:
        low_wind_rmse_ec = np.sqrt(mean_squared_error(y_test[low_wind_mask_obs], ec_baseline[low_wind_mask_obs]))
    else:
        low_wind_rmse_ec = rmse_ec
    
    results_summary.append({
        'Strategy': 'EC_Baseline',
        'Correlation': corr_ec,
        'RMSE': rmse_ec,
        'Low_Wind_Samples(<4)': (ec_baseline < 4).sum(),
        'Low_Wind_Pct(%)': (ec_baseline < 4).mean() * 100,
        'Low_Wind_RMSE': low_wind_rmse_ec
    })
    
    # è¯„ä¼°å„ç­–ç•¥
    for strategy_name, y_pred in strategies_results.items():
        corr, _ = pearsonr(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        
        # ä½é£é€ŸåŒºé—´è¯„ä¼°
        if low_wind_mask_obs.sum() > 0:
            low_wind_rmse = np.sqrt(mean_squared_error(y_test[low_wind_mask_obs], y_pred[low_wind_mask_obs]))
        else:
            low_wind_rmse = rmse
        
        results_summary.append({
            'Strategy': strategy_name,
            'Correlation': corr,
            'RMSE': rmse,
            'Low_Wind_Samples(<4)': (y_pred < 4).sum(),
            'Low_Wind_Pct(%)': (y_pred < 4).mean() * 100,
            'Low_Wind_RMSE': low_wind_rmse
        })
    
    # åˆ›å»ºå¯¹æ¯”è¡¨
    df_results = pd.DataFrame(results_summary)
    
    print(f"{'ç­–ç•¥':<25} {'ç›¸å…³ç³»æ•°':<10} {'æ€»RMSE':<10} {'<4m/sæ ·æœ¬':<12} {'<4m/så æ¯”%':<12} {'ä½é£é€ŸRMSE':<12}")
    print("-" * 95)
    
    for _, row in df_results.iterrows():
        print(f"{row['Strategy']:<25} {row['Correlation']:<10.4f} {row['RMSE']:<10.4f} "
              f"{row['Low_Wind_Samples(<4)']:<12.0f} {row['Low_Wind_Pct(%)']:<12.1f} {row['Low_Wind_RMSE']:<12.4f}")
    
    return df_results

if __name__ == "__main__":
    strategies_results, results_df, output_dir = main()
    print(f"\nâœ… åˆ†æå®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")