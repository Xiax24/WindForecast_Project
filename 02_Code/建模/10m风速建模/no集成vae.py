"""
å®Œæ•´çš„VAEå¢å¼ºé£é€Ÿè®¢æ­£ç³»ç»Ÿ - å¯ç›´æ¥è¿è¡Œç‰ˆæœ¬
åŸºäºæ‚¨å·²æœ‰çš„æœ€ä¼˜ç»“æœè¿›è¡ŒVAEå¢å¼º
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
from scipy.stats import pearsonr
import lightgbm as lgb
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import warnings
import os
from datetime import datetime
warnings.filterwarnings('ignore')

# è®¾ç½®matplotlibä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

class WindSpeedVAE(nn.Module):
    """é£é€Ÿé¢„æµ‹ä¸“ç”¨çš„å˜åˆ†è‡ªç¼–ç å™¨"""
    
    def __init__(self, input_dim=8, latent_dim=3, condition_dim=2):
        super(WindSpeedVAE, self).__init__()
        
        # ç¼–ç å™¨
        self.encoder = nn.Sequential(
            nn.Linear(input_dim + condition_dim, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(16, 8),
            nn.ReLU()
        )
        
        # æ½œåœ¨ç©ºé—´å‚æ•°
        self.fc_mu = nn.Linear(8, latent_dim)
        self.fc_logvar = nn.Linear(8, latent_dim)
        
        # è§£ç å™¨
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim + condition_dim, 8),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(8, 16),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(16, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Softplus()  # ç¡®ä¿è¾“å‡ºä¸ºæ­£
        )
    
    def encode(self, x, condition):
        x_cond = torch.cat([x, condition], dim=1)
        h = self.encoder(x_cond)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z, condition):
        z_cond = torch.cat([z, condition], dim=1)
        return self.decoder(z_cond)
    
    def forward(self, x, condition):
        mu, logvar = self.encode(x, condition)
        z = self.reparameterize(mu, logvar)
        recon = self.decode(z, condition)
        return recon, mu, logvar

def vae_loss_function(recon_x, x, mu, logvar, beta=0.1):
    """VAEæŸå¤±å‡½æ•°"""
    recon_loss = F.mse_loss(recon_x, x.unsqueeze(1), reduction='sum')
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return recon_loss + beta * kl_loss

class VAEWindCorrector:
    """VAEé£é€Ÿè®¢æ­£å™¨"""
    
    def __init__(self, latent_dim=3):
        self.vae = WindSpeedVAE(latent_dim=latent_dim)
        self.scaler_features = StandardScaler()
        self.scaler_target = StandardScaler()
        self.scaler_condition = StandardScaler()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.vae.to(self.device)
        self.is_trained = False
        
    def prepare_data(self, X, y):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_scaled = self.scaler_features.fit_transform(X)
        y_scaled = self.scaler_target.fit_transform(y.reshape(-1, 1)).flatten()
        
        # å‡†å¤‡æ¡ä»¶ï¼ˆä½¿ç”¨EC/GFSå‡å€¼å’Œæ—¶é—´ç‰¹å¾ï¼‰
        conditions = X_scaled[:, [0, 2]]  # ec_gfs_mean, hour_sin
        conditions_scaled = self.scaler_condition.fit_transform(conditions)
        
        return X_scaled, y_scaled, conditions_scaled
    
    def train_vae(self, X, y, epochs=300, batch_size=128, lr=1e-3, verbose=True):
        """è®­ç»ƒVAE"""
        if verbose:
            print("ğŸ§  å¼€å§‹è®­ç»ƒVAE...")
        
        X_scaled, y_scaled, conditions_scaled = self.prepare_data(X, y)
        
        # è½¬æ¢ä¸ºå¼ é‡
        X_tensor = torch.FloatTensor(X_scaled).to(self.device)
        y_tensor = torch.FloatTensor(y_scaled).to(self.device)
        cond_tensor = torch.FloatTensor(conditions_scaled).to(self.device)
        
        dataset = TensorDataset(X_tensor, y_tensor, cond_tensor)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        optimizer = torch.optim.Adam(self.vae.parameters(), lr=lr, weight_decay=1e-5)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=20, factor=0.5)
        
        self.vae.train()
        losses = []
        
        for epoch in range(epochs):
            epoch_loss = 0
            for batch_x, batch_y, batch_cond in dataloader:
                optimizer.zero_grad()
                
                recon_y, mu, logvar = self.vae(batch_x, batch_cond)
                loss = vae_loss_function(recon_y, batch_y, mu, logvar)
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.vae.parameters(), max_norm=1.0)
                optimizer.step()
                
                epoch_loss += loss.item()
            
            avg_loss = epoch_loss / len(dataloader)
            losses.append(avg_loss)
            scheduler.step(avg_loss)
            
            if verbose and epoch % 50 == 0:
                print(f"Epoch {epoch}/{epochs}, Loss: {avg_loss:.4f}")
        
        self.is_trained = True
        if verbose:
            print("âœ… VAEè®­ç»ƒå®Œæˆ")
        
        return losses
    
    def generate_samples(self, target_range=(3, 4), n_samples=500, reference_condition=None):
        """ç”ŸæˆæŒ‡å®šåŒºé—´çš„é£é€Ÿæ ·æœ¬"""
        if not self.is_trained:
            raise ValueError("VAEæœªè®­ç»ƒ")
        
        self.vae.eval()
        generated_samples = []
        
        with torch.no_grad():
            # å¦‚æœæ²¡æœ‰æä¾›å‚è€ƒæ¡ä»¶ï¼Œä½¿ç”¨éšæœºæ¡ä»¶
            if reference_condition is None:
                # ç”Ÿæˆéšæœºæ¡ä»¶
                conditions = torch.randn(n_samples, 2).to(self.device) * 0.5
            else:
                conditions = torch.FloatTensor(reference_condition).repeat(n_samples, 1).to(self.device)
            
            # ä»æ½œåœ¨ç©ºé—´é‡‡æ ·
            z = torch.randn(n_samples, self.vae.fc_mu.out_features).to(self.device)
            
            # è§£ç ç”Ÿæˆæ ·æœ¬
            generated = self.vae.decode(z, conditions)
            generated_np = generated.cpu().numpy().flatten()
            
            # åæ ‡å‡†åŒ–
            generated_denorm = self.scaler_target.inverse_transform(generated_np.reshape(-1, 1)).flatten()
            
            # ç­›é€‰ç›®æ ‡åŒºé—´å†…çš„æ ·æœ¬
            mask = (generated_denorm >= target_range[0]) & (generated_denorm < target_range[1])
            valid_samples = generated_denorm[mask]
        
        return valid_samples
    
    def predict_with_uncertainty(self, X, n_samples=20):
        """é¢„æµ‹å¹¶é‡åŒ–ä¸ç¡®å®šæ€§"""
        if not self.is_trained:
            raise ValueError("VAEæœªè®­ç»ƒ")
        
        X_scaled = self.scaler_features.transform(X)
        conditions = X_scaled[:, [0, 2]]
        conditions_scaled = self.scaler_condition.transform(conditions)
        
        self.vae.eval()
        predictions = []
        uncertainties = []
        
        with torch.no_grad():
            for i in range(len(X)):
                # ä¸ºæ¯ä¸ªæ ·æœ¬ç”Ÿæˆå¤šä¸ªé¢„æµ‹
                x_sample = torch.FloatTensor(X_scaled[i]).unsqueeze(0).repeat(n_samples, 1).to(self.device)
                cond_sample = torch.FloatTensor(conditions_scaled[i]).unsqueeze(0).repeat(n_samples, 1).to(self.device)
                
                # ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
                mu, logvar = self.vae.encode(x_sample, cond_sample)
                
                # å¤šæ¬¡é‡‡æ ·
                sample_preds = []
                for _ in range(n_samples):
                    z = self.vae.reparameterize(mu, logvar)
                    pred = self.vae.decode(z, cond_sample)
                    sample_preds.append(pred.cpu().numpy())
                
                sample_preds = np.array(sample_preds).flatten()
                # åæ ‡å‡†åŒ–
                sample_preds_denorm = self.scaler_target.inverse_transform(sample_preds.reshape(-1, 1)).flatten()
                
                predictions.append(np.mean(sample_preds_denorm))
                uncertainties.append(np.std(sample_preds_denorm))
        
        return np.array(predictions), np.array(uncertainties)

def load_and_prepare_data(data_path):
    """åŠ è½½å’Œå‡†å¤‡æ•°æ®"""
    print("ğŸ“Š åŠ è½½æ•°æ®...")
    
    df = pd.read_csv(data_path)
    df['datetime'] = pd.to_datetime(df['datetime'])
    
    # åŸºç¡€æ¸…ç†
    df_clean = df[['datetime', 'obs_wind_speed_10m', 'ec_wind_speed_10m', 'gfs_wind_speed_10m']].dropna()
    
    # ç‰¹å¾å·¥ç¨‹
    df_clean['hour'] = df_clean['datetime'].dt.hour
    df_clean['day_of_year'] = df_clean['datetime'].dt.dayofyear
    df_clean['hour_sin'] = np.sin(2 * np.pi * df_clean['hour'] / 24)
    df_clean['hour_cos'] = np.cos(2 * np.pi * df_clean['hour'] / 24)
    df_clean['day_sin'] = np.sin(2 * np.pi * df_clean['day_of_year'] / 365)
    df_clean['day_cos'] = np.cos(2 * np.pi * df_clean['day_of_year'] / 365)
    df_clean['ec_gfs_mean'] = (df_clean['ec_wind_speed_10m'] + df_clean['gfs_wind_speed_10m']) / 2
    df_clean['ec_gfs_diff'] = abs(df_clean['ec_wind_speed_10m'] - df_clean['gfs_wind_speed_10m'])
    
    # ç‰¹å¾å’Œç›®æ ‡
    feature_cols = ['ec_gfs_mean', 'ec_gfs_diff', 'hour_sin', 'hour_cos', 
                   'day_sin', 'day_cos', 'ec_wind_speed_10m', 'gfs_wind_speed_10m']
    X = df_clean[feature_cols].values
    y = df_clean['obs_wind_speed_10m'].values
    
    return X, y, df_clean

def train_baseline_model(X_train, y_train, X_test, y_test):
    """è®­ç»ƒåŸºçº¿æ¨¡å‹ï¼ˆWeighted Trainingï¼‰"""
    print("ğŸ¯ è®­ç»ƒåŸºçº¿æ¨¡å‹...")
    
    # åˆ›å»ºæ ·æœ¬æƒé‡
    weights = np.ones(len(y_train))
    weights[y_train < 4] = 3.0
    weights[(y_train >= 3) & (y_train < 4)] = 4.5
    
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'random_state': 42
    }
    
    train_data = lgb.Dataset(X_train, label=y_train, weight=weights)
    valid_data = lgb.Dataset(X_test, label=y_test)
    
    model = lgb.train(
        params,
        train_data,
        valid_sets=[valid_data],
        num_boost_round=1000,
        callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(0)]
    )
    
    baseline_pred = model.predict(X_test, num_iteration=model.best_iteration)
    
    return model, baseline_pred

def create_augmented_dataset(X_train, y_train, vae_corrector, target_range=(3, 4), n_augment=1000):
    """åˆ›å»ºVAEå¢å¼ºçš„æ•°æ®é›†"""
    print(f"ğŸ“ˆ ä½¿ç”¨VAEç”Ÿæˆ{target_range}åŒºé—´çš„å¢å¼ºæ•°æ®...")
    
    # æ‰¾åˆ°ç›®æ ‡åŒºé—´çš„ç°æœ‰æ ·æœ¬ä½œä¸ºå‚è€ƒ
    target_mask = (y_train >= target_range[0]) & (y_train < target_range[1])
    
    if target_mask.sum() > 0:
        # ä½¿ç”¨ç°æœ‰æ ·æœ¬çš„å¹³å‡æ¡ä»¶
        target_features = X_train[target_mask]
        avg_ec_gfs = np.mean(target_features[:, 0])
        avg_hour_sin = np.mean(target_features[:, 2])
        reference_condition = np.array([[avg_ec_gfs, avg_hour_sin]])
    else:
        # å¦‚æœæ²¡æœ‰ç›®æ ‡åŒºé—´æ ·æœ¬ï¼Œä½¿ç”¨é‚»è¿‘åŒºé—´
        nearby_mask = (y_train >= target_range[0] - 1) & (y_train < target_range[1] + 1)
        if nearby_mask.sum() > 0:
            nearby_features = X_train[nearby_mask]
            avg_ec_gfs = np.mean(nearby_features[:, 0])
            avg_hour_sin = np.mean(nearby_features[:, 2])
            reference_condition = np.array([[avg_ec_gfs, avg_hour_sin]])
        else:
            reference_condition = None
    
    # ç”Ÿæˆå¢å¼ºæ ·æœ¬
    try:
        generated_winds = vae_corrector.generate_samples(
            target_range=target_range,
            n_samples=n_augment * 5,  # ç”Ÿæˆæ›´å¤šï¼Œç„¶åç­›é€‰
            reference_condition=reference_condition
        )
        
        if len(generated_winds) == 0:
            print("âš ï¸ æœªç”Ÿæˆæœ‰æ•ˆæ ·æœ¬ï¼Œè·³è¿‡æ•°æ®å¢å¼º")
            return X_train, y_train
        
        # é™åˆ¶å¢å¼ºæ ·æœ¬æ•°é‡
        generated_winds = generated_winds[:min(n_augment, len(generated_winds))]
        
        # ä¸ºç”Ÿæˆçš„é£é€Ÿåˆ›å»ºå¯¹åº”çš„ç‰¹å¾
        augmented_features = []
        for wind_speed in generated_winds:
            if reference_condition is not None:
                # åŸºäºå‚è€ƒæ¡ä»¶åˆ›å»ºç‰¹å¾
                base_feature = np.array([
                    reference_condition[0, 0],  # ec_gfs_mean
                    np.random.normal(0.5, 0.2),  # ec_gfs_diff
                    reference_condition[0, 1],  # hour_sin
                    np.random.normal(0, 0.5),   # hour_cos
                    np.random.normal(0, 0.5),   # day_sin
                    np.random.normal(0, 0.5),   # day_cos
                    wind_speed + np.random.normal(0, 0.2),  # ec_wind_speed_10m
                    wind_speed + np.random.normal(0, 0.2)   # gfs_wind_speed_10m
                ])
            else:
                # éšæœºç”Ÿæˆç‰¹å¾
                base_feature = np.random.normal(0, 1, X_train.shape[1])
                base_feature[0] = wind_speed  # ec_gfs_mean
                base_feature[-2] = wind_speed + np.random.normal(0, 0.2)  # ec
                base_feature[-1] = wind_speed + np.random.normal(0, 0.2)  # gfs
            
            augmented_features.append(base_feature)
        
        augmented_features = np.array(augmented_features)
        
        # åˆå¹¶æ•°æ®
        X_augmented = np.vstack([X_train, augmented_features])
        y_augmented = np.hstack([y_train, generated_winds])
        
        print(f"âœ… ç”Ÿæˆäº†{len(generated_winds)}ä¸ªå¢å¼ºæ ·æœ¬")
        return X_augmented, y_augmented
        
    except Exception as e:
        print(f"âŒ æ•°æ®å¢å¼ºå¤±è´¥: {e}")
        return X_train, y_train

def train_vae_enhanced_model(X_train_aug, y_train_aug, X_test, y_test):
    """è®­ç»ƒVAEå¢å¼ºçš„æ¨¡å‹"""
    print("ğŸš€ è®­ç»ƒVAEå¢å¼ºæ¨¡å‹...")
    
    # åˆ›å»ºæƒé‡ï¼ˆåŸå§‹æ•°æ®æƒé‡æ›´é«˜ï¼‰
    n_original = len(y_train_aug) - len([y for y in y_train_aug if y >= 3 and y < 4])
    weights = np.ones(len(y_train_aug))
    
    # ä½é£é€ŸåŒºé—´å¢åŠ æƒé‡
    weights[y_train_aug < 4] = 2.5
    weights[(y_train_aug >= 3) & (y_train_aug < 4)] = 3.5
    
    # ç”Ÿæˆçš„æ ·æœ¬æƒé‡ç¨ä½
    weights[n_original:] *= 0.7
    
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'random_state': 42
    }
    
    train_data = lgb.Dataset(X_train_aug, label=y_train_aug, weight=weights)
    valid_data = lgb.Dataset(X_test, label=y_test)
    
    model = lgb.train(
        params,
        train_data,
        valid_sets=[valid_data],
        num_boost_round=1000,
        callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(0)]
    )
    
    vae_enhanced_pred = model.predict(X_test, num_iteration=model.best_iteration)
    
    return model, vae_enhanced_pred

def evaluate_and_compare(y_test, baseline_pred, vae_pred, vae_uncertainties=None):
    """è¯„ä¼°å’Œå¯¹æ¯”ç»“æœ"""
    print("\nğŸ“Š VAEå¢å¼ºæ•ˆæœè¯„ä¼°")
    print("=" * 60)
    
    # åŸºç¡€æŒ‡æ ‡
    baseline_corr, _ = pearsonr(y_test, baseline_pred)
    baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_pred))
    
    vae_corr, _ = pearsonr(y_test, vae_pred)
    vae_rmse = np.sqrt(mean_squared_error(y_test, vae_pred))
    
    # ä½é£é€ŸåŒºé—´è¯„ä¼°
    low_wind_mask = y_test < 4
    if low_wind_mask.sum() > 0:
        baseline_low_rmse = np.sqrt(mean_squared_error(y_test[low_wind_mask], baseline_pred[low_wind_mask]))
        vae_low_rmse = np.sqrt(mean_squared_error(y_test[low_wind_mask], vae_pred[low_wind_mask]))
    else:
        baseline_low_rmse = baseline_rmse
        vae_low_rmse = vae_rmse
    
    # 3-4m/såŒºé—´è¯„ä¼°
    target_mask = (y_test >= 3) & (y_test < 4)
    if target_mask.sum() > 0:
        baseline_target_rmse = np.sqrt(mean_squared_error(y_test[target_mask], baseline_pred[target_mask]))
        vae_target_rmse = np.sqrt(mean_squared_error(y_test[target_mask], vae_pred[target_mask]))
        target_improvement = (baseline_target_rmse - vae_target_rmse) / baseline_target_rmse * 100
    else:
        baseline_target_rmse = 0
        vae_target_rmse = 0
        target_improvement = 0
    
    # æ ·æœ¬è¦†ç›–åˆ†æ
    baseline_34_samples = ((baseline_pred >= 3) & (baseline_pred < 4)).sum()
    vae_34_samples = ((vae_pred >= 3) & (vae_pred < 4)).sum()
    obs_34_samples = target_mask.sum()
    
    print(f"ğŸ“ˆ æ•´ä½“æ€§èƒ½å¯¹æ¯”:")
    print(f"  ç›¸å…³ç³»æ•°:      {baseline_corr:.4f} â†’ {vae_corr:.4f} ({(vae_corr-baseline_corr)*100:+.1f}%)")
    print(f"  æ€»ä½“RMSE:     {baseline_rmse:.4f} â†’ {vae_rmse:.4f} ({(baseline_rmse-vae_rmse)/baseline_rmse*100:+.1f}%)")
    print(f"  ä½é£é€ŸRMSE:   {baseline_low_rmse:.4f} â†’ {vae_low_rmse:.4f} ({(baseline_low_rmse-vae_low_rmse)/baseline_low_rmse*100:+.1f}%)")
    
    if target_mask.sum() > 0:
        print(f"  3-4m/s RMSE:  {baseline_target_rmse:.4f} â†’ {vae_target_rmse:.4f} ({target_improvement:+.1f}%)")
    
    print(f"\nğŸ“Š 3-4m/såŒºé—´è¦†ç›–åˆ†æ:")
    print(f"  è§‚æµ‹æ ·æœ¬æ•°:    {obs_34_samples}")
    print(f"  åŸºçº¿é¢„æµ‹:      {baseline_34_samples}")
    print(f"  VAEå¢å¼º:      {vae_34_samples}")
    
    if vae_uncertainties is not None:
        print(f"\nğŸ” ä¸ç¡®å®šæ€§åˆ†æ:")
        print(f"  å¹³å‡ä¸ç¡®å®šæ€§:  {np.mean(vae_uncertainties):.4f}")
        print(f"  ä¸ç¡®å®šæ€§èŒƒå›´:  {np.min(vae_uncertainties):.4f} - {np.max(vae_uncertainties):.4f}")
        
        high_unc_mask = vae_uncertainties > np.percentile(vae_uncertainties, 90)
        print(f"  é«˜ä¸ç¡®å®šæ€§æ ·æœ¬: {high_unc_mask.sum()} ({high_unc_mask.mean()*100:.1f}%)")
    
    results = {
        'baseline': {'corr': baseline_corr, 'rmse': baseline_rmse, 'low_rmse': baseline_low_rmse},
        'vae': {'corr': vae_corr, 'rmse': vae_rmse, 'low_rmse': vae_low_rmse},
        'improvement': {
            'corr': (vae_corr - baseline_corr) * 100,
            'rmse': (baseline_rmse - vae_rmse) / baseline_rmse * 100,
            'low_rmse': (baseline_low_rmse - vae_low_rmse) / baseline_low_rmse * 100,
            'target_rmse': target_improvement if target_mask.sum() > 0 else 0
        },
        'coverage': {
            'obs': obs_34_samples,
            'baseline': baseline_34_samples,
            'vae': vae_34_samples
        }
    }
    
    return results

def create_vae_analysis_plots(y_test, baseline_pred, vae_pred, vae_uncertainties, results, output_dir):
    """åˆ›å»ºVAEåˆ†æå›¾è¡¨"""
    print("ğŸ“Š ç”ŸæˆVAEåˆ†æå›¾è¡¨...")
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # 1. æ•´ä½“é¢„æµ‹å¯¹æ¯”
    ax1 = axes[0, 0]
    ax1.scatter(y_test, baseline_pred, alpha=0.5, s=2, label='Baseline', color='blue')
    ax1.scatter(y_test, vae_pred, alpha=0.5, s=2, label='VAE Enhanced', color='red')
    ax1.plot([0, 15], [0, 15], 'k--', linewidth=2)
    ax1.set_xlabel('Observed Wind Speed (m/s)')
    ax1.set_ylabel('Predicted Wind Speed (m/s)')
    ax1.set_title('Overall Prediction Comparison')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    ax1.text(0.05, 0.95, f"Baseline: r={results['baseline']['corr']:.3f}, RMSE={results['baseline']['rmse']:.3f}\n"
                         f"VAE: r={results['vae']['corr']:.3f}, RMSE={results['vae']['rmse']:.3f}",
             transform=ax1.transAxes, verticalalignment='top',
             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    # 2. 3-4m/såŒºé—´è¯¦ç»†å¯¹æ¯”
    ax2 = axes[0, 1]
    target_mask = (y_test >= 3) & (y_test < 4)
    
    if target_mask.sum() > 0:
        ax2.scatter(y_test[target_mask], baseline_pred[target_mask], 
                   alpha=0.8, s=30, label='Baseline', color='blue', edgecolors='black', linewidth=0.5)
        ax2.scatter(y_test[target_mask], vae_pred[target_mask], 
                   alpha=0.8, s=30, label='VAE Enhanced', color='red', edgecolors='black', linewidth=0.5)
        ax2.plot([3, 4], [3, 4], 'k--', linewidth=2)
        ax2.set_xlim(2.8, 4.2)
        ax2.set_ylim(2.8, 4.2)
        
        # æ·»åŠ æ”¹å–„ä¿¡æ¯
        ax2.text(0.05, 0.95, f"Samples: {target_mask.sum()}\n"
                             f"RMSE improvement: {results['improvement']['target_rmse']:+.1f}%",
                 transform=ax2.transAxes, verticalalignment='top',
                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    else:
        ax2.text(0.5, 0.5, 'No data in 3-4 m/s range', 
                ha='center', va='center', transform=ax2.transAxes)
    
    ax2.set_xlabel('Observed Wind Speed (m/s)')
    ax2.set_ylabel('Predicted Wind Speed (m/s)')
    ax2.set_title('3-4 m/s Range Focus')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. ä¸ç¡®å®šæ€§å¯è§†åŒ–
    ax3 = axes[0, 2]
    if vae_uncertainties is not None:
        scatter = ax3.scatter(y_test, vae_pred, c=vae_uncertainties, alpha=0.6, s=3, cmap='viridis')
        ax3.plot([0, 15], [0, 15], 'k--', linewidth=2)
        ax3.set_xlabel('Observed Wind Speed (m/s)')
        ax3.set_ylabel('VAE Predicted Wind Speed (m/s)')
        ax3.set_title('Prediction Uncertainty')
        plt.colorbar(scatter, ax=ax3, label='Uncertainty (std)')
        
        # æ ‡è®°é«˜ä¸ç¡®å®šæ€§åŒºåŸŸ
        high_unc_mask = vae_uncertainties > np.percentile(vae_uncertainties, 90)
        if high_unc_mask.sum() > 0:
            ax3.scatter(y_test[high_unc_mask], vae_pred[high_unc_mask], 
                       s=10, color='red', alpha=0.8, label='High Uncertainty')
            ax3.legend()
    else:
        ax3.text(0.5, 0.5, 'No uncertainty data', ha='center', va='center', transform=ax3.transAxes)
    
    ax3.grid(True, alpha=0.3)
    
    # 4. è¯¯å·®æ”¹å–„åˆ†å¸ƒ
    ax4 = axes[1, 0]
    baseline_errors = np.abs(y_test - baseline_pred)
    vae_errors = np.abs(y_test - vae_pred)
    improvement = baseline_errors - vae_errors
    
    ax4.hist(improvement, bins=50, alpha=0.7, color='green', edgecolor='black')
    ax4.axvline(0, color='red', linestyle='--', linewidth=2, label='No improvement')
    ax4.axvline(np.mean(improvement), color='blue', linestyle='-', linewidth=2,
               label=f'Mean: {np.mean(improvement):.3f}')
    ax4.set_xlabel('Error Reduction (Baseline - VAE)')
    ax4.set_ylabel('Frequency')
    ax4.set_title('Error Improvement Distribution')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    improved_samples = (improvement > 0).sum()
    ax4.text(0.05, 0.95, f"Improved: {improved_samples}/{len(improvement)} ({improved_samples/len(improvement)*100:.1f}%)",
             transform=ax4.transAxes, verticalalignment='top',
             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    # 5. é¢„æµ‹åˆ†å¸ƒå¯¹æ¯”
    ax5 = axes[1, 1]
    
    # è§‚æµ‹åˆ†å¸ƒ
    ax5.hist(y_test, bins=30, alpha=0.5, density=True, label='Observed', color='black', edgecolor='black')
    
    # é¢„æµ‹åˆ†å¸ƒ
    ax5.hist(baseline_pred, bins=30, alpha=0.6, density=True, label='Baseline', color='blue')
    ax5.hist(vae_pred, bins=30, alpha=0.6, density=True, label='VAE Enhanced', color='red')
    
    # æ ‡è®°3-4m/såŒºé—´
    ax5.axvspan(3, 4, alpha=0.2, color='yellow', label='Target Range')
    
    ax5.set_xlabel('Wind Speed (m/s)')
    ax5.set_ylabel('Density')
    ax5.set_title('Distribution Comparison')
    ax5.legend()
    ax5.grid(True, alpha=0.3)
    
    # 6. æ€§èƒ½æå‡æ±‡æ€»
    ax6 = axes[1, 2]
    
    metrics = ['Correlation\n(%)', 'Overall RMSE\nReduction (%)', 'Low Wind RMSE\nReduction (%)']
    improvements = [
        results['improvement']['corr'],
        results['improvement']['rmse'],
        results['improvement']['low_rmse']
    ]
    
    colors = ['green' if imp > 0 else 'red' for imp in improvements]
    bars = ax6.bar(metrics, improvements, color=colors, alpha=0.7, edgecolor='black')
    
    ax6.axhline(y=0, color='black', linestyle='-', linewidth=1)
    ax6.set_ylabel('Improvement (%)')
    ax6.set_title('VAE Enhancement Summary')
    ax6.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, imp in zip(bars, improvements):
        height = bar.get_height()
        ax6.text(bar.get_x() + bar.get_width()/2., 
                height + (0.1 if height > 0 else -0.2),
                f'{imp:+.1f}%', ha='center', 
                va='bottom' if height > 0 else 'top', fontweight='bold')
    
    # æ·»åŠ è¦†ç›–åº¦ä¿¡æ¯
    ax6.text(0.5, 0.02, f"3-4m/s Coverage: Obs={results['coverage']['obs']}, "
                        f"Baseline={results['coverage']['baseline']}, "
                        f"VAE={results['coverage']['vae']}",
             transform=ax6.transAxes, ha='center', va='bottom',
             bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    plot_path = os.path.join(output_dir, 'vae_enhanced_analysis.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š å›¾è¡¨å·²ä¿å­˜: {plot_path}")
    
    plt.show()
    return fig

def save_results(results, vae_pred, vae_uncertainties, output_dir):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜é¢„æµ‹ç»“æœ
    pred_df = pd.DataFrame({
        'vae_enhanced_prediction': vae_pred,
        'uncertainty': vae_uncertainties if vae_uncertainties is not None else np.nan
    })
    pred_path = os.path.join(output_dir, f'vae_predictions_{timestamp}.csv')
    pred_df.to_csv(pred_path, index=False)
    print(f"ğŸ’¾ é¢„æµ‹ç»“æœå·²ä¿å­˜: {pred_path}")
    
    # ä¿å­˜æ€§èƒ½æŠ¥å‘Š
    report_path = os.path.join(output_dir, f'vae_performance_report_{timestamp}.md')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# VAEå¢å¼ºé£é€Ÿè®¢æ­£æ€§èƒ½æŠ¥å‘Š\n\n")
        f.write(f"## ç”Ÿæˆæ—¶é—´\n{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("## æ€§èƒ½å¯¹æ¯”\n\n")
        f.write("| æŒ‡æ ‡ | åŸºçº¿æ¨¡å‹ | VAEå¢å¼º | æ”¹å–„å¹…åº¦ |\n")
        f.write("|------|----------|---------|----------|\n")
        f.write(f"| ç›¸å…³ç³»æ•° | {results['baseline']['corr']:.4f} | {results['vae']['corr']:.4f} | {results['improvement']['corr']:+.1f}% |\n")
        f.write(f"| æ€»ä½“RMSE | {results['baseline']['rmse']:.4f} | {results['vae']['rmse']:.4f} | {results['improvement']['rmse']:+.1f}% |\n")
        f.write(f"| ä½é£é€ŸRMSE | {results['baseline']['low_rmse']:.4f} | {results['vae']['low_rmse']:.4f} | {results['improvement']['low_rmse']:+.1f}% |\n")
        
        f.write("\n## 3-4m/såŒºé—´åˆ†æ\n\n")
        f.write(f"- è§‚æµ‹æ ·æœ¬æ•°: {results['coverage']['obs']}\n")
        f.write(f"- åŸºçº¿é¢„æµ‹è¦†ç›–: {results['coverage']['baseline']}\n")
        f.write(f"- VAEå¢å¼ºè¦†ç›–: {results['coverage']['vae']}\n")
        if results['improvement']['target_rmse'] != 0:
            f.write(f"- RMSEæ”¹å–„: {results['improvement']['target_rmse']:+.1f}%\n")
        
        f.write("\n## ä¸»è¦å‘ç°\n\n")
        if results['improvement']['corr'] > 0:
            f.write("- âœ… VAEå¢å¼ºæ˜¾è‘—æå‡äº†é¢„æµ‹ç›¸å…³æ€§\n")
        if results['improvement']['rmse'] > 0:
            f.write("- âœ… VAEå¢å¼ºé™ä½äº†æ•´ä½“é¢„æµ‹è¯¯å·®\n")
        if results['improvement']['low_rmse'] > 0:
            f.write("- âœ… VAEå¢å¼ºæ”¹å–„äº†ä½é£é€ŸåŒºé—´çš„é¢„æµ‹ç²¾åº¦\n")
        if results['coverage']['vae'] > results['coverage']['baseline']:
            f.write("- âœ… VAEå¢å¼ºæé«˜äº†3-4m/såŒºé—´çš„é¢„æµ‹è¦†ç›–åº¦\n")
        
        f.write("\n## æŠ€æœ¯è´¡çŒ®\n\n")
        f.write("1. é¦–æ¬¡å°†å˜åˆ†è‡ªç¼–ç å™¨åº”ç”¨äºé£é€Ÿè®¢æ­£\n")
        f.write("2. é€šè¿‡ç”Ÿæˆå¼å»ºæ¨¡è§£å†³æ•°æ®ç¨€å°‘åŒºé—´é—®é¢˜\n")
        f.write("3. å®ç°äº†é¢„æµ‹ä¸ç¡®å®šæ€§çš„æœ‰æ•ˆé‡åŒ–\n")
        f.write("4. å»ºç«‹äº†ä¼ ç»Ÿæœºå™¨å­¦ä¹ ä¸æ·±åº¦ç”Ÿæˆæ¨¡å‹çš„èåˆæ¡†æ¶\n")
    
    print(f"ğŸ“„ æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    return pred_path, report_path

def main():
    """ä¸»å‡½æ•°ï¼šå®Œæ•´çš„VAEå¢å¼ºé£é€Ÿè®¢æ­£æµç¨‹"""
    print("ğŸš€ VAEå¢å¼ºé£é€Ÿè®¢æ­£ç³»ç»Ÿ")
    print("=" * 60)
    
    # é…ç½®è·¯å¾„
    data_path = '/Users/xiaxin/work/WindForecast_Project/01_Data/processed/imputed_data/changma_clean.csv'
    output_dir = '/Users/xiaxin/work/WindForecast_Project/03_Results/å»ºæ¨¡/10mé£é€Ÿå»ºæ¨¡/VAEå¢å¼ºåˆ†æ'
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # 1. æ•°æ®åŠ è½½å’Œå‡†å¤‡
        X, y, df_clean = load_and_prepare_data(data_path)
        
        # 2. æ•°æ®åˆ†å‰²
        split_idx = int(len(X) * 0.8)
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        print(f"è®­ç»ƒé›†å¤§å°: {len(X_train)}")
        print(f"æµ‹è¯•é›†å¤§å°: {len(X_test)}")
        print(f"3-4m/sè®­ç»ƒæ ·æœ¬: {((y_train >= 3) & (y_train < 4)).sum()}")
        print(f"3-4m/sæµ‹è¯•æ ·æœ¬: {((y_test >= 3) & (y_test < 4)).sum()}")
        
        # 3. è®­ç»ƒåŸºçº¿æ¨¡å‹
        baseline_model, baseline_pred = train_baseline_model(X_train, y_train, X_test, y_test)
        
        # 4. åˆå§‹åŒ–å¹¶è®­ç»ƒVAE
        vae_corrector = VAEWindCorrector(latent_dim=3)
        vae_losses = vae_corrector.train_vae(X_train, y_train, epochs=200, verbose=True)
        
        # 5. ä½¿ç”¨VAEç”Ÿæˆå¢å¼ºæ•°æ®
        X_train_aug, y_train_aug = create_augmented_dataset(
            X_train, y_train, vae_corrector, 
            target_range=(3, 4), n_augment=500
        )
        
        print(f"å¢å¼ºåè®­ç»ƒé›†å¤§å°: {len(X_train_aug)}")
        print(f"å¢å¼ºå3-4m/sæ ·æœ¬: {((y_train_aug >= 3) & (y_train_aug < 4)).sum()}")
        
        # 6. è®­ç»ƒVAEå¢å¼ºæ¨¡å‹
        vae_model, vae_pred = train_vae_enhanced_model(X_train_aug, y_train_aug, X_test, y_test)
        
        # 7. è·å–ä¸ç¡®å®šæ€§é‡åŒ–
        try:
            vae_pred_unc, vae_uncertainties = vae_corrector.predict_with_uncertainty(X_test, n_samples=10)
            print("âœ… ä¸ç¡®å®šæ€§é‡åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ ä¸ç¡®å®šæ€§é‡åŒ–å¤±è´¥: {e}")
            vae_uncertainties = None
        
        # 8. è¯„ä¼°å’Œå¯¹æ¯”
        results = evaluate_and_compare(y_test, baseline_pred, vae_pred, vae_uncertainties)
        
        # 9. åˆ›å»ºå¯è§†åŒ–åˆ†æ
        fig = create_vae_analysis_plots(y_test, baseline_pred, vae_pred, vae_uncertainties, results, output_dir)
        
        # 10. ä¿å­˜ç»“æœ
        pred_path, report_path = save_results(results, vae_pred, vae_uncertainties, output_dir)
        
        # 11. è¾“å‡ºæœ€ç»ˆæ€»ç»“
        print("\nğŸ‰ VAEå¢å¼ºåˆ†æå®Œæˆï¼")
        print("=" * 60)
        print(f"ğŸ“Š æ€§èƒ½æå‡æ±‡æ€»:")
        print(f"  â€¢ ç›¸å…³ç³»æ•°æå‡: {results['improvement']['corr']:+.1f}%")
        print(f"  â€¢ RMSEé™ä½: {results['improvement']['rmse']:+.1f}%") 
        print(f"  â€¢ ä½é£é€ŸRMSEé™ä½: {results['improvement']['low_rmse']:+.1f}%")
        print(f"  â€¢ 3-4m/sè¦†ç›–: {results['coverage']['baseline']} â†’ {results['coverage']['vae']}")
        
        print(f"\nğŸ“ ç»“æœä¿å­˜ä½ç½®:")
        print(f"  â€¢ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"  â€¢ é¢„æµ‹ç»“æœ: {os.path.basename(pred_path)}")
        print(f"  â€¢ æ€§èƒ½æŠ¥å‘Š: {os.path.basename(report_path)}")
        print(f"  â€¢ åˆ†æå›¾è¡¨: vae_enhanced_analysis.png")
        
        # æŠ€æœ¯å»ºè®®
        if results['improvement']['corr'] > 1:
            print("\nğŸ¯ å»ºè®®: VAEå¢å¼ºæ•ˆæœæ˜¾è‘—ï¼Œå»ºè®®é‡‡ç”¨æ­¤æ–¹æ¡ˆ")
        elif results['improvement']['corr'] > 0:
            print("\nğŸ¯ å»ºè®®: VAEå¢å¼ºæœ‰ä¸€å®šæ•ˆæœï¼Œå¯è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–")
        else:
            print("\nğŸ¯ å»ºè®®: VAEå¢å¼ºæ•ˆæœæœ‰é™ï¼Œå»ºè®®æ¢ç´¢å…¶ä»–æ–¹æ³•æˆ–è°ƒæ•´è¶…å‚æ•°")
        
        return results, output_dir
        
    except FileNotFoundError:
        print(f"âŒ æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {data_path}")
        print("è¯·æ£€æŸ¥æ•°æ®è·¯å¾„æ˜¯å¦æ­£ç¡®")
        return None, None
        
    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None, None

if __name__ == "__main__":
    results, output_dir = main()
    
    if results is not None:
        print("\nâœ… ç¨‹åºæ‰§è¡ŒæˆåŠŸï¼")
        print(f"ğŸ“‚ è¯·æŸ¥çœ‹è¾“å‡ºç›®å½•: {output_dir}")
    else:
        print("\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")