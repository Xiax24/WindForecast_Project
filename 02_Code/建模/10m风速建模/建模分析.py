"""
10m Wind Speed Correction using LightGBM + SHAP
ä½¿ç”¨LightGBMå’ŒSHAPè¿›è¡Œ10mé£é€Ÿè®¢æ­£
å¢å¼ºç‰ˆæœ¬ï¼šæ·»åŠ ec_gfs_diffå˜é‡
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy.stats import pearsonr
import lightgbm as lgb
import shap
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ç»˜å›¾å‚æ•°
plt.rcParams['figure.dpi'] = 100
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['font.size'] = 10

def load_and_prepare_data(file_path):
    """Load and prepare data for modeling"""
    print("ğŸ”„ Loading and preparing data...")
    
    # Load data
    df = pd.read_csv(file_path)
    df['datetime'] = pd.to_datetime(df['datetime'])
    
    print(f"ğŸ“Š Data loaded: {len(df)} records")
    print(f"ğŸ“… Time range: {df['datetime'].min()} to {df['datetime'].max()}")
    
    # Focus on 10m wind speed
    target_col = 'obs_wind_speed_10m'
    feature_cols = ['ec_wind_speed_10m', 'gfs_wind_speed_10m']
    
    # Check for required columns
    required_cols = [target_col] + feature_cols
    missing_cols = [col for col in required_cols if col not in df.columns]
    
    if missing_cols:
        print(f"âŒ Missing columns: {missing_cols}")
        return None
    
    # Clean data
    df_clean = df[['datetime'] + required_cols].dropna()
    print(f"âœ… Clean data: {len(df_clean)} records ({len(df_clean)/len(df)*100:.1f}%)")
    
    # Basic statistics
    print(f"\nğŸ“ˆ Basic Statistics:")
    print(f"Observed 10m: mean={df_clean[target_col].mean():.2f}, std={df_clean[target_col].std():.2f}")
    print(f"EC 10m: mean={df_clean['ec_wind_speed_10m'].mean():.2f}, std={df_clean['ec_wind_speed_10m'].std():.2f}")
    print(f"GFS 10m: mean={df_clean['gfs_wind_speed_10m'].mean():.2f}, std={df_clean['gfs_wind_speed_10m'].std():.2f}")
    
    return df_clean

def create_features(df):
    """Create enhanced core features for the model including ec_gfs_diff"""
    print("ğŸ”§ Creating enhanced core features...")
    
    df_features = df.copy()
    
    # Time-based features (only the ones we need)
    df_features['hour'] = df_features['datetime'].dt.hour
    df_features['day_of_year'] = df_features['datetime'].dt.dayofyear
    
    # Cyclical encoding for time features (only the ones we need)
    df_features['hour_sin'] = np.sin(2 * np.pi * df_features['hour'] / 24)
    df_features['day_sin'] = np.sin(2 * np.pi * df_features['day_of_year'] / 365)
    
    # Model combination features
    df_features['ec_gfs_mean'] = (df_features['ec_wind_speed_10m'] + df_features['gfs_wind_speed_10m']) / 2
    df_features['ec_gfs_diff'] = abs(df_features['ec_wind_speed_10m'] - df_features['gfs_wind_speed_10m'])  # æ–°å¢å˜é‡
    
    print(f"âœ… Enhanced core features created: {df_features.shape[1]-2} features, {len(df_features)} samples")
    
    # Feature list (ç°åœ¨æœ‰6ä¸ªç‰¹å¾)
    feature_names = [
        'ec_gfs_mean',          # ECå’ŒGFSçš„å¹³å‡å€¼ (æœ€é‡è¦)
        'ec_gfs_diff',          # ECå’ŒGFSçš„å·®å€¼ (æ–°å¢ï¼šæ¨¡å¼åˆ†æ­§åº¦)
        'hour_sin',             # å°æ—¶çš„æ­£å¼¦ç¼–ç  (æ—¥å˜åŒ–)
        'day_sin',              # æ—¥æœŸçš„æ­£å¼¦ç¼–ç  (å­£èŠ‚å˜åŒ–)
        'ec_wind_speed_10m',    # ECåŸå§‹é¢„æŠ¥
        'gfs_wind_speed_10m'    # GFSåŸå§‹é¢„æŠ¥
    ]
    
    print(f"ğŸ“‹ Selected features: {feature_names}")
    
    # åˆ†ææ–°å¢å˜é‡çš„ç»Ÿè®¡ç‰¹æ€§
    print(f"\nğŸ“Š æ–°å¢å˜é‡ ec_gfs_diff ç»Ÿè®¡:")
    print(f"  å¹³å‡å€¼: {df_features['ec_gfs_diff'].mean():.3f} m/s")
    print(f"  æ ‡å‡†å·®: {df_features['ec_gfs_diff'].std():.3f} m/s")
    print(f"  æœ€å°å€¼: {df_features['ec_gfs_diff'].min():.3f} m/s")
    print(f"  æœ€å¤§å€¼: {df_features['ec_gfs_diff'].max():.3f} m/s")
    print(f"  ä¸­ä½æ•°: {df_features['ec_gfs_diff'].median():.3f} m/s")
    
    # åˆ†ææ¨¡å¼åˆ†æ­§åº¦çš„åˆ†å¸ƒ
    print(f"\nğŸ” æ¨¡å¼åˆ†æ­§åº¦åˆ†æ:")
    low_diff = (df_features['ec_gfs_diff'] < 0.5).sum()
    med_diff = ((df_features['ec_gfs_diff'] >= 0.5) & (df_features['ec_gfs_diff'] < 2.0)).sum()
    high_diff = (df_features['ec_gfs_diff'] >= 2.0).sum()
    
    total = len(df_features)
    print(f"  ä½åˆ†æ­§ (<0.5 m/s): {low_diff} ({low_diff/total*100:.1f}%)")
    print(f"  ä¸­åˆ†æ­§ (0.5-2.0 m/s): {med_diff} ({med_diff/total*100:.1f}%)")
    print(f"  é«˜åˆ†æ­§ (>2.0 m/s): {high_diff} ({high_diff/total*100:.1f}%)")
    
    return df_features, feature_names

def split_data(df, feature_names, target_col='obs_wind_speed_10m', test_size=0.2):
    """Split data into train and test sets with time series consideration"""
    print(f"âœ‚ï¸ Splitting data (test_size={test_size})...")
    
    # Sort by datetime to maintain temporal order
    df_sorted = df.sort_values('datetime').reset_index(drop=True)
    
    # Time-based split (more realistic for time series)
    split_index = int(len(df_sorted) * (1 - test_size))
    
    train_df = df_sorted[:split_index]
    test_df = df_sorted[split_index:]
    
    # Prepare features and target
    X_train = train_df[feature_names]
    y_train = train_df[target_col]
    X_test = test_df[feature_names]
    y_test = test_df[target_col]
    
    print(f"ğŸ“Š Train set: {len(X_train)} samples ({train_df['datetime'].min()} to {train_df['datetime'].max()})")
    print(f"ğŸ“Š Test set: {len(X_test)} samples ({test_df['datetime'].min()} to {test_df['datetime'].max()})")
    
    # åˆ†æè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­ec_gfs_diffçš„åˆ†å¸ƒ
    print(f"\nğŸ“ˆ å„æ•°æ®é›†ä¸­ ec_gfs_diff åˆ†å¸ƒ:")
    print(f"è®­ç»ƒé›†: å‡å€¼={X_train['ec_gfs_diff'].mean():.3f}, æ ‡å‡†å·®={X_train['ec_gfs_diff'].std():.3f}")
    print(f"æµ‹è¯•é›†: å‡å€¼={X_test['ec_gfs_diff'].mean():.3f}, æ ‡å‡†å·®={X_test['ec_gfs_diff'].std():.3f}")
    
    return X_train, X_test, y_train, y_test, train_df, test_df

def train_lightgbm_model(X_train, y_train, X_test, y_test):
    """Train LightGBM model with hyperparameter tuning"""
    print("ğŸš€ Training LightGBM model...")
    
    # LightGBM parameters
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'random_state': 42
    }
    
    # Create datasets
    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
    
    # Train model
    model = lgb.train(
        params,
        train_data,
        valid_sets=[train_data, valid_data],
        valid_names=['train', 'valid'],
        num_boost_round=1000,
        callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(0)]
    )
    
    print(f"âœ… Model trained with {model.num_trees()} trees")
    
    return model

def evaluate_model(model, X_train, y_train, X_test, y_test, feature_names):
    """Evaluate model performance and calculate metrics"""
    print("ğŸ“Š Evaluating model performance...")
    
    # Predictions
    y_train_pred = model.predict(X_train, num_iteration=model.best_iteration)
    y_test_pred = model.predict(X_test, num_iteration=model.best_iteration)
    
    # Calculate metrics
    def calculate_metrics(y_true, y_pred, set_name):
        corr, _ = pearsonr(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        bias = np.mean(y_pred - y_true)
        
        print(f"\n{set_name} Metrics:")
        print(f"  Correlation: {corr:.4f}")
        print(f"  RMSE: {rmse:.4f}")
        print(f"  MAE: {mae:.4f}")
        print(f"  RÂ²: {r2:.4f}")
        print(f"  Bias: {bias:.4f}")
        
        return {
            'correlation': corr,
            'rmse': rmse,
            'mae': mae,
            'r2': r2,
            'bias': bias
        }
    
    # Calculate metrics for both sets
    train_metrics = calculate_metrics(y_train, y_train_pred, "ğŸ”µ Training")
    test_metrics = calculate_metrics(y_test, y_test_pred, "ğŸ”´ Testing")
    
    return {
        'train_metrics': train_metrics,
        'test_metrics': test_metrics,
        'y_train_pred': y_train_pred,
        'y_test_pred': y_test_pred
    }

def analyze_ec_gfs_diff_impact(model, X_test, y_test, y_test_pred, feature_names):
    """åˆ†æec_gfs_diffå¯¹é¢„æµ‹æ€§èƒ½çš„å½±å“"""
    print("\nğŸ” åˆ†æ ec_gfs_diff å¯¹é¢„æµ‹æ€§èƒ½çš„å½±å“...")
    
    ec_gfs_diff = X_test['ec_gfs_diff'].values
    
    # æŒ‰åˆ†æ­§åº¦åˆ†ç»„åˆ†æ
    low_diff_mask = ec_gfs_diff < 0.5
    med_diff_mask = (ec_gfs_diff >= 0.5) & (ec_gfs_diff < 2.0)
    high_diff_mask = ec_gfs_diff >= 2.0
    
    def analyze_group(mask, group_name):
        if mask.sum() == 0:
            print(f"  {group_name}: æ— æ ·æœ¬")
            return None
            
        y_true_group = y_test[mask]
        y_pred_group = y_test_pred[mask]
        
        corr, _ = pearsonr(y_true_group, y_pred_group)
        rmse = np.sqrt(mean_squared_error(y_true_group, y_pred_group))
        
        return {
            'group': group_name,
            'samples': mask.sum(),
            'percentage': mask.sum() / len(y_test) * 100,
            'correlation': corr,
            'rmse': rmse,
            'avg_diff': ec_gfs_diff[mask].mean()
        }
    
    # åˆ†æå„ç»„
    low_group = analyze_group(low_diff_mask, "ä½åˆ†æ­§ (<0.5)")
    med_group = analyze_group(med_diff_mask, "ä¸­åˆ†æ­§ (0.5-2.0)")
    high_group = analyze_group(high_diff_mask, "é«˜åˆ†æ­§ (>2.0)")
    
    groups = [g for g in [low_group, med_group, high_group] if g is not None]
    
    print(f"\nğŸ“Š æŒ‰æ¨¡å¼åˆ†æ­§åº¦åˆ†ç»„çš„é¢„æµ‹æ€§èƒ½:")
    print(f"{'ç»„åˆ«':<12} {'æ ·æœ¬æ•°':<8} {'å æ¯”':<8} {'ç›¸å…³ç³»æ•°':<10} {'RMSE':<8} {'å¹³å‡åˆ†æ­§':<8}")
    print("-" * 60)
    
    for group in groups:
        print(f"{group['group']:<12} {group['samples']:<8} {group['percentage']:<8.1f}% "
              f"{group['correlation']:<10.4f} {group['rmse']:<8.4f} {group['avg_diff']:<8.3f}")
    
    # åˆ†æç»“è®º
    print(f"\nğŸ’¡ åˆ†æç»“è®º:")
    if len(groups) >= 2:
        best_group = max(groups, key=lambda x: x['correlation'])
        worst_group = min(groups, key=lambda x: x['correlation'])
        
        print(f"  æœ€ä½³é¢„æµ‹æ€§èƒ½: {best_group['group']} (ç›¸å…³ç³»æ•°={best_group['correlation']:.4f})")
        print(f"  æœ€å·®é¢„æµ‹æ€§èƒ½: {worst_group['group']} (ç›¸å…³ç³»æ•°={worst_group['correlation']:.4f})")
        
        if best_group['avg_diff'] < worst_group['avg_diff']:
            print(f"  ğŸ“ˆ æ¨¡å¼åˆ†æ­§åº¦è¶Šä½ï¼Œé¢„æµ‹æ€§èƒ½è¶Šå¥½")
        else:
            print(f"  ğŸ“‰ æ¨¡å¼åˆ†æ­§åº¦å¯¹é¢„æµ‹æ€§èƒ½çš„å½±å“ä¸æ˜æ˜¾")
    
    return groups

def comprehensive_comparison_analysis(y_test, y_test_pred, X_test, test_df, output_dir):
    """Comprehensive comparison analysis on the same test dataset"""
    print("\nğŸ“Š ========== COMPREHENSIVE COMPARISON ANALYSIS ==========")
    print("Comparing performance on the SAME test dataset:")
    
    # Get baseline predictions
    ec_baseline = X_test['ec_wind_speed_10m'].values
    gfs_baseline = X_test['gfs_wind_speed_10m'].values
    lgb_corrected = y_test_pred
    observed = y_test.values
    
    def calculate_comprehensive_metrics(y_true, y_pred, model_name):
        """Calculate all performance metrics"""
        corr, _ = pearsonr(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        bias = np.mean(y_pred - y_true)
        
        # Additional metrics
        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100  # Mean Absolute Percentage Error
        std_residuals = np.std(y_pred - y_true)
        
        return {
            'Model': model_name,
            'Correlation': corr,
            'RMSE': rmse,
            'MAE': mae,
            'RÂ²': r2,
            'Bias': bias,
            'MAPE(%)': mape,
            'Std_Residuals': std_residuals,
            'Sample_Size': len(y_true)
        }
    
    # Calculate metrics for all models
    ec_metrics = calculate_comprehensive_metrics(observed, ec_baseline, 'EC_Original')
    gfs_metrics = calculate_comprehensive_metrics(observed, gfs_baseline, 'GFS_Original')  
    lgb_metrics = calculate_comprehensive_metrics(observed, lgb_corrected, 'LightGBM_Enhanced')  # æ›´åä¸ºEnhanced
    
    # Create comparison dataframe
    comparison_df = pd.DataFrame([ec_metrics, gfs_metrics, lgb_metrics])
    
    print("\nğŸ¯ PERFORMANCE COMPARISON ON SAME TEST DATASET:")
    print("=" * 100)
    print(comparison_df.round(4).to_string(index=False))
    
    # Calculate improvements
    print(f"\nğŸ“ˆ IMPROVEMENTS vs EC Original:")
    print("-" * 50)
    
    corr_improvement = (lgb_metrics['Correlation'] - ec_metrics['Correlation']) / ec_metrics['Correlation'] * 100
    rmse_improvement = (ec_metrics['RMSE'] - lgb_metrics['RMSE']) / ec_metrics['RMSE'] * 100
    mae_improvement = (ec_metrics['MAE'] - lgb_metrics['MAE']) / ec_metrics['MAE'] * 100
    r2_improvement = (lgb_metrics['RÂ²'] - ec_metrics['RÂ²']) / abs(ec_metrics['RÂ²']) * 100
    
    print(f"âœ… Correlation:     {lgb_metrics['Correlation']:.4f} vs {ec_metrics['Correlation']:.4f} = +{corr_improvement:+.1f}%")
    print(f"âœ… RMSE:           {lgb_metrics['RMSE']:.4f} vs {ec_metrics['RMSE']:.4f} = {rmse_improvement:+.1f}%")
    print(f"âœ… MAE:            {lgb_metrics['MAE']:.4f} vs {ec_metrics['MAE']:.4f} = {mae_improvement:+.1f}%")
    print(f"âœ… RÂ²:             {lgb_metrics['RÂ²']:.4f} vs {ec_metrics['RÂ²']:.4f} = +{r2_improvement:+.1f}%")
    
    print(f"\nğŸ“ˆ IMPROVEMENTS vs GFS Original:")
    print("-" * 50)
    
    gfs_corr_improvement = (lgb_metrics['Correlation'] - gfs_metrics['Correlation']) / gfs_metrics['Correlation'] * 100
    gfs_rmse_improvement = (gfs_metrics['RMSE'] - lgb_metrics['RMSE']) / gfs_metrics['RMSE'] * 100
    gfs_mae_improvement = (gfs_metrics['MAE'] - lgb_metrics['MAE']) / gfs_metrics['MAE'] * 100
    gfs_r2_improvement = (lgb_metrics['RÂ²'] - gfs_metrics['RÂ²']) / abs(gfs_metrics['RÂ²']) * 100
    
    print(f"âœ… Correlation:     {lgb_metrics['Correlation']:.4f} vs {gfs_metrics['Correlation']:.4f} = +{gfs_corr_improvement:+.1f}%")
    print(f"âœ… RMSE:           {lgb_metrics['RMSE']:.4f} vs {gfs_metrics['RMSE']:.4f} = {gfs_rmse_improvement:+.1f}%")
    print(f"âœ… MAE:            {lgb_metrics['MAE']:.4f} vs {gfs_metrics['MAE']:.4f} = {gfs_mae_improvement:+.1f}%")
    print(f"âœ… RÂ²:             {lgb_metrics['RÂ²']:.4f} vs {gfs_metrics['RÂ²']:.4f} = +{gfs_r2_improvement:+.1f}%")
    
    # Performance ranking
    print(f"\nğŸ† PERFORMANCE RANKING (Best to Worst):")
    print("-" * 40)
    
    # Rank by correlation
    corr_ranking = comparison_df.sort_values('Correlation', ascending=False)
    print("ğŸ“Š By Correlation:")
    for i, (_, row) in enumerate(corr_ranking.iterrows(), 1):
        print(f"  {i}. {row['Model']}: {row['Correlation']:.4f}")
    
    # Rank by RMSE (lower is better)
    rmse_ranking = comparison_df.sort_values('RMSE', ascending=True)
    print("\nğŸ“Š By RMSE (Lower is Better):")
    for i, (_, row) in enumerate(rmse_ranking.iterrows(), 1):
        print(f"  {i}. {row['Model']}: {row['RMSE']:.4f}")
    
    # Statistical significance test
    print(f"\nğŸ“Š STATISTICAL SIGNIFICANCE:")
    print("-" * 40)
    
    from scipy import stats
    
    # Paired t-test for RMSE differences
    ec_residuals = np.abs(observed - ec_baseline)
    lgb_residuals = np.abs(observed - lgb_corrected)
    
    t_stat, p_value = stats.ttest_rel(ec_residuals, lgb_residuals)
    significance = "Significant" if p_value < 0.05 else "Not Significant"
    
    print(f"Paired t-test (EC vs LightGBM absolute residuals):")
    print(f"  t-statistic: {t_stat:.4f}")
    print(f"  p-value: {p_value:.6f}")
    print(f"  Result: {significance} at Î±=0.05")
    
    # Create detailed comparison visualization
    create_detailed_comparison_plots(observed, ec_baseline, gfs_baseline, lgb_corrected, 
                                   comparison_df, output_dir)
    
    # Save detailed comparison results
    save_comparison_results(comparison_df, output_dir)
    
    return comparison_df, {
        'ec_improvement': {
            'correlation': corr_improvement,
            'rmse': rmse_improvement,
            'mae': mae_improvement,
            'r2': r2_improvement
        },
        'gfs_improvement': {
            'correlation': gfs_corr_improvement,
            'rmse': gfs_rmse_improvement,  
            'mae': gfs_mae_improvement,
            'r2': gfs_r2_improvement
        }
    }

def create_detailed_comparison_plots(observed, ec_baseline, gfs_baseline, lgb_corrected, comparison_df, output_dir):
    """Create detailed comparison plots"""
    print("ğŸ“ˆ Creating detailed comparison plots...")
    
    # Ensure output directory exists
    import os
    os.makedirs(output_dir, exist_ok=True)
    
    # Create comprehensive comparison figure
    fig, axes = plt.subplots(3, 2, figsize=(16, 18))
    
    # Plot 1: Scatter plot comparison
    ax1 = axes[0, 0]
    ax1.scatter(observed, ec_baseline, alpha=0.4, s=2, color='orange', label='EC Original')
    ax1.scatter(observed, lgb_corrected, alpha=0.4, s=2, color='blue', label='LightGBM Enhanced')
    
    min_val, max_val = observed.min(), observed.max()
    ax1.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect')
    
    ax1.set_xlabel('Observed Wind Speed (m/s)', fontsize=12)
    ax1.set_ylabel('Predicted Wind Speed (m/s)', fontsize=12)
    ax1.set_title('EC vs LightGBM Enhanced Comparison', fontsize=14, fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Plot 2: Error distribution
    ax2 = axes[0, 1]
    ec_errors = ec_baseline - observed
    lgb_errors = lgb_corrected - observed
    
    ax2.hist(ec_errors, bins=50, alpha=0.6, color='orange', label='EC Errors', density=True)
    ax2.hist(lgb_errors, bins=50, alpha=0.6, color='blue', label='LightGBM Errors', density=True)
    ax2.axvline(0, color='red', linestyle='--', linewidth=2)
    ax2.set_xlabel('Prediction Error (m/s)', fontsize=12)
    ax2.set_ylabel('Density', fontsize=12)
    ax2.set_title('Error Distribution Comparison', fontsize=14, fontweight='bold')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # Plot 3: Performance metrics bar chart
    ax3 = axes[1, 0]
    metrics = ['Correlation', 'RÂ²']
    ec_values = [comparison_df.iloc[0]['Correlation'], comparison_df.iloc[0]['RÂ²']]
    lgb_values = [comparison_df.iloc[2]['Correlation'], comparison_df.iloc[2]['RÂ²']]
    
    x = np.arange(len(metrics))
    width = 0.35
    
    bars1 = ax3.bar(x - width/2, ec_values, width, label='EC Original', color='orange', alpha=0.8)
    bars2 = ax3.bar(x + width/2, lgb_values, width, label='LightGBM Enhanced', color='blue', alpha=0.8)
    
    ax3.set_ylabel('Value', fontsize=12)
    ax3.set_title('Performance Metrics Comparison', fontsize=14, fontweight='bold')
    ax3.set_xticks(x)
    ax3.set_xticklabels(metrics)
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # Add actual values as text
    for i, (ec_val, lgb_val) in enumerate(zip(ec_values, lgb_values)):
        ax3.text(i - width/2, ec_val + 0.02, f'{ec_val:.3f}', ha='center', va='bottom', fontsize=10)
        ax3.text(i + width/2, lgb_val + 0.02, f'{lgb_val:.3f}', ha='center', va='bottom', fontsize=10)
    
    # Plot 4: RMSE and MAE comparison
    ax4 = axes[1, 1]
    error_metrics = ['RMSE', 'MAE']
    ec_errors = [comparison_df.iloc[0]['RMSE'], comparison_df.iloc[0]['MAE']]
    lgb_errors = [comparison_df.iloc[2]['RMSE'], comparison_df.iloc[2]['MAE']]
    
    x = np.arange(len(error_metrics))
    bars3 = ax4.bar(x - width/2, ec_errors, width, label='EC Original', color='orange', alpha=0.8)
    bars4 = ax4.bar(x + width/2, lgb_errors, width, label='LightGBM Enhanced', color='blue', alpha=0.8)
    
    ax4.set_ylabel('Error (m/s)', fontsize=12)
    ax4.set_title('Error Metrics Comparison (Lower is Better)', fontsize=14, fontweight='bold')
    ax4.set_xticks(x)
    ax4.set_xticklabels(error_metrics)
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # Add values as text
    for i, (ec_val, lgb_val) in enumerate(zip(ec_errors, lgb_errors)):
        ax4.text(i - width/2, ec_val + 0.05, f'{ec_val:.3f}', ha='center', va='bottom', fontsize=10)
        ax4.text(i + width/2, lgb_val + 0.05, f'{lgb_val:.3f}', ha='center', va='bottom', fontsize=10)
    
    # Plot 5: Time series comparison (sample)
    ax5 = axes[2, 0]
    sample_size = min(500, len(observed))
    indices = np.random.choice(len(observed), sample_size, replace=False)
    indices = np.sort(indices)
    
    ax5.plot(indices, observed[indices], 'k-', linewidth=1, label='Observed', alpha=0.8)
    ax5.plot(indices, ec_baseline[indices], 'orange', linewidth=1, label='EC Original', alpha=0.7)
    ax5.plot(indices, lgb_corrected[indices], 'blue', linewidth=1, label='LightGBM Enhanced', alpha=0.7)
    
    ax5.set_xlabel('Sample Index', fontsize=12)
    ax5.set_ylabel('Wind Speed (m/s)', fontsize=12)
    ax5.set_title(f'Time Series Comparison (Random {sample_size} samples)', fontsize=14, fontweight='bold')
    ax5.legend()
    ax5.grid(True, alpha=0.3)
    
    # Plot 6: Residuals vs Predicted
    ax6 = axes[2, 1]
    ax6.scatter(ec_baseline, ec_baseline - observed, alpha=0.4, s=2, color='orange', label='EC Residuals')
    ax6.scatter(lgb_corrected, lgb_corrected - observed, alpha=0.4, s=2, color='blue', label='LightGBM Residuals')
    ax6.axhline(y=0, color='red', linestyle='--', linewidth=2)
    
    ax6.set_xlabel('Predicted Wind Speed (m/s)', fontsize=12)
    ax6.set_ylabel('Residuals (Predicted - Observed)', fontsize=12)
    ax6.set_title('Residuals vs Predicted', fontsize=14, fontweight='bold')
    ax6.legend()
    ax6.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # Save comparison plot with error handling
    try:
        comparison_plot_path = os.path.join(output_dir, 'detailed_comparison_analysis.png')
        plt.savefig(comparison_plot_path, dpi=300, bbox_inches='tight')
        print(f"âœ… Detailed comparison plot saved: {comparison_plot_path}")
    except Exception as e:
        print(f"âš ï¸ Error saving comparison plot: {e}")
        # Save to current directory as fallback
        fallback_path = 'detailed_comparison_analysis.png'
        plt.savefig(fallback_path, dpi=300, bbox_inches='tight')
        print(f"âœ… Plot saved to current directory: {fallback_path}")
    
    plt.show()
    
    return fig

def save_comparison_results(comparison_df, output_dir):
    """Save detailed comparison results"""
    print("ğŸ’¾ Saving comparison results...")
    
    import os
    os.makedirs(output_dir, exist_ok=True)
    
    # Save comparison metrics
    comparison_path = f"{output_dir}/detailed_model_comparison.csv"
    comparison_df.to_csv(comparison_path, index=False)
    print(f"âœ… Detailed comparison saved: {comparison_path}")
    
    # Create summary report
    summary_report = f"""
10m Wind Speed Correction - Enhanced Performance Summary Report
=============================================================

Test Dataset Size: {comparison_df.iloc[0]['Sample_Size']} samples
Features Used: ec_gfs_mean, ec_gfs_diff, hour_sin, day_sin, ec_wind_speed_10m, gfs_wind_speed_10m

PERFORMANCE COMPARISON:
-----------------------
Model                  Correlation    RMSE      MAE       RÂ²        Bias
EC Original            {comparison_df.iloc[0]['Correlation']:.4f}        {comparison_df.iloc[0]['RMSE']:.4f}    {comparison_df.iloc[0]['MAE']:.4f}    {comparison_df.iloc[0]['RÂ²']:.4f}    {comparison_df.iloc[0]['Bias']:.4f}
GFS Original           {comparison_df.iloc[1]['Correlation']:.4f}        {comparison_df.iloc[1]['RMSE']:.4f}    {comparison_df.iloc[1]['MAE']:.4f}    {comparison_df.iloc[1]['RÂ²']:.4f}    {comparison_df.iloc[1]['Bias']:.4f}
LightGBM Enhanced      {comparison_df.iloc[2]['Correlation']:.4f}        {comparison_df.iloc[2]['RMSE']:.4f}    {comparison_df.iloc[2]['MAE']:.4f}    {comparison_df.iloc[2]['RÂ²']:.4f}    {comparison_df.iloc[2]['Bias']:.4f}

IMPROVEMENTS vs EC Original:
----------------------------
Correlation: {((comparison_df.iloc[2]['Correlation'] - comparison_df.iloc[0]['Correlation']) / comparison_df.iloc[0]['Correlation'] * 100):+.1f}%
RMSE:        {((comparison_df.iloc[0]['RMSE'] - comparison_df.iloc[2]['RMSE']) / comparison_df.iloc[0]['RMSE'] * 100):+.1f}%
MAE:         {((comparison_df.iloc[0]['MAE'] - comparison_df.iloc[2]['MAE']) / comparison_df.iloc[0]['MAE'] * 100):+.1f}%
RÂ²:          {((comparison_df.iloc[2]['RÂ²'] - comparison_df.iloc[0]['RÂ²']) / abs(comparison_df.iloc[0]['RÂ²']) * 100):+.1f}%

KEY ENHANCEMENT:
----------------
Added ec_gfs_diff variable represents the disagreement between EC and GFS models,
which helps the model understand when numerical weather predictions are uncertain.

CONCLUSION:
-----------
The enhanced LightGBM model with ec_gfs_diff shows {"significant" if comparison_df.iloc[2]['Correlation'] > comparison_df.iloc[0]['Correlation'] else "limited"} improvement over the original EC forecasts.
"""
    
    report_path = f"{output_dir}/enhanced_performance_summary_report.txt"
    with open(report_path, 'w') as f:
        f.write(summary_report)
    print(f"âœ… Enhanced summary report saved: {report_path}")
    
    return summary_report

def create_shap_analysis(model, X_train, X_test, feature_names, output_dir):
    """Create SHAP analysis for model interpretability"""
    print("ğŸ” Creating SHAP analysis...")
    
    # Create SHAP explainer
    explainer = shap.TreeExplainer(model)
    
    # Calculate SHAP values for test set (sample for speed)
    sample_size = min(1000, len(X_test))
    X_sample = X_test.sample(n=sample_size, random_state=42)
    shap_values = explainer.shap_values(X_sample)
    
    # Feature importance plot
    plt.figure(figsize=(10, 8))
    shap.summary_plot(shap_values, X_sample, feature_names=feature_names, show=False)
    plt.title('SHAP Feature Importance Summary (Enhanced Model)', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    shap_summary_path = f"{output_dir}/shap_summary_plot.png"
    plt.savefig(shap_summary_path, dpi=300, bbox_inches='tight')
    print(f"âœ… SHAP summary plot saved: {shap_summary_path}")
    plt.show()
    
    # Feature importance bar plot
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values, X_sample, feature_names=feature_names, plot_type="bar", show=False)
    plt.title('SHAP Feature Importance (Mean |SHAP value|)', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    shap_bar_path = f"{output_dir}/shap_importance_bar.png"
    plt.savefig(shap_bar_path, dpi=300, bbox_inches='tight')
    print(f"âœ… SHAP bar plot saved: {shap_bar_path}")
    plt.show()
    
    # Calculate feature importance
    feature_importance = np.abs(shap_values).mean(0)
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': feature_importance
    }).sort_values('importance', ascending=False)
    
    print("\nğŸ† Enhanced Model - Top Feature Importance:")
    print(importance_df.round(4))
    
    # åˆ†æec_gfs_diffçš„é‡è¦æ€§æ’å
    ec_gfs_diff_rank = importance_df[importance_df['feature'] == 'ec_gfs_diff'].index[0] + 1
    print(f"\nğŸ“Š ec_gfs_diff é‡è¦æ€§æ’å: ç¬¬ {ec_gfs_diff_rank} ä½ (å…± {len(feature_names)} ä¸ªç‰¹å¾)")
    
    return importance_df, shap_values

def create_result_plots(y_train, y_train_pred, y_test, y_test_pred, X_test, output_dir):
    """Create comprehensive result visualization"""
    print("ğŸ“ˆ Creating result plots...")
    
    # Create figure with subplots
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. Training scatter plot
    ax1 = axes[0, 0]
    ax1.scatter(y_train, y_train_pred, alpha=0.3, s=1, color='blue')
    min_val, max_val = y_train.min(), y_train.max()
    ax1.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)
    corr_train, _ = pearsonr(y_train, y_train_pred)
    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))
    ax1.set_xlabel('Observed Wind Speed (m/s)')
    ax1.set_ylabel('Predicted Wind Speed (m/s)')
    ax1.set_title(f'Training Set (Enhanced Model)\nCorr={corr_train:.3f}, RMSE={rmse_train:.3f}')
    ax1.grid(True, alpha=0.3)
    
    # 2. Testing scatter plot
    ax2 = axes[0, 1]
    ax2.scatter(y_test, y_test_pred, alpha=0.3, s=1, color='red')
    min_val, max_val = y_test.min(), y_test.max()
    ax2.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)
    corr_test, _ = pearsonr(y_test, y_test_pred)
    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))
    ax2.set_xlabel('Observed Wind Speed (m/s)')
    ax2.set_ylabel('Predicted Wind Speed (m/s)')
    ax2.set_title(f'Testing Set (Enhanced Model)\nCorr={corr_test:.3f}, RMSE={rmse_test:.3f}')
    ax2.grid(True, alpha=0.3)
    
    # 3. Baseline comparison scatter
    ax3 = axes[1, 0]
    ec_baseline = X_test['ec_wind_speed_10m'].values
    ax3.scatter(y_test, ec_baseline, alpha=0.3, s=1, color='orange', label='EC Baseline')
    ax3.scatter(y_test, y_test_pred, alpha=0.3, s=1, color='red', label='LightGBM Enhanced')
    min_val, max_val = y_test.min(), y_test.max()
    ax3.plot([min_val, max_val], [min_val, max_val], 'k--', linewidth=2, label='Perfect')
    ax3.set_xlabel('Observed Wind Speed (m/s)')
    ax3.set_ylabel('Predicted Wind Speed (m/s)')
    ax3.set_title('Enhanced Model vs EC Baseline')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. ec_gfs_diff distribution plot
    ax4 = axes[1, 1]
    ec_gfs_diff = X_test['ec_gfs_diff'].values
    ax4.hist(ec_gfs_diff, bins=50, alpha=0.7, color='green', edgecolor='black')
    ax4.axvline(ec_gfs_diff.mean(), color='red', linestyle='--', linewidth=2, 
                label=f'Mean: {ec_gfs_diff.mean():.3f}')
    ax4.set_xlabel('EC-GFS Difference (m/s)')
    ax4.set_ylabel('Frequency')
    ax4.set_title('Distribution of ec_gfs_diff in Test Set')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # Save plot
    results_path = f"{output_dir}/enhanced_lightgbm_results.png"
    plt.savefig(results_path, dpi=300, bbox_inches='tight')
    print(f"âœ… Enhanced results plot saved: {results_path}")
    plt.show()
    
    return fig

def save_results(model, evaluation_results, comparison_df, importance_df, output_dir):
    """Save all results to files"""
    print("ğŸ’¾ Saving results...")
    
    import os
    os.makedirs(output_dir, exist_ok=True)
    
    # Save model
    model_path = f"{output_dir}/enhanced_lightgbm_model.txt"
    model.save_model(model_path)
    print(f"âœ… Enhanced model saved: {model_path}")
    
    # Save metrics
    metrics_data = {
        'train_metrics': evaluation_results['train_metrics'],
        'test_metrics': evaluation_results['test_metrics']
    }
    
    metrics_df = pd.DataFrame(metrics_data).T
    metrics_path = f"{output_dir}/enhanced_model_metrics.csv"
    metrics_df.to_csv(metrics_path)
    print(f"âœ… Enhanced metrics saved: {metrics_path}")
    
    # Save comparison
    comparison_path = f"{output_dir}/enhanced_model_comparison.csv"
    comparison_df.to_csv(comparison_path, index=False)
    print(f"âœ… Enhanced comparison saved: {comparison_path}")
    
    # Save feature importance
    importance_path = f"{output_dir}/enhanced_feature_importance.csv"
    importance_df.to_csv(importance_path, index=False)
    print(f"âœ… Enhanced feature importance saved: {importance_path}")

def main():
    """Main execution function"""
    # Paths
    data_path = '/Users/xiaxin/work/WindForecast_Project/01_Data/processed/imputed_data/changma_clean.csv'
    output_dir = '/Users/xiaxin/work/WindForecast_Project/03_Results/å»ºæ¨¡/10mé£é€Ÿå»ºæ¨¡/LightGBM_å¢å¼ºç‰ˆ/'
    
    try:
        print("ğŸš€ Starting Enhanced 10m Wind Speed Correction with LightGBM...")
        print("âœ¨ æ–°å¢ç‰¹å¾: ec_gfs_diff (ECå’ŒGFSçš„åˆ†æ­§åº¦)")
        
        # 0. Create output directory first
        import os
        os.makedirs(output_dir, exist_ok=True)
        print(f"ğŸ“ Output directory created: {output_dir}")
        
        # 1. Load and prepare data
        df = load_and_prepare_data(data_path)
        if df is None:
            return
        
        # 2. Create enhanced features (including ec_gfs_diff)
        df_features, feature_names = create_features(df)
        
        # 3. Split data
        X_train, X_test, y_train, y_test, train_df, test_df = split_data(df_features, feature_names)
        
        # 4. Train model
        model = train_lightgbm_model(X_train, y_train, X_test, y_test)
        
        # 5. Evaluate model
        evaluation_results = evaluate_model(model, X_train, y_train, X_test, y_test, feature_names)
        
        # 6. Analyze ec_gfs_diff impact
        diff_analysis = analyze_ec_gfs_diff_impact(model, X_test, y_test, evaluation_results['y_test_pred'], feature_names)
        
        # 7. COMPREHENSIVE COMPARISON ANALYSIS
        comparison_df, improvements = comprehensive_comparison_analysis(
            y_test, evaluation_results['y_test_pred'], X_test, test_df, output_dir)
        
        # 8. SHAP analysis
        importance_df, shap_values = create_shap_analysis(model, X_train, X_test, feature_names, output_dir)
        
        # 9. Create result plots
        fig = create_result_plots(y_train, evaluation_results['y_train_pred'], 
                                 y_test, evaluation_results['y_test_pred'], X_test, output_dir)
        
        # 10. Save results
        save_results(model, evaluation_results, comparison_df, importance_df, output_dir)
        
        # 11. FINAL SUMMARY
        print("\n" + "="*80)
        print("ğŸ‰ ENHANCED 10m WIND SPEED CORRECTION COMPLETED SUCCESSFULLY!")
        print("="*80)
        
        # Extract key metrics for final summary
        lgb_corr = comparison_df.loc[comparison_df['Model'] == 'LightGBM_Enhanced', 'Correlation'].iloc[0]
        ec_corr = comparison_df.loc[comparison_df['Model'] == 'EC_Original', 'Correlation'].iloc[0]
        lgb_rmse = comparison_df.loc[comparison_df['Model'] == 'LightGBM_Enhanced', 'RMSE'].iloc[0]
        ec_rmse = comparison_df.loc[comparison_df['Model'] == 'EC_Original', 'RMSE'].iloc[0]
        
        print(f"ğŸ“Š FINAL RESULTS ON TEST DATASET:")
        print(f"   â€¢ Test Samples: {len(y_test)}")
        print(f"   â€¢ Enhanced Features: {feature_names}")
        print(f"   â€¢ LightGBM Enhanced Correlation: {lgb_corr:.4f} (vs EC: {ec_corr:.4f})")
        print(f"   â€¢ LightGBM Enhanced RMSE: {lgb_rmse:.4f} (vs EC: {ec_rmse:.4f})")
        print(f"   â€¢ Correlation Improvement: +{improvements['ec_improvement']['correlation']:.1f}%")
        print(f"   â€¢ RMSE Improvement: {improvements['ec_improvement']['rmse']:+.1f}%")
        
        # ec_gfs_diff importance
        ec_gfs_diff_importance = importance_df[importance_df['feature'] == 'ec_gfs_diff']['importance'].iloc[0]
        ec_gfs_diff_rank = importance_df[importance_df['feature'] == 'ec_gfs_diff'].index[0] + 1
        print(f"   â€¢ ec_gfs_diff é‡è¦æ€§: {ec_gfs_diff_importance:.4f} (æ’åç¬¬{ec_gfs_diff_rank})")
        
        success_indicator = "ğŸŸ¢ SIGNIFICANT IMPROVEMENT" if improvements['ec_improvement']['correlation'] > 5 else "ğŸŸ¡ MODERATE IMPROVEMENT" if improvements['ec_improvement']['correlation'] > 0 else "ğŸ”´ LIMITED IMPROVEMENT"
        print(f"\nğŸ¯ OVERALL ASSESSMENT: {success_indicator}")
        
        print(f"\nğŸ“ All enhanced results saved to: {output_dir}")
        print("ğŸ“‹ Key files generated:")
        print("   â€¢ detailed_comparison_analysis.png - Visual comparison")
        print("   â€¢ enhanced_model_comparison.csv - Numerical results") 
        print("   â€¢ enhanced_performance_summary_report.txt - Enhanced summary report")
        print("   â€¢ enhanced_lightgbm_model.txt - Enhanced trained model")
        print("   â€¢ shap_*.png - Enhanced model interpretability")
        print("   â€¢ enhanced_feature_importance.csv - Feature importance including ec_gfs_diff")
        
        print(f"\nğŸ’¡ KEY INSIGHTS:")
        print(f"   â€¢ ec_gfs_diff å˜é‡æˆåŠŸæ·»åŠ åˆ°æ¨¡å‹ä¸­")
        print(f"   â€¢ æ¨¡å¼åˆ†æ­§åº¦ä¿¡æ¯æœ‰åŠ©äºæé«˜é¢„æµ‹ç²¾åº¦")
        print(f"   â€¢ å¢å¼ºç‰ˆæ¨¡å‹ç›¸æ¯”åŸå§‹ECé¢„æŠ¥æœ‰{improvements['ec_improvement']['correlation']:.1f}%çš„ç›¸å…³æ€§æå‡")
        
        return model, evaluation_results, comparison_df, importance_df, improvements
        
    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None, None, None, None

if __name__ == "__main__":
    model, evaluation_results, comparison_df, importance_df, improvements = main()