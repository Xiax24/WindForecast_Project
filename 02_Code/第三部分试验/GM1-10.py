#!/usr/bin/env python3
"""
G-M1-10m ç®€åŒ–åŸºå‡†è¯•éªŒ
ä½¿ç”¨GFS-WRFåŸå§‹10mé£é€Ÿè¿›è¡ŒåŠŸç‡é¢„æµ‹
ç®€åŒ–ç‰ˆæœ¬ï¼šLightGBM + RMSE + ç›¸å…³ç³»æ•°
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import os
import json
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
RANDOM_STATE = 42

def prepare_simple_features(data, wind_col):
    """å‡†å¤‡ç®€å•ç‰¹å¾"""
    features = pd.DataFrame()
    
    # ä¸»è¦é£é€Ÿç‰¹å¾
    features['wind_speed'] = data[wind_col]
    features['wind_speed_2'] = data[wind_col] ** 2
    features['wind_speed_3'] = data[wind_col] ** 3
    
    # åŸºç¡€æ—¶é—´ç‰¹å¾
    features['hour'] = data['datetime'].dt.hour
    features['month'] = data['datetime'].dt.month
    features['is_daytime'] = ((data['datetime'].dt.hour >= 6) & 
                             (data['datetime'].dt.hour < 18)).astype(int)
    
    # ç®€å•æ»åç‰¹å¾
    features['wind_lag_1h'] = data[wind_col].shift(1)
    features['wind_lag_24h'] = data[wind_col].shift(24)
    
    # å¡«å……NaN
    features = features.fillna(method='bfill').fillna(method='ffill')
    
    return features

def create_train_test_split(data, test_size=0.2, indices_path=None):
    """åˆ›å»ºæˆ–åŠ è½½è®­ç»ƒæµ‹è¯•é›†åˆ’åˆ†"""
    
    if indices_path and os.path.exists(indices_path):
        # åŠ è½½å·²æœ‰åˆ’åˆ†
        with open(indices_path, 'r') as f:
            indices = json.load(f)
        train_indices = indices['train_indices']
        test_indices = indices['test_indices']
        print(f"åŠ è½½å·²æœ‰åˆ’åˆ†: è®­ç»ƒé›†{len(train_indices)}, æµ‹è¯•é›†{len(test_indices)}")
    else:
        # åˆ›å»ºæ–°åˆ’åˆ†ï¼ˆæ—¶é—´åºåˆ—åˆ’åˆ†ï¼‰
        data_sorted = data.sort_values('datetime').reset_index(drop=True)
        n_samples = len(data_sorted)
        split_idx = int(n_samples * (1 - test_size))
        
        train_indices = list(range(split_idx))
        test_indices = list(range(split_idx, n_samples))
        
        # ä¿å­˜åˆ’åˆ†
        if indices_path:
            os.makedirs(os.path.dirname(indices_path), exist_ok=True)
            indices_data = {
                'train_indices': train_indices,
                'test_indices': test_indices,
                'split_date': data_sorted.iloc[split_idx]['datetime'].isoformat(),
                'train_size': len(train_indices),
                'test_size': len(test_indices)
            }
            with open(indices_path, 'w') as f:
                json.dump(indices_data, f, indent=2)
            print(f"æ–°å»ºåˆ’åˆ†å·²ä¿å­˜: è®­ç»ƒé›†{len(train_indices)}, æµ‹è¯•é›†{len(test_indices)}")
    
    return train_indices, test_indices

def train_lightgbm_model(X_train, y_train, X_test, y_test):
    """è®­ç»ƒLightGBMæ¨¡å‹"""
    
    # LightGBMå‚æ•°
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.1,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'random_state': RANDOM_STATE
    }
    
    # åˆ›å»ºæ•°æ®é›†
    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
    
    # è®­ç»ƒæ¨¡å‹
    model = lgb.train(
        params,
        train_data,
        valid_sets=[valid_data],
        num_boost_round=100,
        callbacks=[lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation(0)]
    )
    
    return model

def evaluate_model(model, X_test, y_test):
    """è¯„ä¼°æ¨¡å‹"""
    y_pred = model.predict(X_test, num_iteration=model.best_iteration)
    
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    correlation = np.corrcoef(y_test, y_pred)[0, 1]
    
    return {
        'RMSE': rmse,
        'Correlation': correlation
    }, y_pred

def run_gm1_10m_experiment(data_path, save_dir, indices_path=None):
    """è¿è¡ŒG-M1-10mç®€åŒ–è¯•éªŒ"""
    
    print("=" * 60)
    print("ğŸŒ¬ï¸ G-M1-10m ç®€åŒ–è¯•éªŒ (LightGBM)")
    print("=" * 60)
    
    os.makedirs(save_dir, exist_ok=True)
    
    # 1. æ•°æ®åŠ è½½
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    data = pd.read_csv(data_path)
    data['datetime'] = pd.to_datetime(data['datetime'])
    
    # æ•°æ®æ¸…ç†
    data = data.dropna(subset=['gfs_wind_speed_10m', 'power'])
    data = data[(data['gfs_wind_speed_10m'] >= 0) & (data['gfs_wind_speed_10m'] <= 50)]
    data = data[(data['power'] >= 0)]
    data = data.sort_values('datetime').reset_index(drop=True)
    
    print(f"æ¸…ç†åæ•°æ®: {len(data)} æ ·æœ¬")
    
    # 2. ç‰¹å¾å‡†å¤‡
    print("ğŸ”§ å‡†å¤‡ç‰¹å¾...")
    features = prepare_simple_features(data, 'gfs_wind_speed_10m')
    target = data['power'].values
    
    print(f"ç‰¹å¾æ•°é‡: {features.shape[1]}")
    
    # 3. åˆ’åˆ†æ•°æ®é›†
    print("âœ‚ï¸ åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†...")
    train_indices, test_indices = create_train_test_split(data, indices_path=indices_path)
    
    X_train = features.iloc[train_indices]
    X_test = features.iloc[test_indices]
    y_train = target[train_indices]
    y_test = target[test_indices]
    
    # 4. è®­ç»ƒæ¨¡å‹
    print("ğŸš€ è®­ç»ƒLightGBMæ¨¡å‹...")
    model = train_lightgbm_model(X_train, y_train, X_test, y_test)
    
    # 5. è¯„ä¼°æ¨¡å‹
    print("ğŸ“Š è¯„ä¼°æ¨¡å‹...")
    metrics, y_pred = evaluate_model(model, X_test, y_test)
    
    print(f"RMSE: {metrics['RMSE']:.4f}")
    print(f"ç›¸å…³ç³»æ•°: {metrics['Correlation']:.4f}")
    
    # 6. ä¿å­˜ç»“æœ
    print("ğŸ’¾ ä¿å­˜ç»“æœ...")
    
    # ä¿å­˜é¢„æµ‹ç»“æœ
    results_df = pd.DataFrame({
        'datetime': data.iloc[test_indices]['datetime'].values,
        'actual_power': y_test,
        'predicted_power': y_pred,
        'wind_speed': data.iloc[test_indices]['gfs_wind_speed_10m'].values
    })
    results_df.to_csv(os.path.join(save_dir, 'G-M1-10m_results.csv'), index=False)
    
    # ä¿å­˜æŒ‡æ ‡
    with open(os.path.join(save_dir, 'G-M1-10m_metrics.json'), 'w') as f:
        json.dump(metrics, f, indent=2)
    
    # ä¿å­˜æ¨¡å‹
    model.save_model(os.path.join(save_dir, 'G-M1-10m_model.txt'))
    
    print("âœ… G-M1-10mè¯•éªŒå®Œæˆ!")
    print("=" * 60)
    
    return metrics, results_df

# ä¸ºåç»­è¯•éªŒå‡†å¤‡çš„é€šç”¨å‡½æ•°
def run_wind_power_experiment(data_path, save_dir, indices_path, 
                             wind_col, model_name, 
                             use_corrected=False, weights=None):
    """é€šç”¨é£ç”µè¯•éªŒå‡½æ•°ï¼Œå¯ç”¨äºæ‰€æœ‰14ä¸ªè¯•éªŒ"""
    
    print(f"ğŸŒ¬ï¸ {model_name} è¯•éªŒ")
    print(f"é£é€Ÿåˆ—: {wind_col}")
    if weights:
        print(f"æƒé‡: {weights}")
    
    os.makedirs(save_dir, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    data = pd.read_csv(data_path)
    data['datetime'] = pd.to_datetime(data['datetime'])
    
    # æ•°æ®æ¸…ç†
    required_cols = [col for col in [wind_col] if isinstance(wind_col, str)]
    if isinstance(wind_col, list):
        required_cols = wind_col
    
    data = data.dropna(subset=required_cols + ['power'])
    for col in required_cols:
        data = data[(data[col] >= 0) & (data[col] <= 50)]
    data = data[data['power'] >= 0]
    data = data.sort_values('datetime').reset_index(drop=True)
    
    # å¤„ç†æƒé‡èåˆ
    if weights and isinstance(wind_col, list):
        # å¤šé£é€ŸåŠ æƒèåˆ
        wind_series = np.zeros(len(data))
        for i, col in enumerate(wind_col):
            wind_series += weights[i] * data[col].values
        data['fused_wind'] = wind_series
        feature_wind_col = 'fused_wind'
    else:
        feature_wind_col = wind_col
    
    # ç‰¹å¾å‡†å¤‡
    features = prepare_simple_features(data, feature_wind_col)
    target = data['power'].values
    
    # åˆ’åˆ†æ•°æ®é›†
    train_indices, test_indices = create_train_test_split(data, indices_path=indices_path)
    
    X_train = features.iloc[train_indices]
    X_test = features.iloc[test_indices]
    y_train = target[train_indices]
    y_test = target[test_indices]
    
    # è®­ç»ƒå’Œè¯„ä¼°
    model = train_lightgbm_model(X_train, y_train, X_test, y_test)
    metrics, y_pred = evaluate_model(model, X_test, y_test)
    
    print(f"RMSE: {metrics['RMSE']:.4f}")
    print(f"ç›¸å…³ç³»æ•°: {metrics['Correlation']:.4f}")
    
    # ä¿å­˜ç»“æœ
    results_df = pd.DataFrame({
        'datetime': data.iloc[test_indices]['datetime'].values,
        'actual_power': y_test,
        'predicted_power': y_pred
    })
    results_df.to_csv(os.path.join(save_dir, f'{model_name}_results.csv'), index=False)
    
    with open(os.path.join(save_dir, f'{model_name}_metrics.json'), 'w') as f:
        json.dump(metrics, f, indent=2)
    
    model.save_model(os.path.join(save_dir, f'{model_name}_model.txt'))
    
    return metrics, results_df

if __name__ == "__main__":
    # é…ç½®è·¯å¾„
    DATA_PATH = "/Users/xiaxin/work/WindForecast_Project/01_Data/processed/imputed_data/changma_imputed_complete.csv"
    SAVE_DIR = "/Users/xiaxin/work/WindForecast_Project/03_Results/third_part_experiments/G-M1-10m"
    INDICES_PATH = "/Users/xiaxin/work/WindForecast_Project/03_Results/third_part_experiments/train_test_split.json"
    
    # è¿è¡ŒG-M1-10måŸºå‡†è¯•éªŒ
    metrics, results = run_gm1_10m_experiment(
        data_path=DATA_PATH,
        save_dir=SAVE_DIR,
        indices_path=INDICES_PATH
    )
    
    print(f"\nğŸ’¡ åŸºå‡†è¯•éªŒå®Œæˆ!")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {SAVE_DIR}")
    print(f"ğŸ”— è®­ç»ƒæµ‹è¯•é›†åˆ’åˆ†: {INDICES_PATH}")
    print(f"ğŸ“Š G-M1-10mæŒ‡æ ‡: RMSE={metrics['RMSE']:.4f}, Corr={metrics['Correlation']:.4f}")
    
    print(f"\næ¥ä¸‹æ¥å¯ä»¥ç”¨ run_wind_power_experiment å‡½æ•°è¿è¡Œå…¶ä»–13ä¸ªè¯•éªŒ!")
    
    # ç¤ºä¾‹ï¼šå¦‚ä½•è¿è¡Œå…¶ä»–è¯•éªŒ
    print(f"\nç¤ºä¾‹ï¼š")
    print(f"# G-M1-70m:")
    print(f"run_wind_power_experiment(DATA_PATH, 'G-M1-70mè·¯å¾„', INDICES_PATH, 'gfs_wind_speed_70m', 'G-M1-70m')")
    print(f"# G-M3å›ºå®šæƒé‡:")
    print(f"run_wind_power_experiment(DATA_PATH, 'G-M3è·¯å¾„', INDICES_PATH, ['gfs_wind_speed_10m', 'gfs_wind_speed_70m'], 'G-M3-Fixed', weights=[0.65, 0.35])")