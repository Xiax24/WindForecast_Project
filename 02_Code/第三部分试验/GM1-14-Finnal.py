#!/usr/bin/env python3
"""
ç®€åŒ–å‘½åç‰ˆå¢å¼ºè¯•éªŒç³»ç»Ÿ
ä½¿ç”¨ç®€æ´çš„è¯•éªŒåç§°ï¼šX-M2-10m, X-M3-10m, G-M4-Dualç­‰
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.metrics import mean_squared_error
import os
import json
import warnings
warnings.filterwarnings('ignore')

RANDOM_STATE = 42

def create_train_test_split_if_needed(data_path, indices_path, test_ratio=0.2):
    """å¦‚æœåˆ’åˆ†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºå®ƒ"""
    
    if os.path.exists(indices_path):
        print(f"  ğŸ“‹ ä½¿ç”¨å·²å­˜åœ¨çš„åˆ’åˆ†æ–‡ä»¶: {indices_path}")
        return
    
    print(f"  ğŸ“‹ åˆ›å»ºè®­ç»ƒæµ‹è¯•é›†åˆ’åˆ†æ–‡ä»¶...")
    
    # åŠ è½½æ•°æ®
    data = pd.read_csv(data_path)
    data['datetime'] = pd.to_datetime(data['datetime'])
    
    # åŸºç¡€æ¸…ç†
    data = data.dropna(subset=['power'])
    data = data[data['power'] >= 0]
    data = data.sort_values('datetime').reset_index(drop=True)
    
    # æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†
    total_samples = len(data)
    test_size = int(total_samples * test_ratio)
    train_size = total_samples - test_size
    
    # è®­ç»ƒé›†ï¼šå‰80%ï¼Œæµ‹è¯•é›†ï¼šå20%
    train_indices = list(range(train_size))
    test_indices = list(range(train_size, total_samples))
    
    # ä¿å­˜åˆ’åˆ†
    split_data = {
        'train_indices': train_indices,
        'test_indices': test_indices,
        'total_samples': total_samples,
        'train_size': len(train_indices),
        'test_size': len(test_indices),
        'test_ratio': test_ratio,
        'split_method': 'time_based'
    }
    
    # åˆ›å»ºç›®å½•
    os.makedirs(os.path.dirname(indices_path), exist_ok=True)
    
    # ä¿å­˜
    with open(indices_path, 'w') as f:
        json.dump(split_data, f, indent=2)
    
    print(f"  âœ… åˆ’åˆ†æ–‡ä»¶åˆ›å»ºå®Œæˆ: {len(train_indices)} è®­ç»ƒ, {len(test_indices)} æµ‹è¯•")

class WindCorrectionModel:
    """é£é€Ÿæ ¡æ­£æ¨¡å‹"""
    
    def __init__(self, target_height='10m', source='gfs'):
        self.target_height = target_height
        self.source = source
        self.model = None
        self.feature_names = None
        
    def prepare_correction_features(self, data):
        """å‡†å¤‡é£é€Ÿæ ¡æ­£çš„ç‰¹å¾"""
        
        features = pd.DataFrame()
        
        # ä¸»è¦é¢„æŠ¥é£é€Ÿ
        main_wind_col = f'{self.source}_wind_speed_{self.target_height}'
        features['forecast_wind'] = data[main_wind_col]
        features['forecast_wind_2'] = data[main_wind_col] ** 2
        
        # å…¶ä»–é«˜åº¦çš„é£é€Ÿ
        other_height = '70m' if self.target_height == '10m' else '10m'
        other_wind_col = f'{self.source}_wind_speed_{other_height}'
        if other_wind_col in data.columns:
            features['other_height_wind'] = data[other_wind_col]
            features['wind_shear'] = np.log(data[other_wind_col] / (data[main_wind_col] + 0.1))
        
        # æ¸©åº¦ç‰¹å¾
        if f'{self.source}_temperature_10m' in data.columns:
            features['temperature'] = data[f'{self.source}_temperature_10m']
        
        # æ—¶é—´ç‰¹å¾
        features['hour'] = data['datetime'].dt.hour
        features['month'] = data['datetime'].dt.month
        features['is_daytime'] = ((data['datetime'].dt.hour >= 6) & 
                                 (data['datetime'].dt.hour < 18)).astype(int)
        
        # æ»åç‰¹å¾
        features['wind_lag_1h'] = data[main_wind_col].shift(1)
        features['wind_lag_24h'] = data[main_wind_col].shift(24)
        
        # æ»šåŠ¨ç»Ÿè®¡
        features['wind_24h_mean'] = data[main_wind_col].rolling(window=24, min_periods=1).mean()
        
        # å¡«å……NaN
        features = features.fillna(method='bfill').fillna(method='ffill')
        
        self.feature_names = features.columns.tolist()
        return features
    
    def train(self, data, train_indices):
        """è®­ç»ƒé£é€Ÿæ ¡æ­£æ¨¡å‹"""
        
        features = self.prepare_correction_features(data)
        target_col = f'obs_wind_speed_{self.target_height}'
        target = data[target_col].values
        
        # åˆ’åˆ†è®­ç»ƒéªŒè¯é›†
        val_size = int(len(train_indices) * 0.2)
        train_only_indices = train_indices[:-val_size]
        val_indices = train_indices[-val_size:]
        
        X_train = features.iloc[train_only_indices]
        y_train = target[train_only_indices]
        X_val = features.iloc[val_indices]
        y_val = target[val_indices]
        
        # è®­ç»ƒLightGBM
        params = {
            'objective': 'regression',
            'metric': 'rmse',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.1,
            'feature_fraction': 0.9,
            'verbose': -1,
            'random_state': RANDOM_STATE
        }
        
        train_data = lgb.Dataset(X_train, label=y_train)
        valid_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
        
        self.model = lgb.train(
            params,
            train_data,
            valid_sets=[valid_data],
            num_boost_round=100,
            callbacks=[lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation(0)]
        )
        
        # è¯„ä¼°æ ¡æ­£æ•ˆæœ
        y_pred = self.model.predict(features.iloc[train_indices], num_iteration=self.model.best_iteration)
        y_true = target[train_indices]
        
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        corr = np.corrcoef(y_true, y_pred)[0, 1]
        
        return {'rmse': rmse, 'correlation': corr}
    
    def predict(self, data):
        """é¢„æµ‹æ ¡æ­£åçš„é£é€Ÿ"""
        features = self.prepare_correction_features(data)
        return self.model.predict(features, num_iteration=self.model.best_iteration)

class PowerPredictionModel:
    """åŠŸç‡é¢„æµ‹æ¨¡å‹"""
    
    def __init__(self):
        self.model = None
        self.feature_names = None
    
    def prepare_power_features(self, wind_data, original_data):
        """å‡†å¤‡åŠŸç‡é¢„æµ‹ç‰¹å¾"""
        
        features = pd.DataFrame()
        
        # é£é€Ÿç‰¹å¾
        if isinstance(wind_data, dict):
            # å¤šé£é€Ÿæƒ…å†µ
            for key, wind_values in wind_data.items():
                features[f'wind_{key}'] = wind_values
                features[f'wind_{key}_2'] = wind_values ** 2
                features[f'wind_{key}_3'] = wind_values ** 3
        else:
            # å•é£é€Ÿæƒ…å†µ
            features['wind'] = wind_data
            features['wind_2'] = wind_data ** 2
            features['wind_3'] = wind_data ** 3
        
        # æ—¶é—´ç‰¹å¾
        features['hour'] = original_data['datetime'].dt.hour
        features['month'] = original_data['datetime'].dt.month
        features['is_daytime'] = ((original_data['datetime'].dt.hour >= 6) & 
                                 (original_data['datetime'].dt.hour < 18)).astype(int)
        
        # æ»åç‰¹å¾
        main_wind = list(wind_data.values())[0] if isinstance(wind_data, dict) else wind_data
        features['wind_lag_1h'] = pd.Series(main_wind).shift(1)
        features['wind_lag_24h'] = pd.Series(main_wind).shift(24)
        
        # å¡«å……NaN
        features = features.fillna(method='bfill').fillna(method='ffill')
        
        self.feature_names = features.columns.tolist()
        return features
    
    def train(self, wind_data, original_data, train_indices, test_indices):
        """è®­ç»ƒåŠŸç‡é¢„æµ‹æ¨¡å‹"""
        
        features = self.prepare_power_features(wind_data, original_data)
        target = original_data['power'].values
        
        X_train = features.iloc[train_indices]
        X_test = features.iloc[test_indices]
        y_train = target[train_indices]
        y_test = target[test_indices]
        
        # è®­ç»ƒLightGBM
        params = {
            'objective': 'regression',
            'metric': 'rmse',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.1,
            'feature_fraction': 0.9,
            'verbose': -1,
            'random_state': RANDOM_STATE
        }
        
        train_data = lgb.Dataset(X_train, label=y_train)
        valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
        
        self.model = lgb.train(
            params,
            train_data,
            valid_sets=[valid_data],
            num_boost_round=100,
            callbacks=[lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation(0)]
        )
        
        # è¯„ä¼°åŠŸç‡é¢„æµ‹æ•ˆæœ
        y_pred = self.model.predict(X_test, num_iteration=self.model.best_iteration)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        corr = np.corrcoef(y_test, y_pred)[0, 1]
        
        return {
            'rmse': rmse,
            'correlation': corr,
            'predictions': y_pred,
            'actual': y_test,
            'features_test': X_test
        }

class AverageWindCorrectionModel:
    """å¹³å‡é£é€Ÿæ ¡æ­£æ¨¡å‹ï¼ˆç”¨äºX-M3ç³»åˆ—ï¼‰"""
    
    def __init__(self, target_height='10m', source_name='average'):
        self.target_height = target_height
        self.source_name = source_name
        self.model = None
        self.feature_names = None
        
    def prepare_correction_features(self, data, avg_wind_col):
        """å‡†å¤‡å¹³å‡é£é€Ÿæ ¡æ­£çš„ç‰¹å¾"""
        
        features = pd.DataFrame()
        
        # ä¸»è¦å¹³å‡é£é€Ÿç‰¹å¾
        features['forecast_wind'] = data[avg_wind_col]
        features['forecast_wind_2'] = data[avg_wind_col] ** 2
        
        # æ—¶é—´ç‰¹å¾
        features['hour'] = data['datetime'].dt.hour
        features['month'] = data['datetime'].dt.month
        features['is_daytime'] = ((data['datetime'].dt.hour >= 6) & 
                                 (data['datetime'].dt.hour < 18)).astype(int)
        
        # æ»åç‰¹å¾
        features['wind_lag_1h'] = data[avg_wind_col].shift(1)
        features['wind_lag_24h'] = data[avg_wind_col].shift(24)
        
        # æ»šåŠ¨ç»Ÿè®¡
        features['wind_24h_mean'] = data[avg_wind_col].rolling(window=24, min_periods=1).mean()
        
        # å¡«å……NaN
        features = features.fillna(method='bfill').fillna(method='ffill')
        
        self.feature_names = features.columns.tolist()
        return features
    
    def train(self, data, avg_wind_col, train_indices):
        """è®­ç»ƒå¹³å‡é£é€Ÿæ ¡æ­£æ¨¡å‹"""
        
        features = self.prepare_correction_features(data, avg_wind_col)
        target_col = f'obs_wind_speed_{self.target_height}'
        target = data[target_col].values
        
        # åˆ’åˆ†è®­ç»ƒéªŒè¯é›†
        val_size = int(len(train_indices) * 0.2)
        train_only_indices = train_indices[:-val_size]
        val_indices = train_indices[-val_size:]
        
        X_train = features.iloc[train_only_indices]
        y_train = target[train_only_indices]
        X_val = features.iloc[val_indices]
        y_val = target[val_indices]
        
        # è®­ç»ƒLightGBM
        params = {
            'objective': 'regression',
            'metric': 'rmse',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.1,
            'feature_fraction': 0.9,
            'verbose': -1,
            'random_state': RANDOM_STATE
        }
        
        train_data = lgb.Dataset(X_train, label=y_train)
        valid_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
        
        self.model = lgb.train(
            params,
            train_data,
            valid_sets=[valid_data],
            num_boost_round=100,
            callbacks=[lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation(0)]
        )
        
        # è¯„ä¼°æ ¡æ­£æ•ˆæœ
        y_pred = self.model.predict(features.iloc[train_indices], num_iteration=self.model.best_iteration)
        y_true = target[train_indices]
        
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        corr = np.corrcoef(y_true, y_pred)[0, 1]
        
        return {'rmse': rmse, 'correlation': corr}
    
    def predict(self, data, avg_wind_col):
        """é¢„æµ‹æ ¡æ­£åçš„å¹³å‡é£é€Ÿ"""
        features = self.prepare_correction_features(data, avg_wind_col)
        return self.model.predict(features, num_iteration=self.model.best_iteration)

def load_train_test_split(indices_path):
    """åŠ è½½è®­ç»ƒæµ‹è¯•é›†åˆ’åˆ†"""
    with open(indices_path, 'r') as f:
        indices = json.load(f)
    return indices['train_indices'], indices['test_indices']

def prepare_simple_features(data, wind_col):
    """ä¸ºM1ç³»åˆ—å‡†å¤‡ç®€å•ç‰¹å¾ï¼ˆç›´æ¥é¢„æµ‹ï¼‰"""
    features = pd.DataFrame()
    
    features['wind_speed'] = data[wind_col]
    features['wind_speed_2'] = data[wind_col] ** 2
    features['wind_speed_3'] = data[wind_col] ** 3
    
    features['hour'] = data['datetime'].dt.hour
    features['month'] = data['datetime'].dt.month
    features['is_daytime'] = ((data['datetime'].dt.hour >= 6) & 
                             (data['datetime'].dt.hour < 18)).astype(int)
    
    features['wind_lag_1h'] = data[wind_col].shift(1)
    features['wind_lag_24h'] = data[wind_col].shift(24)
    
    features = features.fillna(method='bfill').fillna(method='ffill')
    
    return features

def train_simple_lightgbm(X_train, y_train, X_test, y_test):
    """è®­ç»ƒç®€å•LightGBMæ¨¡å‹ï¼ˆM1ç³»åˆ—ç”¨ï¼‰"""
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.1,
        'verbose': -1,
        'random_state': RANDOM_STATE
    }
    
    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
    
    model = lgb.train(
        params,
        train_data,
        valid_sets=[valid_data],
        num_boost_round=100,
        callbacks=[lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation(0)]
    )
    
    return model

def run_experiment(data_path, save_dir, indices_path, exp_config):
    """è¿è¡ŒåŸæœ‰ç±»å‹çš„å•ä¸ªè¯•éªŒ"""
    
    print(f"\n{'='*60}")
    print(f"ğŸ”¬ è¯•éªŒ: {exp_config['name']}")
    print(f"ğŸ“ {exp_config['description']}")
    print(f"{'='*60}")
    
    os.makedirs(save_dir, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    print("  ğŸ“‚ åŠ è½½æ•°æ®...")
    data = pd.read_csv(data_path)
    data['datetime'] = pd.to_datetime(data['datetime'])
    
    # åŸºç¡€æ•°æ®æ¸…ç†
    data = data.dropna(subset=['power'])
    data = data[data['power'] >= 0]
    data = data.sort_values('datetime').reset_index(drop=True)
    
    # è·å–è®­ç»ƒæµ‹è¯•é›†åˆ’åˆ†
    train_indices, test_indices = load_train_test_split(indices_path)
    
    processing_log = []
    processing_log.append(f"è¯•éªŒç±»å‹: {exp_config['type']}")
    processing_log.append(f"æ•°æ®å¤§å°: {len(data)}")
    
    try:
        if exp_config['type'] == 'direct':
            # M1ç³»åˆ—ï¼šç›´æ¥é¢„æµ‹
            wind_col = exp_config['wind_col']
            processing_log.append(f"ç›´æ¥é¢„æµ‹é£é€Ÿåˆ—: {wind_col}")
            
            # æ¸…ç†é£é€Ÿæ•°æ®
            data = data.dropna(subset=[wind_col])
            data = data[(data[wind_col] >= 0) & (data[wind_col] <= 50)]
            
            # é‡æ–°è·å–æœ‰æ•ˆç´¢å¼•
            valid_indices = data.index.tolist()
            train_indices = [i for i in train_indices if i in valid_indices]
            test_indices = [i for i in test_indices if i in valid_indices]
            
            # å‡†å¤‡ç‰¹å¾å’Œè®­ç»ƒæ¨¡å‹
            features = prepare_simple_features(data, wind_col)
            target = data['power'].values
            
            X_train = features.iloc[train_indices]
            X_test = features.iloc[test_indices]
            y_train = target[train_indices]
            y_test = target[test_indices]
            
            print("  ğŸš€ è®­ç»ƒç›´æ¥é¢„æµ‹æ¨¡å‹...")
            model = train_simple_lightgbm(X_train, y_train, X_test, y_test)
            
            y_pred = model.predict(X_test, num_iteration=model.best_iteration)
            rmse = np.sqrt(mean_squared_error(y_test, y_pred))
            corr = np.corrcoef(y_test, y_pred)[0, 1]
            
            processing_log.append(f"ç›´æ¥é¢„æµ‹æ€§èƒ½: RMSE={rmse:.4f}, ç›¸å…³ç³»æ•°={corr:.4f}")
            
            # ä¿å­˜ç»“æœ
            detailed_results = pd.DataFrame({
                'datetime': data.iloc[test_indices]['datetime'].values,
                'actual_power': y_test,
                'predicted_power': y_pred,
                'wind_speed': data.iloc[test_indices][wind_col].values,
                'error': y_pred - y_test,
                'abs_error': np.abs(y_pred - y_test)
            })
            
        elif exp_config['type'] == 'two_step':
            # M2ç³»åˆ—ï¼šä¸¤æ­¥æ³•æ ¡æ­£
            wind_source = exp_config['wind_source']
            wind_height = exp_config['wind_height']
            
            processing_log.append(f"ä¸¤æ­¥æ³•æ ¡æ­£: {wind_source.upper()}-{wind_height}")
            
            # æ¸…ç†æ•°æ®
            required_cols = [f'{wind_source}_wind_speed_{wind_height}', f'obs_wind_speed_{wind_height}']
            data = data.dropna(subset=required_cols)
            for col in required_cols:
                data = data[(data[col] >= 0) & (data[col] <= 50)]
            
            # é‡æ–°è·å–æœ‰æ•ˆç´¢å¼•
            valid_indices = data.index.tolist()
            train_indices = [i for i in train_indices if i in valid_indices]
            test_indices = [i for i in test_indices if i in valid_indices]
            
            # ç¬¬ä¸€æ­¥ï¼šé£é€Ÿæ ¡æ­£
            print("  ğŸ¯ ç¬¬ä¸€æ­¥ï¼šé£é€Ÿæ ¡æ­£...")
            wind_corrector = WindCorrectionModel(target_height=wind_height, source=wind_source)
            correction_stats = wind_corrector.train(data, train_indices)
            
            corrected_wind = wind_corrector.predict(data)
            processing_log.append(f"é£é€Ÿæ ¡æ­£æ€§èƒ½: RMSE={correction_stats['rmse']:.4f}")
            
            # ç¬¬äºŒæ­¥ï¼šåŠŸç‡é¢„æµ‹
            print("  âš¡ ç¬¬äºŒæ­¥ï¼šåŠŸç‡é¢„æµ‹...")
            power_predictor = PowerPredictionModel()
            power_results = power_predictor.train(corrected_wind, data, train_indices, test_indices)
            
            processing_log.append(f"åŠŸç‡é¢„æµ‹æ€§èƒ½: RMSE={power_results['rmse']:.4f}")
            
            rmse = power_results['rmse']
            corr = power_results['correlation']
            y_pred = power_results['predictions']
            y_test = power_results['actual']
            
            # ä¿å­˜ç»“æœ
            detailed_results = pd.DataFrame({
                'datetime': data.iloc[test_indices]['datetime'].values,
                'actual_power': y_test,
                'predicted_power': y_pred,
                'corrected_wind': corrected_wind[test_indices],
                'original_wind': data.iloc[test_indices][f'{wind_source}_wind_speed_{wind_height}'].values,
                'observed_wind': data.iloc[test_indices][f'obs_wind_speed_{wind_height}'].values,
                'error': y_pred - y_test,
                'abs_error': np.abs(y_pred - y_test)
            })
            
        elif exp_config['type'] == 'fusion':
            # Fusionç³»åˆ—ï¼šå¤šé£é€Ÿèåˆ
            wind_configs = exp_config['wind_configs']
            fusion_strategy = exp_config['fusion_strategy']
            
            processing_log.append(f"èåˆç­–ç•¥: {fusion_strategy}")
            processing_log.append(f"é£é€Ÿé…ç½®: {wind_configs}")
            
            # æ¸…ç†æ‰€æœ‰éœ€è¦çš„é£é€Ÿæ•°æ®
            all_required_cols = ['power']
            for config in wind_configs:
                source = config['source']
                height = config['height']
                all_required_cols.extend([f'{source}_wind_speed_{height}'])
                if fusion_strategy != 'direct':
                    all_required_cols.append(f'obs_wind_speed_{height}')
            
            data = data.dropna(subset=all_required_cols)
            for col in all_required_cols:
                if 'wind_speed' in col:
                    data = data[(data[col] >= 0) & (data[col] <= 50)]
            
            # é‡æ–°è·å–æœ‰æ•ˆç´¢å¼•
            valid_indices = data.index.tolist()
            train_indices = [i for i in train_indices if i in valid_indices]
            test_indices = [i for i in test_indices if i in valid_indices]
            
            # æ ¡æ­£åèåˆ
            print("  ğŸ¯ å¤šé£é€Ÿæ ¡æ­£...")
            
            corrected_winds = {}
            correction_stats_all = {}
            
            # åˆ†åˆ«æ ¡æ­£æ¯ä¸ªé£é€Ÿ
            for config in wind_configs:
                source = config['source']
                height = config['height']
                key = f"{source}_{height}"
                
                wind_corrector = WindCorrectionModel(target_height=height, source=source)
                correction_stats = wind_corrector.train(data, train_indices)
                correction_stats_all[key] = correction_stats
                
                corrected_winds[key] = wind_corrector.predict(data)
                
                processing_log.append(f"{key}æ ¡æ­£æ€§èƒ½: RMSE={correction_stats['rmse']:.4f}")
            
            # èåˆæ ¡æ­£åçš„é£é€Ÿ
            print("  ğŸ”— èåˆæ ¡æ­£åé£é€Ÿ...")
            
            weights = exp_config.get('weights', [0.25, 0.25, 0.25, 0.25])
            fused_wind = np.zeros(len(data))
            for i, key in enumerate(corrected_winds.keys()):
                fused_wind += weights[i] * corrected_winds[key]
            
            # åŠŸç‡é¢„æµ‹
            print("  âš¡ åŠŸç‡é¢„æµ‹...")
            power_predictor = PowerPredictionModel()
            power_results = power_predictor.train(fused_wind, data, train_indices, test_indices)
            
            rmse = power_results['rmse']
            corr = power_results['correlation']
            y_pred = power_results['predictions']
            y_test = power_results['actual']
            
            # ä¿å­˜ç»“æœ
            detailed_results = pd.DataFrame({
                'datetime': data.iloc[test_indices]['datetime'].values,
                'actual_power': y_test,
                'predicted_power': y_pred,
                'fused_corrected_wind': fused_wind[test_indices],
                'error': y_pred - y_test,
                'abs_error': np.abs(y_pred - y_test)
            })
            
            # æ·»åŠ å„ä¸ªæ ¡æ­£åçš„é£é€Ÿ
            for key, wind_values in corrected_winds.items():
                detailed_results[f'corrected_{key}'] = wind_values[test_indices]
        
        print(f"  âœ… å®Œæˆ! RMSE: {rmse:.4f}, ç›¸å…³ç³»æ•°: {corr:.4f}")
        
        # ä¿å­˜æ‰€æœ‰ç»“æœæ–‡ä»¶
        detailed_results.to_csv(os.path.join(save_dir, f'{exp_config["name"]}_detailed_results.csv'), index=False)
        
        # ä¿å­˜å¤„ç†æ—¥å¿—
        process_log = {
            'experiment_name': exp_config['name'],
            'experiment_config': exp_config,
            'processing_log': processing_log,
            'final_performance': {'rmse': rmse, 'correlation': corr}
        }
        
        with open(os.path.join(save_dir, f'{exp_config["name"]}_process_log.json'), 'w') as f:
            json.dump(process_log, f, indent=2, default=str)
        
        # ä¿å­˜æŒ‡æ ‡
        metrics = {'RMSE': rmse, 'Correlation': corr}
        with open(os.path.join(save_dir, f'{exp_config["name"]}_metrics.json'), 'w') as f:
            json.dump(metrics, f, indent=2)
        
        return metrics
        
    except Exception as e:
        print(f"  âŒ å¤±è´¥: {str(e)}")
        return {'RMSE': None, 'Correlation': None, 'error': str(e)}

def run_dual_height_corrected_fusion_experiment(data_path, save_dir, indices_path, exp_config):
    """è¿è¡ŒåŒé«˜åº¦æ ¡æ­£åèåˆè¯•éªŒï¼ˆG-M4-Dual, E-M4-Dualï¼‰"""
    
    print(f"ğŸ”¬ åŒé«˜åº¦æ ¡æ­£èåˆè¯•éªŒ: {exp_config['name']}")
    
    os.makedirs(save_dir, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    data = pd.read_csv(data_path)
    data['datetime'] = pd.to_datetime(data['datetime'])
    
    source = exp_config['source']
    heights = exp_config['heights']
    
    # æ•°æ®æ¸…ç†
    required_cols = ['power']
    for height in heights:
        required_cols.extend([
            f'{source}_wind_speed_{height}',
            f'obs_wind_speed_{height}'
        ])
    
    data = data.dropna(subset=required_cols)
    for col in required_cols:
        if 'wind_speed' in col:
            data = data[(data[col] >= 0) & (data[col] <= 50)]
    
    data = data.sort_values('datetime').reset_index(drop=True)
    
    # è·å–è®­ç»ƒæµ‹è¯•åˆ’åˆ†
    train_indices, test_indices = load_train_test_split(indices_path)
    
    # é‡æ–°è·å–æœ‰æ•ˆç´¢å¼•
    valid_indices = data.index.tolist()
    train_indices = [i for i in train_indices if i in valid_indices]
    test_indices = [i for i in test_indices if i in valid_indices]
    
    processing_log = []
    processing_log.append(f"åŒé«˜åº¦æ ¡æ­£èåˆè¯•éªŒ: {exp_config['name']}")
    processing_log.append(f"æ•°æ®å¤§å°: {len(data)}")
    
    # ç¬¬ä¸€æ­¥ï¼šåˆ†åˆ«æ ¡æ­£ä¸¤ä¸ªé«˜åº¦çš„é£é€Ÿ
    print(f"  ğŸ¯ ç¬¬ä¸€æ­¥ï¼šåˆ†åˆ«æ ¡æ­£{heights}é£é€Ÿ...")
    
    corrected_winds = {}
    correction_stats = {}
    
    for height in heights:
        print(f"    æ ¡æ­£{source.upper()}-{height}...")
        
        wind_corrector = WindCorrectionModel(target_height=height, source=source)
        correction_stat = wind_corrector.train(data, train_indices)
        correction_stats[f'{source}_{height}'] = correction_stat
        
        corrected_winds[f'{source}_{height}'] = wind_corrector.predict(data)
        
        print(f"    {source.upper()}-{height}æ ¡æ­£RMSE: {correction_stat['rmse']:.4f}")
        processing_log.append(f"{source}_{height}æ ¡æ­£æ€§èƒ½: RMSE={correction_stat['rmse']:.4f}")
    
    # ç¬¬äºŒæ­¥ï¼šèåˆæ ¡æ­£åçš„é£é€Ÿ
    print(f"  ğŸ”— ç¬¬äºŒæ­¥ï¼šèåˆæ ¡æ­£åé£é€Ÿ...")
    
    weights = exp_config['fusion_weights']
    fused_corrected_wind = np.zeros(len(data))
    
    for i, height in enumerate(heights):
        key = f'{source}_{height}'
        fused_corrected_wind += weights[i] * corrected_winds[key]
        print(f"    {key}æƒé‡: {weights[i]}")
        processing_log.append(f"{key}æƒé‡: {weights[i]}")
    
    # ç¬¬ä¸‰æ­¥ï¼šç”¨èåˆé£é€Ÿé¢„æµ‹åŠŸç‡
    print(f"  âš¡ ç¬¬ä¸‰æ­¥ï¼šåŠŸç‡é¢„æµ‹...")
    
    power_predictor = PowerPredictionModel()
    power_results = power_predictor.train(
        fused_corrected_wind, data, train_indices, test_indices
    )
    
    rmse = power_results['rmse']
    corr = power_results['correlation']
    y_pred = power_results['predictions']
    y_test = power_results['actual']
    
    processing_log.append(f"æœ€ç»ˆæµ‹è¯•æ€§èƒ½: RMSE={rmse:.4f}, ç›¸å…³ç³»æ•°={corr:.4f}")
    
    print(f"  âœ… å®Œæˆ! RMSE: {rmse:.4f}, ç›¸å…³ç³»æ•°: {corr:.4f}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    detailed_results = pd.DataFrame({
        'datetime': data.iloc[test_indices]['datetime'].values,
        'actual_power': y_test,
        'predicted_power': y_pred,
        'fused_corrected_wind': fused_corrected_wind[test_indices],
        'error': y_pred - y_test,
        'abs_error': np.abs(y_pred - y_test)
    })
    
    # æ·»åŠ å„ä¸ªæ ¡æ­£åçš„é£é€Ÿ
    for height in heights:
        key = f'{source}_{height}'
        detailed_results[f'corrected_{key}'] = corrected_winds[key][test_indices]
        detailed_results[f'original_{key}'] = data.iloc[test_indices][f'{source}_wind_speed_{height}'].values
    
    # ä¿å­˜ç»“æœ
    detailed_results.to_csv(os.path.join(save_dir, f'{exp_config["name"]}_detailed_results.csv'), index=False)
    
    # ä¿å­˜å¤„ç†æ—¥å¿—
    process_log = {
        'experiment_name': exp_config['name'],
        'experiment_config': exp_config,
        'correction_stats': correction_stats,
        'fusion_weights': weights,
        'processing_log': processing_log,
        'final_performance': {'rmse': rmse, 'correlation': corr}
    }
    
    with open(os.path.join(save_dir, f'{exp_config["name"]}_process_log.json'), 'w') as f:
        json.dump(process_log, f, indent=2, default=str)
    
    # ä¿å­˜æŒ‡æ ‡
    metrics = {'RMSE': rmse, 'Correlation': corr}
    with open(os.path.join(save_dir, f'{exp_config["name"]}_metrics.json'), 'w') as f:
        json.dump(metrics, f, indent=2)
    
    return metrics

def run_cross_source_corrected_fusion_experiment(data_path, save_dir, indices_path, exp_config):
    """è¿è¡Œè·¨æºæ ¡æ­£èåˆè¯•éªŒï¼ˆX-M2-10m, X-M2-70mï¼‰"""
    
    print(f"ğŸ”¬ è·¨æºæ ¡æ­£èåˆè¯•éªŒ: {exp_config['name']}")
    
    os.makedirs(save_dir, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    data = pd.read_csv(data_path)
    data['datetime'] = pd.to_datetime(data['datetime'])
    
    height = exp_config['height']
    sources = exp_config['sources']
    
    # æ•°æ®æ¸…ç†
    required_cols = ['power', f'obs_wind_speed_{height}']
    for source in sources:
        required_cols.append(f'{source}_wind_speed_{height}')
    
    data = data.dropna(subset=required_cols)
    for col in required_cols:
        if 'wind_speed' in col:
            data = data[(data[col] >= 0) & (data[col] <= 50)]
    
    data = data.sort_values('datetime').reset_index(drop=True)
    
    # è·å–è®­ç»ƒæµ‹è¯•åˆ’åˆ†
    train_indices, test_indices = load_train_test_split(indices_path)
    
    # é‡æ–°è·å–æœ‰æ•ˆç´¢å¼•
    valid_indices = data.index.tolist()
    train_indices = [i for i in train_indices if i in valid_indices]
    test_indices = [i for i in test_indices if i in valid_indices]
    
    processing_log = []
    processing_log.append(f"è·¨æºæ ¡æ­£èåˆè¯•éªŒ: {exp_config['name']}")
    processing_log.append(f"æ•°æ®å¤§å°: {len(data)}")
    
    # ç¬¬ä¸€æ­¥ï¼šåˆ†åˆ«æ ¡æ­£æ¯ä¸ªæºçš„é£é€Ÿ
    print(f"  ğŸ¯ ç¬¬ä¸€æ­¥ï¼šåˆ†åˆ«æ ¡æ­£{sources}çš„{height}é£é€Ÿ...")
    
    corrected_winds = {}
    correction_stats = {}
    
    for source in sources:
        print(f"    æ ¡æ­£{source.upper()}-{height}...")
        
        wind_corrector = WindCorrectionModel(target_height=height, source=source)
        correction_stat = wind_corrector.train(data, train_indices)
        correction_stats[f'{source}_{height}'] = correction_stat
        
        corrected_winds[f'{source}_{height}'] = wind_corrector.predict(data)
        
        print(f"    {source.upper()}-{height}æ ¡æ­£RMSE: {correction_stat['rmse']:.4f}")
        processing_log.append(f"{source}_{height}æ ¡æ­£æ€§èƒ½: RMSE={correction_stat['rmse']:.4f}")
    
    # ç¬¬äºŒæ­¥ï¼šèåˆæ ¡æ­£åçš„é£é€Ÿ
    print(f"  ğŸ”— ç¬¬äºŒæ­¥ï¼šèåˆæ ¡æ­£åé£é€Ÿ...")
    
    weights = exp_config['fusion_weights']
    fused_corrected_wind = np.zeros(len(data))
    
    for i, source in enumerate(sources):
        key = f'{source}_{height}'
        fused_corrected_wind += weights[i] * corrected_winds[key]
        print(f"    {key}æƒé‡: {weights[i]}")
        processing_log.append(f"{key}æƒé‡: {weights[i]}")
    
    # ç¬¬ä¸‰æ­¥ï¼šç”¨èåˆé£é€Ÿé¢„æµ‹åŠŸç‡
    print(f"  âš¡ ç¬¬ä¸‰æ­¥ï¼šåŠŸç‡é¢„æµ‹...")
    
    power_predictor = PowerPredictionModel()
    power_results = power_predictor.train(
        fused_corrected_wind, data, train_indices, test_indices
    )
    
    rmse = power_results['rmse']
    corr = power_results['correlation']
    y_pred = power_results['predictions']
    y_test = power_results['actual']
    
    processing_log.append(f"æœ€ç»ˆæµ‹è¯•æ€§èƒ½: RMSE={rmse:.4f}, ç›¸å…³ç³»æ•°={corr:.4f}")
    
    print(f"  âœ… å®Œæˆ! RMSE: {rmse:.4f}, ç›¸å…³ç³»æ•°: {corr:.4f}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    detailed_results = pd.DataFrame({
        'datetime': data.iloc[test_indices]['datetime'].values,
        'actual_power': y_test,
        'predicted_power': y_pred,
        'fused_corrected_wind': fused_corrected_wind[test_indices],
        'error': y_pred - y_test,
        'abs_error': np.abs(y_pred - y_test)
    })
    
    # æ·»åŠ å„ä¸ªæ ¡æ­£åçš„é£é€Ÿ
    for source in sources:
        key = f'{source}_{height}'
        detailed_results[f'corrected_{key}'] = corrected_winds[key][test_indices]
        detailed_results[f'original_{key}'] = data.iloc[test_indices][f'{source}_wind_speed_{height}'].values
    
    # ä¿å­˜ç»“æœ
    detailed_results.to_csv(os.path.join(save_dir, f'{exp_config["name"]}_detailed_results.csv'), index=False)
    
    # ä¿å­˜å¤„ç†æ—¥å¿—
    process_log = {
        'experiment_name': exp_config['name'],
        'experiment_config': exp_config,
        'correction_stats': correction_stats,
        'fusion_weights': weights,
        'processing_log': processing_log,
        'final_performance': {'rmse': rmse, 'correlation': corr}
    }
    
    with open(os.path.join(save_dir, f'{exp_config["name"]}_process_log.json'), 'w') as f:
        json.dump(process_log, f, indent=2, default=str)
    
    # ä¿å­˜æŒ‡æ ‡
    metrics = {'RMSE': rmse, 'Correlation': corr}
    with open(os.path.join(save_dir, f'{exp_config["name"]}_metrics.json'), 'w') as f:
        json.dump(metrics, f, indent=2)
    
    return metrics

def run_cross_source_average_corrected_experiment(data_path, save_dir, indices_path, exp_config):
    """è¿è¡Œè·¨æºå¹³å‡æ ¡æ­£è¯•éªŒï¼ˆX-M3-10m, X-M3-70mï¼‰- ä½ çš„åˆ›æ–°æ–¹æ³•"""
    
    print(f"ğŸ”¬ è·¨æºå¹³å‡æ ¡æ­£è¯•éªŒ: {exp_config['name']} â­")
    
    os.makedirs(save_dir, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    data = pd.read_csv(data_path)
    data['datetime'] = pd.to_datetime(data['datetime'])
    
    height = exp_config['height']
    sources = exp_config['sources']
    
    # æ•°æ®æ¸…ç†
    required_cols = ['power', f'obs_wind_speed_{height}']
    for source in sources:
        required_cols.append(f'{source}_wind_speed_{height}')
    
    data = data.dropna(subset=required_cols)
    for col in required_cols:
        if 'wind_speed' in col:
            data = data[(data[col] >= 0) & (data[col] <= 50)]
    
    data = data.sort_values('datetime').reset_index(drop=True)
    
    # è·å–è®­ç»ƒæµ‹è¯•åˆ’åˆ†
    train_indices, test_indices = load_train_test_split(indices_path)
    
    # é‡æ–°è·å–æœ‰æ•ˆç´¢å¼•
    valid_indices = data.index.tolist()
    train_indices = [i for i in train_indices if i in valid_indices]
    test_indices = [i for i in test_indices if i in valid_indices]
    
    processing_log = []
    processing_log.append(f"è·¨æºå¹³å‡æ ¡æ­£è¯•éªŒ: {exp_config['name']} (åˆ›æ–°æ–¹æ³•)")
    processing_log.append(f"æ•°æ®å¤§å°: {len(data)}")
    
    # ç¬¬ä¸€æ­¥ï¼šè®¡ç®—è·¨æºå¹³å‡é£é€Ÿ
    print(f"  ğŸ“Š ç¬¬ä¸€æ­¥ï¼šè®¡ç®—{sources}åœ¨{height}çš„å¹³å‡é£é€Ÿ...")
    
    wind_values = []
    for source in sources:
        wind_col = f'{source}_wind_speed_{height}'
        wind_values.append(data[wind_col].values)
    
    # è®¡ç®—å¹³å‡å€¼
    average_wind = np.mean(wind_values, axis=0)
    
    print(f"    åŸå§‹é£é€ŸèŒƒå›´: {np.min(wind_values):.2f} - {np.max(wind_values):.2f}")
    print(f"    å¹³å‡é£é€ŸèŒƒå›´: {np.min(average_wind):.2f} - {np.max(average_wind):.2f}")
    
    processing_log.append(f"è®¡ç®—{sources}å¹³å‡é£é€Ÿï¼ŒèŒƒå›´: {np.min(average_wind):.2f} - {np.max(average_wind):.2f}")
    
    # ç¬¬äºŒæ­¥ï¼šè®­ç»ƒå¹³å‡é£é€Ÿçš„æ ¡æ­£æ¨¡å‹
    print(f"  ğŸ¯ ç¬¬äºŒæ­¥ï¼šè®­ç»ƒå¹³å‡é£é€Ÿæ ¡æ­£æ¨¡å‹...")
    
    # æ·»åŠ å¹³å‡é£é€Ÿåˆ—åˆ°æ•°æ®ä¸­
    avg_wind_col = f'avg_wind_speed_{height}'
    data[avg_wind_col] = average_wind
    
    avg_corrector = AverageWindCorrectionModel(target_height=height, source_name='average')
    correction_stats = avg_corrector.train(data, avg_wind_col, train_indices)
    
    print(f"    å¹³å‡é£é€Ÿæ ¡æ­£RMSE: {correction_stats['rmse']:.4f}")
    processing_log.append(f"å¹³å‡é£é€Ÿæ ¡æ­£æ€§èƒ½: RMSE={correction_stats['rmse']:.4f}")
    
    # ç¬¬ä¸‰æ­¥ï¼šè·å–æ ¡æ­£åçš„å¹³å‡é£é€Ÿ
    print(f"  ğŸ”§ ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆæ ¡æ­£åçš„å¹³å‡é£é€Ÿ...")
    
    corrected_average_wind = avg_corrector.predict(data, avg_wind_col)
    
    # ç¬¬å››æ­¥ï¼šç”¨æ ¡æ­£åçš„å¹³å‡é£é€Ÿé¢„æµ‹åŠŸç‡
    print(f"  âš¡ ç¬¬å››æ­¥ï¼šåŠŸç‡é¢„æµ‹...")
    
    power_predictor = PowerPredictionModel()
    power_results = power_predictor.train(
        corrected_average_wind, data, train_indices, test_indices
    )
    
    rmse = power_results['rmse']
    corr = power_results['correlation']
    y_pred = power_results['predictions']
    y_test = power_results['actual']
    
    processing_log.append(f"æœ€ç»ˆæµ‹è¯•æ€§èƒ½: RMSE={rmse:.4f}, ç›¸å…³ç³»æ•°={corr:.4f}")
    
    print(f"  âœ… å®Œæˆ! RMSE: {rmse:.4f}, ç›¸å…³ç³»æ•°: {corr:.4f}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    detailed_results = pd.DataFrame({
        'datetime': data.iloc[test_indices]['datetime'].values,
        'actual_power': y_test,
        'predicted_power': y_pred,
        'corrected_average_wind': corrected_average_wind[test_indices],
        'original_average_wind': average_wind[test_indices],
        'error': y_pred - y_test,
        'abs_error': np.abs(y_pred - y_test)
    })
    
    # æ·»åŠ å„ä¸ªæºçš„åŸå§‹é£é€Ÿ
    for source in sources:
        wind_col = f'{source}_wind_speed_{height}'
        detailed_results[f'original_{source}_{height}'] = data.iloc[test_indices][wind_col].values
    
    # ä¿å­˜ç»“æœ
    detailed_results.to_csv(os.path.join(save_dir, f'{exp_config["name"]}_detailed_results.csv'), index=False)
    
    # ä¿å­˜å¤„ç†æ—¥å¿—
    process_log = {
        'experiment_name': exp_config['name'],
        'experiment_config': exp_config,
        'sources_averaged': sources,
        'height': height,
        'correction_stats': correction_stats,
        'processing_log': processing_log,
        'final_performance': {'rmse': rmse, 'correlation': corr}
    }
    
    with open(os.path.join(save_dir, f'{exp_config["name"]}_process_log.json'), 'w') as f:
        json.dump(process_log, f, indent=2, default=str)
    
    # ä¿å­˜æŒ‡æ ‡
    metrics = {'RMSE': rmse, 'Correlation': corr}
    with open(os.path.join(save_dir, f'{exp_config["name"]}_metrics.json'), 'w') as f:
        json.dump(metrics, f, indent=2)
    
    return metrics

def run_enhanced_experiment(data_path, save_dir, indices_path, exp_config):
    """è¿è¡Œå¢å¼ºç‰ˆå•ä¸ªè¯•éªŒï¼ˆæ”¯æŒæ–°çš„è¯•éªŒç±»å‹ï¼‰"""
    
    try:
        # æ ¹æ®è¯•éªŒç±»å‹è°ƒç”¨ä¸åŒçš„å‡½æ•°
        if exp_config['type'] == 'dual_height_corrected_fusion':
            return run_dual_height_corrected_fusion_experiment(data_path, save_dir, indices_path, exp_config)
        
        elif exp_config['type'] == 'cross_source_corrected_fusion':
            return run_cross_source_corrected_fusion_experiment(data_path, save_dir, indices_path, exp_config)
        
        elif exp_config['type'] == 'cross_source_average_corrected':
            return run_cross_source_average_corrected_experiment(data_path, save_dir, indices_path, exp_config)
        
        else:
            # å¯¹äºåŸæœ‰ç±»å‹ï¼Œä½¿ç”¨åŸæ¥çš„run_experimentå‡½æ•°
            return run_experiment(data_path, save_dir, indices_path, exp_config)
            
    except Exception as e:
        print(f"  âŒ å¤±è´¥: {str(e)}")
        return {'RMSE': None, 'Correlation': None, 'error': str(e)}

def run_enhanced_experiments(data_path, base_save_dir, indices_path):
    """è¿è¡Œç®€åŒ–å‘½åç‰ˆå¢å¼ºè¯•éªŒç³»ç»Ÿ"""
    
    print("=" * 80)
    print("ğŸš€ è¿è¡Œç®€åŒ–å‘½åç‰ˆå¢å¼ºè¯•éªŒç³»ç»Ÿ")
    print("=" * 80)
    
    # ç¡®ä¿è®­ç»ƒæµ‹è¯•é›†åˆ’åˆ†æ–‡ä»¶å­˜åœ¨
    create_train_test_split_if_needed(data_path, indices_path)
    
    # å®šä¹‰ç®€åŒ–å‘½åçš„è¯•éªŒé…ç½®
    experiments = [
        # M1ç³»åˆ—ï¼šç›´æ¥é¢„æµ‹ï¼ˆä¿ç•™4ä¸ªï¼‰
        {
            'name': 'G-M1-10m',
            'description': 'GFSåŸå§‹10mé£é€Ÿç›´æ¥é¢„æµ‹åŠŸç‡',
            'type': 'direct',
            'wind_col': 'gfs_wind_speed_10m'
        },
        {
            'name': 'G-M1-70m', 
            'description': 'GFSåŸå§‹70mé£é€Ÿç›´æ¥é¢„æµ‹åŠŸç‡',
            'type': 'direct',
            'wind_col': 'gfs_wind_speed_70m'
        },
        {
            'name': 'E-M1-10m',
            'description': 'ECåŸå§‹10mé£é€Ÿç›´æ¥é¢„æµ‹åŠŸç‡', 
            'type': 'direct',
            'wind_col': 'ec_wind_speed_10m'
        },
        {
            'name': 'E-M1-70m',
            'description': 'ECåŸå§‹70mé£é€Ÿç›´æ¥é¢„æµ‹åŠŸç‡',
            'type': 'direct', 
            'wind_col': 'ec_wind_speed_70m'
        },
        
        # M2ç³»åˆ—ï¼šä¸¤æ­¥æ³•MLæ ¡æ­£ï¼ˆä¿ç•™4ä¸ªï¼‰
        {
            'name': 'G-M2-10m',
            'description': 'GFS 10mé£é€Ÿä¸¤æ­¥æ³•MLæ ¡æ­£',
            'type': 'two_step',
            'wind_source': 'gfs',
            'wind_height': '10m'
        },
        {
            'name': 'G-M2-70m',
            'description': 'GFS 70mé£é€Ÿä¸¤æ­¥æ³•MLæ ¡æ­£',
            'type': 'two_step',
            'wind_source': 'gfs', 
            'wind_height': '70m'
        },
        {
            'name': 'E-M2-10m',
            'description': 'EC 10mé£é€Ÿä¸¤æ­¥æ³•MLæ ¡æ­£',
            'type': 'two_step',
            'wind_source': 'ec',
            'wind_height': '10m'
        },
        {
            'name': 'E-M2-70m',
            'description': 'EC 70mé£é€Ÿä¸¤æ­¥æ³•MLæ ¡æ­£', 
            'type': 'two_step',
            'wind_source': 'ec',
            'wind_height': '70m'
        },
        
        # ä¿ç•™Fusion-M1ï¼ˆ1ä¸ªï¼‰
        {
            'name': 'Fusion-M1',
            'description': 'å››é£é€Ÿæ ¡æ­£åé™æ€èåˆ',
            'type': 'fusion',
            'wind_configs': [
                {'source': 'ec', 'height': '10m'},
                {'source': 'ec', 'height': '70m'},
                {'source': 'gfs', 'height': '10m'},
                {'source': 'gfs', 'height': '70m'}
            ],
            'fusion_strategy': 'corrected',
            'weights': [0.25, 0.25, 0.25, 0.25]
        },
        
        # ========== æ–°å¢è¯•éªŒï¼ˆç®€åŒ–å‘½åï¼‰==========
        
        # æ–°å¢1: ECåŒé«˜åº¦åˆ†åˆ«æ ¡æ­£åèåˆ
        {
            'name': 'E-M4-Dual',
            'description': 'ECä¸¤é«˜åº¦åˆ†åˆ«æ ¡æ­£åå‡æƒèåˆ',
            'type': 'dual_height_corrected_fusion',
            'source': 'ec',
            'heights': ['10m', '70m'],
            'fusion_weights': [0.5, 0.5]  # å‡æƒ
        },
        
        # æ–°å¢2: GFSåŒé«˜åº¦åˆ†åˆ«æ ¡æ­£åèåˆ  
        {
            'name': 'G-M4-Dual',
            'description': 'GFSä¸¤é«˜åº¦åˆ†åˆ«æ ¡æ­£åå‡æƒèåˆ',
            'type': 'dual_height_corrected_fusion',
            'source': 'gfs', 
            'heights': ['10m', '70m'],
            'fusion_weights': [0.5, 0.5]  # å‡æƒ
        },
        
        # æ–°å¢3: è·¨æº10mæ ¡æ­£èåˆ
        {
            'name': 'X-M2-10m',
            'description': 'GFSå’ŒECçš„10måˆ†åˆ«æ ¡æ­£åå‡æƒèåˆ',
            'type': 'cross_source_corrected_fusion',
            'height': '10m',
            'sources': ['gfs', 'ec'],
            'fusion_weights': [0.5, 0.5]  # å‡æƒ
        },
        
        # æ–°å¢4: è·¨æº70mæ ¡æ­£èåˆ
        {
            'name': 'X-M2-70m', 
            'description': 'GFSå’ŒECçš„70måˆ†åˆ«æ ¡æ­£åå‡æƒèåˆ',
            'type': 'cross_source_corrected_fusion',
            'height': '70m',
            'sources': ['gfs', 'ec'],
            'fusion_weights': [0.5, 0.5]  # å‡æƒ
        },
        
        # æ–°å¢5: è·¨æº10må¹³å‡æ ¡æ­£ï¼ˆä½ çš„åˆ›æ–°æ–¹æ³•ï¼‰â­
        {
            'name': 'X-M3-10m',
            'description': 'GFSå’ŒECçš„10må¹³å‡åæ ¡æ­£å†é¢„æµ‹åŠŸç‡ â­åˆ›æ–°æ–¹æ³•',
            'type': 'cross_source_average_corrected',
            'height': '10m',
            'sources': ['gfs', 'ec']
        },
        
        # æ–°å¢6: è·¨æº70må¹³å‡æ ¡æ­£ â­
        {
            'name': 'X-M3-70m',
            'description': 'GFSå’ŒECçš„70må¹³å‡åæ ¡æ­£å†é¢„æµ‹åŠŸç‡ â­åˆ›æ–°æ–¹æ³•', 
            'type': 'cross_source_average_corrected',
            'height': '70m',
            'sources': ['gfs', 'ec']
        }
    ]
    
    print(f"ğŸ“Š è¯•éªŒæ€»æ•°: {len(experiments)}")
    print(f"åŒ…æ‹¬:")
    print(f"   M1ç³»åˆ— (4ä¸ª): G/E-M1-10m/70m")
    print(f"   M2ç³»åˆ— (4ä¸ª): G/E-M2-10m/70m")  
    print(f"   Fusionç³»åˆ— (1ä¸ª): Fusion-M1")
    print(f"   M4ç³»åˆ— (2ä¸ª): G/E-M4-Dual")
    print(f"   X-M2ç³»åˆ— (2ä¸ª): X-M2-10m/70m")
    print(f"   X-M3ç³»åˆ— (2ä¸ª): X-M3-10m/70m â­ä½ çš„åˆ›æ–°æ–¹æ³•")
    print(f"")
    print(f"ğŸ¯ ç®€åŒ–å‘½åè§„åˆ™:")
    print(f"   G-/E-: GFS/ECæ•°æ®æº")
    print(f"   X-: è·¨æº(Cross)èåˆ")
    print(f"   M1: ç›´æ¥é¢„æµ‹, M2: ä¸¤æ­¥æ ¡æ­£, M3: å¹³å‡æ ¡æ­£, M4: åŒé«˜åº¦èåˆ")
    print(f"   Dual: åŒé«˜åº¦, 10m/70m: å•ä¸€é«˜åº¦")
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = []
    
    for i, exp_config in enumerate(experiments, 1):
        print(f"\nè¿›åº¦: {i}/{len(experiments)}")
        
        try:
            # è¿è¡Œå•ä¸ªè¯•éªŒ
            save_dir = os.path.join(base_save_dir, exp_config['name'])
            metrics = run_enhanced_experiment(data_path, save_dir, indices_path, exp_config)
            
            # è®°å½•ç»“æœ
            result = {
                'experiment': exp_config['name'],
                'description': exp_config['description'],
                'type': exp_config['type'],
                'RMSE': metrics.get('RMSE'),
                'Correlation': metrics.get('Correlation')
            }
            
            if 'error' in metrics:
                result['error'] = metrics['error']
            
            all_results.append(result)
            
        except Exception as e:
            print(f"âŒ {exp_config['name']} è¯•éªŒå¤±è´¥: {str(e)}")
            result = {
                'experiment': exp_config['name'],
                'description': exp_config['description'],
                'type': exp_config['type'],
                'RMSE': None,
                'Correlation': None,
                'error': str(e)
            }
            all_results.append(result)
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    create_enhanced_summary(all_results, base_save_dir)
    
    print(f"\n{'='*80}")
    print(f"ğŸ‰ ç®€åŒ–å‘½åç‰ˆå¢å¼ºè¯•éªŒç³»ç»Ÿå®Œæˆ!")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {base_save_dir}")
    print(f"ğŸ“Š æ±‡æ€»æŠ¥å‘Š: {base_save_dir}/enhanced_summary.csv")
    print(f"{'='*80}")
    
    return all_results

def create_enhanced_summary(all_results, base_save_dir):
    """åˆ›å»ºå¢å¼ºç‰ˆæ±‡æ€»æŠ¥å‘Š"""
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(all_results)
    
    # æŒ‰RMSEæ’åºï¼ˆåªåŒ…å«æœ‰æ•ˆç»“æœï¼‰
    df_valid = df[df['RMSE'].notna()].copy()
    df_valid = df_valid.sort_values('RMSE')
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    os.makedirs(base_save_dir, exist_ok=True)
    df.to_csv(os.path.join(base_save_dir, 'enhanced_summary.csv'), index=False)
    
    # åˆ›å»ºæ’åæŠ¥å‘Š
    print(f"\nğŸ“Š ç®€åŒ–å‘½åç‰ˆè¯•éªŒç»“æœæ’å (æŒ‰RMSEæ’åº):")
    print(f"{'æ’å':<4} {'è¯•éªŒåç§°':<15} {'ç±»å‹':<25} {'RMSE':<10} {'ç›¸å…³ç³»æ•°':<10}")
    print(f"-" * 85)
    
    for i, (_, row) in enumerate(df_valid.iterrows(), 1):
        print(f"{i:<4} {row['experiment']:<15} {row['type']:<25} {row['RMSE']:<10.4f} {row['Correlation']:<10.4f}")
    
    # æŒ‰è¯•éªŒç±»å‹åˆ†æ
    print(f"\nğŸ“ˆ æŒ‰è¯•éªŒç±»å‹åˆ†æ:")
    
    type_mapping = {
        'direct': 'M1-ç›´æ¥é¢„æµ‹',
        'two_step': 'M2-ä¸¤æ­¥æ ¡æ­£',
        'fusion': 'Fusion-å››é£é€Ÿèåˆ',
        'dual_height_corrected_fusion': 'M4-åŒé«˜åº¦æ ¡æ­£èåˆ',
        'cross_source_corrected_fusion': 'X-M2-è·¨æºæ ¡æ­£èåˆ', 
        'cross_source_average_corrected': 'X-M3-è·¨æºå¹³å‡æ ¡æ­£â­'
    }
    
    type_analysis = {}
    for exp_type in type_mapping.keys():
        type_data = df_valid[df_valid['type'] == exp_type]
        if len(type_data) > 0:
            type_analysis[exp_type] = {
                'count': len(type_data),
                'best_rmse': type_data['RMSE'].min(),
                'best_experiment': type_data.loc[type_data['RMSE'].idxmin(), 'experiment'],
                'avg_rmse': type_data['RMSE'].mean(),
                'avg_correlation': type_data['Correlation'].mean()
            }
            
            print(f"  {type_mapping[exp_type]}:")
            print(f"    è¯•éªŒæ•°é‡: {type_analysis[exp_type]['count']}")
            print(f"    æœ€ä½³RMSE: {type_analysis[exp_type]['best_rmse']:.4f} ({type_analysis[exp_type]['best_experiment']})")
            print(f"    å¹³å‡RMSE: {type_analysis[exp_type]['avg_rmse']:.4f}")
    
    # ä½ çš„åˆ›æ–°æ–¹æ³•ç‰¹åˆ«åˆ†æ
    innovation_experiments = df_valid[df_valid['type'] == 'cross_source_average_corrected']
    if len(innovation_experiments) > 0:
        print(f"\nâ­ ä½ çš„åˆ›æ–°æ–¹æ³• (X-M3ç³»åˆ—) ç‰¹åˆ«åˆ†æ:")
        for _, row in innovation_experiments.iterrows():
            rank = list(df_valid['experiment']).index(row['experiment']) + 1
            print(f"  {row['experiment']}: æ’åç¬¬{rank}å, RMSE={row['RMSE']:.4f}")
            
        best_innovation = innovation_experiments.loc[innovation_experiments['RMSE'].idxmin()]
        overall_best = df_valid.iloc[0]
        
        if best_innovation['experiment'] == overall_best['experiment']:
            print(f"  ğŸ† ä½ çš„åˆ›æ–°æ–¹æ³•æ˜¯å…¨å±€æœ€ä½³ï¼")
        else:
            improvement = (overall_best['RMSE'] - best_innovation['RMSE']) / overall_best['RMSE'] * 100
            print(f"  ğŸ“Š ä¸å…¨å±€æœ€ä½³å·®è·: {abs(improvement):.2f}%")
    
    # ç®€åŒ–ç‰ˆæ•°æ®æºå¯¹æ¯”
    print(f"\nğŸ” æ•°æ®æºå¯¹æ¯”:")
    
    gfs_experiments = df_valid[df_valid['experiment'].str.contains('G-')]
    ec_experiments = df_valid[df_valid['experiment'].str.contains('E-')]
    cross_experiments = df_valid[df_valid['experiment'].str.contains('X-')]
    
    if len(gfs_experiments) > 0:
        print(f"  GFSç³»åˆ—: å¹³å‡RMSE={gfs_experiments['RMSE'].mean():.4f}")
    if len(ec_experiments) > 0:
        print(f"  ECç³»åˆ—: å¹³å‡RMSE={ec_experiments['RMSE'].mean():.4f}")
    if len(cross_experiments) > 0:
        print(f"  è·¨æºç³»åˆ—: å¹³å‡RMSE={cross_experiments['RMSE'].mean():.4f}")
    
    # ä¿å­˜åˆ†ææŠ¥å‘Š
    with open(os.path.join(base_save_dir, 'simplified_analysis_summary.txt'), 'w', encoding='utf-8') as f:
        f.write("ç®€åŒ–å‘½åç‰ˆè¯•éªŒç³»ç»Ÿåˆ†ææŠ¥å‘Š\n")
        f.write("="*50 + "\n\n")
        
        f.write("1. æ€»ä½“æ’å (æŒ‰RMSE):\n")
        for i, (_, row) in enumerate(df_valid.iterrows(), 1):
            f.write(f"{i}. {row['experiment']}: RMSE={row['RMSE']:.4f}, Corr={row['Correlation']:.4f}\n")
        
        f.write(f"\n2. æŒ‰ç±»å‹åˆ†æ:\n")
        for exp_type, stats in type_analysis.items():
            f.write(f"{type_mapping.get(exp_type, exp_type)}: æœ€ä½³RMSE={stats['best_rmse']:.4f} ({stats['best_experiment']})\n")
        
        f.write(f"\n3. åˆ›æ–°æ–¹æ³•åˆ†æ:\n")
        if len(innovation_experiments) > 0:
            for _, row in innovation_experiments.iterrows():
                rank = list(df_valid['experiment']).index(row['experiment']) + 1
                f.write(f"- {row['experiment']}: æ’åç¬¬{rank}å, RMSE={row['RMSE']:.4f}\n")
        
        f.write(f"\n4. ä¸»è¦å‘ç°:\n")
        if len(df_valid) > 0:
            best_overall = df_valid.iloc[0]
            f.write(f"- å…¨å±€æœ€ä½³: {best_overall['experiment']} (RMSE: {best_overall['RMSE']:.4f})\n")
            f.write(f"- æœ€ä½³ç­–ç•¥: {best_overall['description']}\n")
            
            if len(innovation_experiments) > 0:
                best_innovation = innovation_experiments.loc[innovation_experiments['RMSE'].idxmin()]
                f.write(f"- åˆ›æ–°æ–¹æ³•æœ€ä½³: {best_innovation['experiment']} (RMSE: {best_innovation['RMSE']:.4f})\n")
    
    # åˆ†ææœ€ä½³ç­–ç•¥
    if len(df_valid) > 0:
        best_result = df_valid.iloc[0]
        print(f"\nğŸ† æœ€ä½³è¯•éªŒ: {best_result['experiment']}")
        print(f"   RMSE: {best_result['RMSE']:.4f}")
        print(f"   ç›¸å…³ç³»æ•°: {best_result['Correlation']:.4f}")
        print(f"   ç­–ç•¥: {best_result['description']}")
        print(f"   ç±»å‹: {type_mapping.get(best_result['type'], best_result['type'])}")
        
        # åˆ¤æ–­æ˜¯å¦æ˜¯åˆ›æ–°æ–¹æ³•
        if best_result['type'] == 'cross_source_average_corrected':
            print(f"   ğŸ‰ æœ€ä½³è¯•éªŒæ˜¯ä½ çš„åˆ›æ–°æ–¹æ³•ï¼")

if __name__ == "__main__":
    # é…ç½®è·¯å¾„
    DATA_PATH = "/Users/xiaxin/work/WindForecast_Project/01_Data/processed/imputed_data/changma_imputed_complete.csv"
    BASE_SAVE_DIR = "/Users/xiaxin/work/WindForecast_Project/03_Results/simplified_enhanced_experiments"
    INDICES_PATH = "/Users/xiaxin/work/WindForecast_Project/03_Results/third_part_experiments/train_test_split.json"
    
    # è¿è¡Œç®€åŒ–å‘½åç‰ˆå¢å¼ºè¯•éªŒç³»ç»Ÿ
    results = run_enhanced_experiments(DATA_PATH, BASE_SAVE_DIR, INDICES_PATH)
    
    print(f"\nğŸ’¡ ç®€åŒ–å‘½åç‰ˆå¢å¼ºè¯•éªŒç³»ç»Ÿè¿è¡Œå®Œæˆ!")
    print(f"\nğŸ¯ ç®€æ´å‘½åç³»ç»Ÿ:")
    print(f"   åŸæœ‰è¯•éªŒ: G/E-M1/M2-10m/70m, Fusion-M1")
    print(f"   æ–°å¢è¯•éªŒ: G/E-M4-Dual, X-M2-10m/70m, X-M3-10m/70m")
    print(f"   åˆ›æ–°æ–¹æ³•: X-M3-10m/70m (è·¨æºå¹³å‡æ ¡æ­£)")
    print(f"\nğŸ“Š ç°åœ¨å¯ä»¥ç”¨ç®€æ´çš„åç§°:")
    print(f"   - å¼•ç”¨ä½ çš„åˆ›æ–°: X-M3-10m")
    print(f"   - å¯¹æ¯”åŸºçº¿æ–¹æ³•: G-M2-70m vs X-M3-10m")
    print(f"   - åˆ†æèåˆç­–ç•¥: Fusion-M1 vs X-M2-10m vs X-M3-10m")
    print(f"   - è®ºæ–‡ä¸­æ›´ç®€æ´ç¾è§‚!")