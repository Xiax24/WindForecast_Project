#!/usr/bin/env python3
"""
æ‰¹é‡è¿è¡Œç¬¬ä¸‰éƒ¨åˆ†çš„14ä¸ªè¯•éªŒ
å®Œæ•´ç‰ˆæœ¬ï¼ŒåŒ…å«æ‰€æœ‰å¿…éœ€å‡½æ•°ï¼Œæ— å¤–éƒ¨ä¾èµ–
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.metrics import mean_squared_error
import os
import json
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
RANDOM_STATE = 42

def prepare_simple_features(data, wind_col):
    """å‡†å¤‡ç®€å•ç‰¹å¾"""
    features = pd.DataFrame()
    
    # ä¸»è¦é£é€Ÿç‰¹å¾
    features['wind_speed'] = data[wind_col]
    features['wind_speed_2'] = data[wind_col] ** 2
    features['wind_speed_3'] = data[wind_col] ** 3
    
    # åŸºç¡€æ—¶é—´ç‰¹å¾
    features['hour'] = data['datetime'].dt.hour
    features['month'] = data['datetime'].dt.month
    features['is_daytime'] = ((data['datetime'].dt.hour >= 6) & 
                             (data['datetime'].dt.hour < 18)).astype(int)
    
    # ç®€å•æ»åç‰¹å¾
    features['wind_lag_1h'] = data[wind_col].shift(1)
    features['wind_lag_24h'] = data[wind_col].shift(24)
    
    # å¡«å……NaN
    features = features.fillna(method='bfill').fillna(method='ffill')
    
    return features

def create_train_test_split(data, test_size=0.2, indices_path=None):
    """åˆ›å»ºæˆ–åŠ è½½è®­ç»ƒæµ‹è¯•é›†åˆ’åˆ†"""
    
    if indices_path and os.path.exists(indices_path):
        # åŠ è½½å·²æœ‰åˆ’åˆ†
        with open(indices_path, 'r') as f:
            indices = json.load(f)
        train_indices = indices['train_indices']
        test_indices = indices['test_indices']
        print(f"åŠ è½½å·²æœ‰åˆ’åˆ†: è®­ç»ƒé›†{len(train_indices)}, æµ‹è¯•é›†{len(test_indices)}")
    else:
        # åˆ›å»ºæ–°åˆ’åˆ†ï¼ˆæ—¶é—´åºåˆ—åˆ’åˆ†ï¼‰
        data_sorted = data.sort_values('datetime').reset_index(drop=True)
        n_samples = len(data_sorted)
        split_idx = int(n_samples * (1 - test_size))
        
        train_indices = list(range(split_idx))
        test_indices = list(range(split_idx, n_samples))
        
        # ä¿å­˜åˆ’åˆ†
        if indices_path:
            os.makedirs(os.path.dirname(indices_path), exist_ok=True)
            indices_data = {
                'train_indices': train_indices,
                'test_indices': test_indices,
                'split_date': data_sorted.iloc[split_idx]['datetime'].isoformat(),
                'train_size': len(train_indices),
                'test_size': len(test_indices)
            }
            with open(indices_path, 'w') as f:
                json.dump(indices_data, f, indent=2)
            print(f"æ–°å»ºåˆ’åˆ†å·²ä¿å­˜: è®­ç»ƒé›†{len(train_indices)}, æµ‹è¯•é›†{len(test_indices)}")
    
    return train_indices, test_indices

def train_lightgbm_model(X_train, y_train, X_test, y_test):
    """è®­ç»ƒLightGBMæ¨¡å‹"""
    
    # LightGBMå‚æ•°
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.1,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'random_state': RANDOM_STATE
    }
    
    # åˆ›å»ºæ•°æ®é›†
    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
    
    # è®­ç»ƒæ¨¡å‹
    model = lgb.train(
        params,
        train_data,
        valid_sets=[valid_data],
        num_boost_round=100,
        callbacks=[lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation(0)]
    )
    
    return model

def evaluate_model(model, X_test, y_test):
    """è¯„ä¼°æ¨¡å‹"""
    y_pred = model.predict(X_test, num_iteration=model.best_iteration)
    
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    correlation = np.corrcoef(y_test, y_pred)[0, 1]
    
    return {
        'RMSE': rmse,
        'Correlation': correlation
    }, y_pred

def calculate_time_adaptive_weights(hour):
    """åŸºäºSHAPåˆ†æçš„æ˜¼å¤œåŠ¨æ€æƒé‡
    
    åŸºäºSHAPé‡è¦æ€§åˆ†æï¼š
    - ç™½å¤©: 10mâ‰ˆ15.5, 70mâ‰ˆ12.5 â†’ æƒé‡æ¯” 0.55:0.45  
    - å¤œé—´: 10mâ‰ˆ16.5, 70mâ‰ˆ10.5 â†’ æƒé‡æ¯” 0.61:0.39
    """
    is_daytime = (6 <= hour < 18)
    
    if is_daytime:
        # ç™½å¤©ï¼šåŸºäºSHAPåˆ†æçš„æƒé‡æ¯”ä¾‹
        return [0.55, 0.45]  # [10mæƒé‡, 70mæƒé‡]
    else:
        # å¤œé—´ï¼š10mé‡è¦æ€§æ›´çªå‡º
        return [0.61, 0.39]

def apply_bias_correction(forecast_data, bias_correction_factor):
    """åº”ç”¨åå·®æ ¡æ­£"""
    return forecast_data * bias_correction_factor

def run_single_experiment(data_path, save_dir, indices_path, exp_config):
    """è¿è¡Œå•ä¸ªè¯•éªŒ"""
    
    os.makedirs(save_dir, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    data = pd.read_csv(data_path)
    data['datetime'] = pd.to_datetime(data['datetime'])
    
    # æ•°æ®æ¸…ç†
    data = data.dropna(subset=['power'])
    data = data[data['power'] >= 0]
    
    # å¤„ç†ä¸åŒçš„è¯•éªŒç±»å‹
    if 'wind_col' in exp_config:
        # å•é£é€Ÿè¯•éªŒ
        wind_col = exp_config['wind_col']
        data = data.dropna(subset=[wind_col])
        data = data[(data[wind_col] >= 0) & (data[wind_col] <= 50)]
        
        # åº”ç”¨æ ¡æ­£
        if exp_config.get('use_correction', False):
            factor = exp_config['correction_factor']
            data[wind_col] = apply_bias_correction(data[wind_col], factor)
        
        feature_wind_col = wind_col
        
    elif 'wind_cols' in exp_config:
        # å¤šé£é€Ÿèåˆè¯•éªŒ
        wind_cols = exp_config['wind_cols']
        data = data.dropna(subset=wind_cols)
        for col in wind_cols:
            data = data[(data[col] >= 0) & (data[col] <= 50)]
        
        # å¤„ç†ä¸åŒèåˆç­–ç•¥
        if exp_config.get('adaptive_weights', False):
            # æ˜¼å¤œåŠ¨æ€æƒé‡
            fused_values = []
            for idx, row in data.iterrows():
                weights = calculate_time_adaptive_weights(row['datetime'].hour)
                fused_value = sum(weights[i] * row[wind_cols[i]] for i in range(len(wind_cols)))
                fused_values.append(fused_value)
            data['fused_wind'] = fused_values
                
        elif exp_config.get('weights'):
            # å›ºå®šæƒé‡
            weights = exp_config['weights']
            data['fused_wind'] = sum(weights[i] * data[wind_cols[i]] for i in range(len(weights)))
            
        elif exp_config.get('fusion_strategy') == 'corrected_average':
            # è”åˆæ ¡æ­£åå¹³å‡
            ec_10m = apply_bias_correction(data['ec_wind_speed_10m'], 1.03)
            ec_70m = apply_bias_correction(data['ec_wind_speed_70m'], 1.01)
            gfs_10m = apply_bias_correction(data['gfs_wind_speed_10m'], 1.05)
            gfs_70m = apply_bias_correction(data['gfs_wind_speed_70m'], 0.98)
            
            # åŠ æƒå¹³å‡ (ECæƒé‡æ›´é«˜ï¼Œ10mæƒé‡æ›´é«˜)
            data['fused_wind'] = (0.4 * ec_10m + 0.2 * ec_70m + 
                                 0.3 * gfs_10m + 0.1 * gfs_70m)
            
        elif exp_config.get('fusion_strategy') == 'optimal_adaptive':
            # åŸºäºè¯•éªŒç»“æœçš„æœ€ä¼˜åŠ¨æ€èåˆç­–ç•¥
            fused_values = []
            for idx, row in data.iterrows():
                hour = row['datetime'].hour
                is_day = 6 <= hour < 18
                
                # åŸºäºæ’åç»“æœï¼šEC >> GFS, 10m > 70m
                if is_day:
                    # ç™½å¤©ï¼šåŸºäºSHAPæ˜¼å¤œåˆ†æï¼ŒECä¸»å¯¼
                    weights = [0.45, 0.35, 0.12, 0.08]  # EC-10m(0.55*0.8), EC-70m(0.45*0.8), GFSæƒé‡è¾ƒå°
                else:
                    # å¤œé—´ï¼š10mæ›´é‡è¦ï¼ŒECä¸»å¯¼
                    weights = [0.50, 0.25, 0.15, 0.10]  # æ›´åå‘EC-10m
                
                fused_value = sum(weights[i] * row[wind_cols[i]] for i in range(len(wind_cols)))
                fused_values.append(fused_value)
            data['fused_wind'] = fused_values
        
        feature_wind_col = 'fused_wind'
    
    data = data.sort_values('datetime').reset_index(drop=True)
    
    # ç‰¹å¾å‡†å¤‡
    features = prepare_simple_features(data, feature_wind_col)
    target = data['power'].values
    
    # åˆ’åˆ†æ•°æ®é›†
    train_indices, test_indices = create_train_test_split(data, indices_path=indices_path)
    
    X_train = features.iloc[train_indices]
    X_test = features.iloc[test_indices]
    y_train = target[train_indices]
    y_test = target[test_indices]
    
    # è®­ç»ƒå’Œè¯„ä¼°
    model = train_lightgbm_model(X_train, y_train, X_test, y_test)
    metrics, y_pred = evaluate_model(model, X_test, y_test)
    
    # ä¿å­˜ç»“æœ
    results_df = pd.DataFrame({
        'datetime': data.iloc[test_indices]['datetime'].values,
        'actual_power': y_test,
        'predicted_power': y_pred
    })
    results_df.to_csv(os.path.join(save_dir, f'{exp_config["name"]}_results.csv'), index=False)
    
    with open(os.path.join(save_dir, f'{exp_config["name"]}_metrics.json'), 'w') as f:
        json.dump(metrics, f, indent=2)
    
    model.save_model(os.path.join(save_dir, f'{exp_config["name"]}_model.txt'))
    
    return metrics

def run_all_experiments(data_path, base_save_dir, indices_path):
    """è¿è¡Œæ‰€æœ‰14ä¸ªè¯•éªŒ"""
    
    print("=" * 80)
    print("ğŸš€ å¼€å§‹æ‰¹é‡è¿è¡Œç¬¬ä¸‰éƒ¨åˆ†è¯•éªŒ (å…±14ä¸ª)")
    print("=" * 80)
    
    # è¯•éªŒé…ç½®
    experiments = [
        # GFSè¯•éªŒ
        {
            'name': 'G-M1-10m',
            'wind_col': 'gfs_wind_speed_10m',
            'description': 'GFSåŸå§‹10mé£é€Ÿ'
        },
        {
            'name': 'G-M2-10m', 
            'wind_col': 'gfs_wind_speed_10m',
            'use_correction': True,
            'correction_factor': 1.05,
            'description': 'GFSæ ¡æ­£å10mé£é€Ÿ'
        },
        {
            'name': 'G-M1-70m',
            'wind_col': 'gfs_wind_speed_70m', 
            'description': 'GFSåŸå§‹70mé£é€Ÿ'
        },
        {
            'name': 'G-M2-70m',
            'wind_col': 'gfs_wind_speed_70m',
            'use_correction': True,
            'correction_factor': 0.98,
            'description': 'GFSæ ¡æ­£å70mé£é€Ÿ'
        },
        {
            'name': 'G-M3-Fixed',
            'wind_cols': ['gfs_wind_speed_10m', 'gfs_wind_speed_70m'],
            'weights': [0.55, 0.45],  # åŸºäºSHAPæ€»ä½“æƒé‡æ¯”ä¾‹
            'description': 'GFSå›ºå®šæƒé‡èåˆ(åŸºäºSHAPæ€»ä½“æ¯”ä¾‹)'
        },
        {
            'name': 'G-M3-TimeAdaptive',
            'wind_cols': ['gfs_wind_speed_10m', 'gfs_wind_speed_70m'],
            'adaptive_weights': True,
            'description': 'GFSæ˜¼å¤œåŠ¨æ€æƒé‡èåˆ(åŸºäºSHAPæ˜¼å¤œåˆ†æ)'
        },
        
        # ECè¯•éªŒ
        {
            'name': 'E-M1-10m',
            'wind_col': 'ec_wind_speed_10m',
            'description': 'ECåŸå§‹10mé£é€Ÿ'
        },
        {
            'name': 'E-M2-10m',
            'wind_col': 'ec_wind_speed_10m',
            'use_correction': True,
            'correction_factor': 1.03,
            'description': 'ECæ ¡æ­£å10mé£é€Ÿ'
        },
        {
            'name': 'E-M1-70m',
            'wind_col': 'ec_wind_speed_70m',
            'description': 'ECåŸå§‹70mé£é€Ÿ'
        },
        {
            'name': 'E-M2-70m',
            'wind_col': 'ec_wind_speed_70m',
            'use_correction': True,
            'correction_factor': 1.01,
            'description': 'ECæ ¡æ­£å70mé£é€Ÿ'
        },
        {
            'name': 'E-M3-Fixed',
            'wind_cols': ['ec_wind_speed_10m', 'ec_wind_speed_70m'],
            'weights': [0.55, 0.45],  # åŸºäºSHAPæ€»ä½“æƒé‡æ¯”ä¾‹
            'description': 'ECå›ºå®šæƒé‡èåˆ(åŸºäºSHAPæ€»ä½“æ¯”ä¾‹)'
        },
        {
            'name': 'E-M3-TimeAdaptive',
            'wind_cols': ['ec_wind_speed_10m', 'ec_wind_speed_70m'],
            'adaptive_weights': True,
            'description': 'ECæ˜¼å¤œåŠ¨æ€æƒé‡èåˆ(åŸºäºSHAPæ˜¼å¤œåˆ†æ)'
        },
        
        # èåˆè¯•éªŒ
        {
            'name': 'Fusion-M1',
            'wind_cols': ['ec_wind_speed_10m', 'ec_wind_speed_70m', 'gfs_wind_speed_10m', 'gfs_wind_speed_70m'],
            'fusion_strategy': 'corrected_average',
            'description': 'è”åˆæ ¡æ­£åèåˆ'
        },
        {
            'name': 'Fusion-M2',
            'wind_cols': ['ec_wind_speed_10m', 'ec_wind_speed_70m', 'gfs_wind_speed_10m', 'gfs_wind_speed_70m'],
            'fusion_strategy': 'optimal_adaptive',
            'description': 'åŸºäºè¯•éªŒç»“æœçš„åŠ¨æ€æƒé‡èåˆ(ECä¸»å¯¼+SHAPæ˜¼å¤œæƒé‡)'
        }
    ]
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = []
    
    for i, exp_config in enumerate(experiments, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ”¬ è¯•éªŒ {i}/14: {exp_config['name']}")
        print(f"ğŸ“ æè¿°: {exp_config['description']}")
        print(f"{'='*60}")
        
        try:
            # è¿è¡Œå•ä¸ªè¯•éªŒ
            save_dir = os.path.join(base_save_dir, exp_config['name'])
            metrics = run_single_experiment(data_path, save_dir, indices_path, exp_config)
            
            # è®°å½•ç»“æœ
            result = {
                'experiment': exp_config['name'],
                'description': exp_config['description'],
                'RMSE': metrics['RMSE'],
                'Correlation': metrics['Correlation']
            }
            all_results.append(result)
            
            print(f"âœ… {exp_config['name']} å®Œæˆ")
            print(f"   RMSE: {metrics['RMSE']:.4f}")
            print(f"   ç›¸å…³ç³»æ•°: {metrics['Correlation']:.4f}")
            
        except Exception as e:
            print(f"âŒ {exp_config['name']} å¤±è´¥: {str(e)}")
            result = {
                'experiment': exp_config['name'],
                'description': exp_config['description'],
                'RMSE': None,
                'Correlation': None,
                'error': str(e)
            }
            all_results.append(result)
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    create_summary_report(all_results, base_save_dir)
    
    print(f"\n{'='*80}")
    print(f"ğŸ‰ æ‰€æœ‰è¯•éªŒå®Œæˆ!")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {base_save_dir}")
    print(f"ğŸ“Š æ±‡æ€»æŠ¥å‘Š: {base_save_dir}/experiment_summary.csv")
    print(f"{'='*80}")
    
    return all_results

def create_summary_report(all_results, base_save_dir):
    """åˆ›å»ºæ±‡æ€»æŠ¥å‘Š"""
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(all_results)
    
    # æŒ‰RMSEæ’åº
    df_valid = df[df['RMSE'].notna()].copy()
    df_valid = df_valid.sort_values('RMSE')
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    df.to_csv(os.path.join(base_save_dir, 'experiment_summary.csv'), index=False)
    
    # åˆ›å»ºæ’åæŠ¥å‘Š
    print(f"\nğŸ“Š è¯•éªŒç»“æœæ’å (æŒ‰RMSEæ’åº):")
    print(f"{'æ’å':<4} {'è¯•éªŒåç§°':<20} {'RMSE':<10} {'ç›¸å…³ç³»æ•°':<10} {'æè¿°'}")
    print(f"-" * 80)
    
    for i, (_, row) in enumerate(df_valid.iterrows(), 1):
        print(f"{i:<4} {row['experiment']:<20} {row['RMSE']:<10.4f} {row['Correlation']:<10.4f} {row['description']}")
    
    # ä¿å­˜æ’å
    with open(os.path.join(base_save_dir, 'ranking_summary.txt'), 'w') as f:
        f.write("è¯•éªŒç»“æœæ’å (æŒ‰RMSEæ’åº):\n")
        f.write("="*80 + "\n")
        for i, (_, row) in enumerate(df_valid.iterrows(), 1):
            f.write(f"{i}. {row['experiment']}: RMSE={row['RMSE']:.4f}, Corr={row['Correlation']:.4f}\n")
    
    # åˆ†ææœ€ä½³ç­–ç•¥
    if len(df_valid) > 0:
        best_result = df_valid.iloc[0]
        print(f"\nğŸ† æœ€ä½³è¯•éªŒ: {best_result['experiment']}")
        print(f"   RMSE: {best_result['RMSE']:.4f}")
        print(f"   ç›¸å…³ç³»æ•°: {best_result['Correlation']:.4f}")
        print(f"   ç­–ç•¥: {best_result['description']}")

if __name__ == "__main__":
    # é…ç½®è·¯å¾„
    DATA_PATH = "/Users/xiaxin/work/WindForecast_Project/01_Data/processed/imputed_data/changma_imputed_complete.csv"
    BASE_SAVE_DIR = "/Users/xiaxin/work/WindForecast_Project/03_Results/third_part_experiments"
    INDICES_PATH = "/Users/xiaxin/work/WindForecast_Project/03_Results/third_part_experiments/train_test_split.json"
    
    # è¿è¡Œæ‰€æœ‰è¯•éªŒ
    results = run_all_experiments(DATA_PATH, BASE_SAVE_DIR, INDICES_PATH)
    
    print(f"\nğŸ’¡ è¯•éªŒå®Œæˆï¼")
    print(f"ç°åœ¨å¯ä»¥åˆ†æä¸åŒç­–ç•¥çš„æ•ˆæœï¼Œæ‰¾å‡ºæœ€ä¼˜çš„é£é€Ÿèåˆæ–¹æ¡ˆï¼")