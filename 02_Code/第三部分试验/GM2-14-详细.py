#!/usr/bin/env python3
"""
æ‰¹é‡è¿è¡Œç¬¬ä¸‰éƒ¨åˆ†çš„14ä¸ªè¯•éªŒ
å®Œæ•´ç‰ˆæœ¬ï¼ŒåŒ…å«æ‰€æœ‰å¿…éœ€å‡½æ•°ï¼Œæ— å¤–éƒ¨ä¾èµ–
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.metrics import mean_squared_error
import os
import json
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
RANDOM_STATE = 42

def prepare_simple_features(data, wind_col):
    """å‡†å¤‡ç®€å•ç‰¹å¾"""
    features = pd.DataFrame()
    
    # ä¸»è¦é£é€Ÿç‰¹å¾
    features['wind_speed'] = data[wind_col]
    features['wind_speed_2'] = data[wind_col] ** 2
    features['wind_speed_3'] = data[wind_col] ** 3
    
    # åŸºç¡€æ—¶é—´ç‰¹å¾
    features['hour'] = data['datetime'].dt.hour
    features['month'] = data['datetime'].dt.month
    features['is_daytime'] = ((data['datetime'].dt.hour >= 6) & 
                             (data['datetime'].dt.hour < 18)).astype(int)
    
    # ç®€å•æ»åç‰¹å¾
    features['wind_lag_1h'] = data[wind_col].shift(1)
    features['wind_lag_24h'] = data[wind_col].shift(24)
    
    # å¡«å……NaN
    features = features.fillna(method='bfill').fillna(method='ffill')
    
    return features

def create_train_test_split(data, test_size=0.2, indices_path=None):
    """åˆ›å»ºæˆ–åŠ è½½è®­ç»ƒæµ‹è¯•é›†åˆ’åˆ†"""
    
    if indices_path and os.path.exists(indices_path):
        # åŠ è½½å·²æœ‰åˆ’åˆ†
        with open(indices_path, 'r') as f:
            indices = json.load(f)
        train_indices = indices['train_indices']
        test_indices = indices['test_indices']
        print(f"åŠ è½½å·²æœ‰åˆ’åˆ†: è®­ç»ƒé›†{len(train_indices)}, æµ‹è¯•é›†{len(test_indices)}")
    else:
        # åˆ›å»ºæ–°åˆ’åˆ†ï¼ˆæ—¶é—´åºåˆ—åˆ’åˆ†ï¼‰
        data_sorted = data.sort_values('datetime').reset_index(drop=True)
        n_samples = len(data_sorted)
        split_idx = int(n_samples * (1 - test_size))
        
        train_indices = list(range(split_idx))
        test_indices = list(range(split_idx, n_samples))
        
        # ä¿å­˜åˆ’åˆ†
        if indices_path:
            os.makedirs(os.path.dirname(indices_path), exist_ok=True)
            indices_data = {
                'train_indices': train_indices,
                'test_indices': test_indices,
                'split_date': data_sorted.iloc[split_idx]['datetime'].isoformat(),
                'train_size': len(train_indices),
                'test_size': len(test_indices)
            }
            with open(indices_path, 'w') as f:
                json.dump(indices_data, f, indent=2)
            print(f"æ–°å»ºåˆ’åˆ†å·²ä¿å­˜: è®­ç»ƒé›†{len(train_indices)}, æµ‹è¯•é›†{len(test_indices)}")
    
    return train_indices, test_indices

def train_lightgbm_model(X_train, y_train, X_test, y_test):
    """è®­ç»ƒLightGBMæ¨¡å‹"""
    
    # LightGBMå‚æ•°
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.1,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'random_state': RANDOM_STATE
    }
    
    # åˆ›å»ºæ•°æ®é›†
    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
    
    # è®­ç»ƒæ¨¡å‹
    model = lgb.train(
        params,
        train_data,
        valid_sets=[valid_data],
        num_boost_round=100,
        callbacks=[lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation(0)]
    )
    
    return model

def evaluate_model(model, X_test, y_test):
    """è¯„ä¼°æ¨¡å‹"""
    y_pred = model.predict(X_test, num_iteration=model.best_iteration)
    
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    correlation = np.corrcoef(y_test, y_pred)[0, 1]
    
    return {
        'RMSE': rmse,
        'Correlation': correlation
    }, y_pred

def calculate_time_adaptive_weights(hour):
    """åŸºäºSHAPåˆ†æçš„æ˜¼å¤œåŠ¨æ€æƒé‡
    
    åŸºäºSHAPé‡è¦æ€§åˆ†æï¼š
    - ç™½å¤©: 10mâ‰ˆ15.5, 70mâ‰ˆ12.5 â†’ æƒé‡æ¯” 0.55:0.45  
    - å¤œé—´: 10mâ‰ˆ16.5, 70mâ‰ˆ10.5 â†’ æƒé‡æ¯” 0.61:0.39
    """
    is_daytime = (6 <= hour < 18)
    
    if is_daytime:
        # ç™½å¤©ï¼šåŸºäºSHAPåˆ†æçš„æƒé‡æ¯”ä¾‹
        return [0.55, 0.45]  # [10mæƒé‡, 70mæƒé‡]
    else:
        # å¤œé—´ï¼š10mé‡è¦æ€§æ›´çªå‡º
        return [0.61, 0.39]

def apply_bias_correction(forecast_data, bias_correction_factor):
    """åº”ç”¨åå·®æ ¡æ­£"""
    return forecast_data * bias_correction_factor

def run_single_experiment(data_path, save_dir, indices_path, exp_config):
    """è¿è¡Œå•ä¸ªè¯•éªŒï¼Œè¯¦ç»†è®°å½•æ‰€æœ‰è¾“å…¥è¾“å‡º"""
    
    os.makedirs(save_dir, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    print(f"  ğŸ“‚ åŠ è½½åŸå§‹æ•°æ®...")
    data = pd.read_csv(data_path)
    data['datetime'] = pd.to_datetime(data['datetime'])
    print(f"  åŸå§‹æ•°æ®å½¢çŠ¶: {data.shape}")
    
    # è®°å½•åŸå§‹è¾“å…¥æ•°æ®
    original_input_cols = ['datetime', 'power']
    if 'wind_col' in exp_config:
        original_input_cols.append(exp_config['wind_col'])
    elif 'wind_cols' in exp_config:
        original_input_cols.extend(exp_config['wind_cols'])
    
    input_data_sample = data[original_input_cols].head(1000)  # ä¿å­˜å‰1000è¡Œæ ·æœ¬
    input_data_sample.to_csv(os.path.join(save_dir, f'{exp_config["name"]}_input_sample.csv'), index=False)
    
    # æ•°æ®æ¸…ç†è®°å½•
    cleaning_log = []
    original_size = len(data)
    
    # æ¸…ç†åŠŸç‡æ•°æ®
    data = data.dropna(subset=['power'])
    after_power_clean = len(data)
    cleaning_log.append(f"ç§»é™¤åŠŸç‡NaN: {original_size} -> {after_power_clean}")
    
    data = data[data['power'] >= 0]
    after_power_filter = len(data)
    cleaning_log.append(f"ç§»é™¤è´ŸåŠŸç‡: {after_power_clean} -> {after_power_filter}")
    
    # å¤„ç†ä¸åŒçš„è¯•éªŒç±»å‹å¹¶è®°å½•å¤„ç†è¿‡ç¨‹
    processing_log = []
    
    if 'wind_col' in exp_config:
        # å•é£é€Ÿè¯•éªŒ
        wind_col = exp_config['wind_col']
        processing_log.append(f"è¯•éªŒç±»å‹: å•é£é€Ÿ")
        processing_log.append(f"è¾“å…¥é£é€Ÿåˆ—: {wind_col}")
        
        # æ¸…ç†é£é€Ÿæ•°æ®
        before_clean = len(data)
        data = data.dropna(subset=[wind_col])
        after_nan_clean = len(data)
        processing_log.append(f"ç§»é™¤{wind_col}çš„NaN: {before_clean} -> {after_nan_clean}")
        
        data = data[(data[wind_col] >= 0) & (data[wind_col] <= 50)]
        after_range_clean = len(data)
        processing_log.append(f"é£é€ŸèŒƒå›´è¿‡æ»¤[0,50]: {after_nan_clean} -> {after_range_clean}")
        
        # åº”ç”¨æ ¡æ­£
        if exp_config.get('use_correction', False):
            factor = exp_config['correction_factor']
            original_mean = data[wind_col].mean()
            data[wind_col] = apply_bias_correction(data[wind_col], factor)
            corrected_mean = data[wind_col].mean()
            processing_log.append(f"åå·®æ ¡æ­£: å› å­={factor}, å‡å€¼ {original_mean:.4f} -> {corrected_mean:.4f}")
        
        feature_wind_col = wind_col
        processing_log.append(f"æœ€ç»ˆç‰¹å¾é£é€Ÿåˆ—: {feature_wind_col}")
        
    elif 'wind_cols' in exp_config:
        # å¤šé£é€Ÿèåˆè¯•éªŒ
        wind_cols = exp_config['wind_cols']
        processing_log.append(f"è¯•éªŒç±»å‹: å¤šé£é€Ÿèåˆ")
        processing_log.append(f"è¾“å…¥é£é€Ÿåˆ—: {wind_cols}")
        
        # æ¸…ç†å¤šé£é€Ÿæ•°æ®
        before_clean = len(data)
        data = data.dropna(subset=wind_cols)
        after_nan_clean = len(data)
        processing_log.append(f"ç§»é™¤å¤šé£é€ŸNaN: {before_clean} -> {after_nan_clean}")
        
        for col in wind_cols:
            data = data[(data[col] >= 0) & (data[col] <= 50)]
        after_range_clean = len(data)
        processing_log.append(f"å¤šé£é€ŸèŒƒå›´è¿‡æ»¤: {after_nan_clean} -> {after_range_clean}")
        
        # è®°å½•èåˆå‰çš„åŸå§‹é£é€Ÿç»Ÿè®¡
        wind_stats_before = {}
        for col in wind_cols:
            wind_stats_before[col] = {
                'mean': data[col].mean(),
                'std': data[col].std(),
                'min': data[col].min(),
                'max': data[col].max()
            }
        
        # å¤„ç†ä¸åŒèåˆç­–ç•¥
        if exp_config.get('adaptive_weights', False):
            # æ˜¼å¤œåŠ¨æ€æƒé‡
            processing_log.append(f"èåˆç­–ç•¥: æ˜¼å¤œåŠ¨æ€æƒé‡")
            
            fused_values = []
            weights_used = {'day': [], 'night': []}
            
            for idx, row in data.iterrows():
                weights = calculate_time_adaptive_weights(row['datetime'].hour)
                fused_value = sum(weights[i] * row[wind_cols[i]] for i in range(len(wind_cols)))
                fused_values.append(fused_value)
                
                # è®°å½•æƒé‡ä½¿ç”¨æƒ…å†µ
                is_day = 6 <= row['datetime'].hour < 18
                time_key = 'day' if is_day else 'night'
                if len(weights_used[time_key]) < 5:  # åªè®°å½•å‰5ä¸ªæ ·æœ¬
                    weights_used[time_key].append({
                        'datetime': row['datetime'],
                        'weights': weights,
                        'wind_values': [row[col] for col in wind_cols],
                        'fused_value': fused_value
                    })
            
            data['fused_wind'] = fused_values
            processing_log.append(f"ç™½å¤©æƒé‡æ ·ä¾‹: {weights_used['day'][:2]}")
            processing_log.append(f"å¤œé—´æƒé‡æ ·ä¾‹: {weights_used['night'][:2]}")
                
        elif exp_config.get('weights'):
            # å›ºå®šæƒé‡
            weights = exp_config['weights']
            processing_log.append(f"èåˆç­–ç•¥: å›ºå®šæƒé‡ {weights}")
            
            fused_series = sum(weights[i] * data[wind_cols[i]] for i in range(len(weights)))
            data['fused_wind'] = fused_series
            
            # è®°å½•æƒé‡åº”ç”¨æ ·ä¾‹
            sample_row = data.iloc[0]
            sample_calculation = sum(weights[i] * sample_row[wind_cols[i]] for i in range(len(weights)))
            processing_log.append(f"æƒé‡åº”ç”¨æ ·ä¾‹: {[f'{weights[i]}*{sample_row[wind_cols[i]]:.2f}' for i in range(len(weights))]} = {sample_calculation:.2f}")
            
        elif exp_config.get('fusion_strategy') == 'corrected_average':
            # è”åˆæ ¡æ­£åå¹³å‡
            processing_log.append(f"èåˆç­–ç•¥: è”åˆæ ¡æ­£åå¹³å‡")
            
            ec_10m_orig = data['ec_wind_speed_10m'].mean()
            ec_10m = apply_bias_correction(data['ec_wind_speed_10m'], 1.03)
            ec_10m_corr = ec_10m.mean()
            processing_log.append(f"EC-10mæ ¡æ­£: {ec_10m_orig:.4f} -> {ec_10m_corr:.4f}")
            
            ec_70m_orig = data['ec_wind_speed_70m'].mean()
            ec_70m = apply_bias_correction(data['ec_wind_speed_70m'], 1.01)
            ec_70m_corr = ec_70m.mean()
            processing_log.append(f"EC-70mæ ¡æ­£: {ec_70m_orig:.4f} -> {ec_70m_corr:.4f}")
            
            gfs_10m_orig = data['gfs_wind_speed_10m'].mean()
            gfs_10m = apply_bias_correction(data['gfs_wind_speed_10m'], 1.05)
            gfs_10m_corr = gfs_10m.mean()
            processing_log.append(f"GFS-10mæ ¡æ­£: {gfs_10m_orig:.4f} -> {gfs_10m_corr:.4f}")
            
            gfs_70m_orig = data['gfs_wind_speed_70m'].mean()
            gfs_70m = apply_bias_correction(data['gfs_wind_speed_70m'], 0.98)
            gfs_70m_corr = gfs_70m.mean()
            processing_log.append(f"GFS-70mæ ¡æ­£: {gfs_70m_orig:.4f} -> {gfs_70m_corr:.4f}")
            
            # åŠ æƒå¹³å‡ (ECæƒé‡æ›´é«˜ï¼Œ10mæƒé‡æ›´é«˜)
            fusion_weights = [0.4, 0.2, 0.3, 0.1]
            data['fused_wind'] = (fusion_weights[0] * ec_10m + fusion_weights[1] * ec_70m + 
                                 fusion_weights[2] * gfs_10m + fusion_weights[3] * gfs_70m)
            processing_log.append(f"èåˆæƒé‡: EC-10m({fusion_weights[0]}) + EC-70m({fusion_weights[1]}) + GFS-10m({fusion_weights[2]}) + GFS-70m({fusion_weights[3]})")
            
        elif exp_config.get('fusion_strategy') == 'optimal_adaptive':
            # åŸºäºè¯•éªŒç»“æœçš„æœ€ä¼˜åŠ¨æ€èåˆç­–ç•¥
            processing_log.append(f"èåˆç­–ç•¥: åŸºäºè¯•éªŒç»“æœçš„æœ€ä¼˜åŠ¨æ€èåˆ")
            
            fused_values = []
            weights_used = {'day': [], 'night': []}
            
            for idx, row in data.iterrows():
                hour = row['datetime'].hour
                is_day = 6 <= hour < 18
                
                # åŸºäºæ’åç»“æœï¼šEC >> GFS, 10m > 70m
                if is_day:
                    weights = [0.45, 0.35, 0.12, 0.08]  # EC-10m, EC-70m, GFS-10m, GFS-70m
                else:
                    weights = [0.50, 0.25, 0.15, 0.10]
                
                fused_value = sum(weights[i] * row[wind_cols[i]] for i in range(len(wind_cols)))
                fused_values.append(fused_value)
                
                # è®°å½•æƒé‡ä½¿ç”¨æƒ…å†µ
                time_key = 'day' if is_day else 'night'
                if len(weights_used[time_key]) < 3:
                    weights_used[time_key].append({
                        'datetime': row['datetime'],
                        'weights': weights,
                        'wind_values': [row[col] for col in wind_cols],
                        'fused_value': fused_value
                    })
            
            data['fused_wind'] = fused_values
            processing_log.append(f"ç™½å¤©æƒé‡: EC-10m(0.45) + EC-70m(0.35) + GFS-10m(0.12) + GFS-70m(0.08)")
            processing_log.append(f"å¤œé—´æƒé‡: EC-10m(0.50) + EC-70m(0.25) + GFS-10m(0.15) + GFS-70m(0.10)")
        
        feature_wind_col = 'fused_wind'
        
        # è®°å½•èåˆåçš„é£é€Ÿç»Ÿè®¡
        fused_stats = {
            'mean': data[feature_wind_col].mean(),
            'std': data[feature_wind_col].std(),
            'min': data[feature_wind_col].min(),
            'max': data[feature_wind_col].max()
        }
        processing_log.append(f"èåˆåé£é€Ÿç»Ÿè®¡: å‡å€¼={fused_stats['mean']:.4f}, æ ‡å‡†å·®={fused_stats['std']:.4f}")
    
    data = data.sort_values('datetime').reset_index(drop=True)
    final_data_size = len(data)
    processing_log.append(f"æœ€ç»ˆæ•°æ®å¤§å°: {final_data_size}")
    
    # ç‰¹å¾å‡†å¤‡
    print(f"  ğŸ”§ ç‰¹å¾å·¥ç¨‹...")
    features = prepare_simple_features(data, feature_wind_col)
    target = data['power'].values
    
    # è®°å½•ç‰¹å¾ç»Ÿè®¡
    feature_stats = {}
    for col in features.columns:
        feature_stats[col] = {
            'mean': features[col].mean(),
            'std': features[col].std(),
            'min': features[col].min(),
            'max': features[col].max()
        }
    
    # åˆ’åˆ†æ•°æ®é›†
    print(f"  âœ‚ï¸ åˆ’åˆ†æ•°æ®é›†...")
    train_indices, test_indices = create_train_test_split(data, indices_path=indices_path)
    
    X_train = features.iloc[train_indices]
    X_test = features.iloc[test_indices]
    y_train = target[train_indices]
    y_test = target[test_indices]
    
    # è®°å½•è®­ç»ƒæµ‹è¯•é›†ç»Ÿè®¡
    split_stats = {
        'train_size': len(train_indices),
        'test_size': len(test_indices),
        'train_power_mean': y_train.mean(),
        'test_power_mean': y_test.mean(),
        'train_wind_mean': X_train['wind_speed'].mean(),
        'test_wind_mean': X_test['wind_speed'].mean()
    }
    
    # è®­ç»ƒå’Œè¯„ä¼°
    print(f"  ğŸš€ è®­ç»ƒæ¨¡å‹...")
    model = train_lightgbm_model(X_train, y_train, X_test, y_test)
    metrics, y_pred = evaluate_model(model, X_test, y_test)
    
    # ä¿å­˜è¯¦ç»†çš„è¾“å…¥æ•°æ®è®°å½•
    print(f"  ğŸ’¾ ä¿å­˜è¯¦ç»†è®°å½•...")
    
    # 1. ä¿å­˜å®Œæ•´çš„æµ‹è¯•é›†è¾“å…¥è¾“å‡ºæ•°æ®
    detailed_results = pd.DataFrame({
        'datetime': data.iloc[test_indices]['datetime'].values,
        'actual_power': y_test,
        'predicted_power': y_pred,
        'error': y_pred - y_test,
        'abs_error': np.abs(y_pred - y_test),
        'relative_error': (y_pred - y_test) / (y_test + 1e-8) * 100
    })
    
    # æ·»åŠ æ‰€æœ‰ç‰¹å¾åˆ°ç»“æœä¸­
    for col in features.columns:
        detailed_results[f'feature_{col}'] = features.iloc[test_indices][col].values
    
    # æ·»åŠ åŸå§‹é£é€Ÿæ•°æ®
    if 'wind_col' in exp_config:
        detailed_results[f'original_{exp_config["wind_col"]}'] = data.iloc[test_indices][exp_config['wind_col']].values
    elif 'wind_cols' in exp_config:
        for col in exp_config['wind_cols']:
            detailed_results[f'original_{col}'] = data.iloc[test_indices][col].values
        detailed_results['fused_wind'] = data.iloc[test_indices]['fused_wind'].values
    
    detailed_results.to_csv(os.path.join(save_dir, f'{exp_config["name"]}_detailed_results.csv'), index=False)
    
    # 2. ä¿å­˜å¤„ç†è¿‡ç¨‹æ—¥å¿—
    process_log = {
        'experiment_name': exp_config['name'],
        'experiment_config': exp_config,
        'data_cleaning_log': cleaning_log,
        'data_processing_log': processing_log,
        'feature_statistics': feature_stats,
        'split_statistics': split_stats,
        'final_metrics': metrics
    }
    
    with open(os.path.join(save_dir, f'{exp_config["name"]}_process_log.json'), 'w') as f:
        json.dump(process_log, f, indent=2, default=str)
    
    # 3. ä¿å­˜è®­ç»ƒé›†æ ·æœ¬ï¼ˆç”¨äºéªŒè¯ï¼‰
    train_sample = pd.DataFrame({
        'datetime': data.iloc[train_indices[:1000]]['datetime'].values,  # å‰1000ä¸ªè®­ç»ƒæ ·æœ¬
        'power': y_train[:1000]
    })
    
    # æ·»åŠ è®­ç»ƒé›†ç‰¹å¾
    for col in features.columns:
        train_sample[f'feature_{col}'] = features.iloc[train_indices[:1000]][col].values
    
    train_sample.to_csv(os.path.join(save_dir, f'{exp_config["name"]}_train_sample.csv'), index=False)
    
    # 4. ä¿å­˜æ¨¡å‹å’ŒæŒ‡æ ‡
    with open(os.path.join(save_dir, f'{exp_config["name"]}_metrics.json'), 'w') as f:
        json.dump(metrics, f, indent=2)
    
    model.save_model(os.path.join(save_dir, f'{exp_config["name"]}_model.txt'))
    
    # 5. ä¿å­˜ç‰¹å¾é‡è¦æ€§
    feature_importance = pd.DataFrame({
        'feature': features.columns,
        'importance': model.feature_importance()  # ä¿®å¤ï¼šä½¿ç”¨feature_importance()æ–¹æ³•
    }).sort_values('importance', ascending=False)
    feature_importance.to_csv(os.path.join(save_dir, f'{exp_config["name"]}_feature_importance.csv'), index=False)
    
    print(f"  âœ… è¯¦ç»†è®°å½•å·²ä¿å­˜åˆ° {save_dir}")
    
    return metrics

def run_all_experiments(data_path, base_save_dir, indices_path):
    """è¿è¡Œæ‰€æœ‰14ä¸ªè¯•éªŒ"""
    
    print("=" * 80)
    print("ğŸš€ å¼€å§‹æ‰¹é‡è¿è¡Œç¬¬ä¸‰éƒ¨åˆ†è¯•éªŒ (å…±14ä¸ª)")
    print("=" * 80)
    
    # è¯•éªŒé…ç½®
    experiments = [
        # GFSè¯•éªŒ
        {
            'name': 'G-M1-10m',
            'wind_col': 'gfs_wind_speed_10m',
            'description': 'GFSåŸå§‹10mé£é€Ÿ'
        },
        {
            'name': 'G-M2-10m', 
            'wind_col': 'gfs_wind_speed_10m',
            'use_correction': True,
            'correction_factor': 1.05,
            'description': 'GFSæ ¡æ­£å10mé£é€Ÿ'
        },
        {
            'name': 'G-M1-70m',
            'wind_col': 'gfs_wind_speed_70m', 
            'description': 'GFSåŸå§‹70mé£é€Ÿ'
        },
        {
            'name': 'G-M2-70m',
            'wind_col': 'gfs_wind_speed_70m',
            'use_correction': True,
            'correction_factor': 0.98,
            'description': 'GFSæ ¡æ­£å70mé£é€Ÿ'
        },
        {
            'name': 'G-M3-Fixed',
            'wind_cols': ['gfs_wind_speed_10m', 'gfs_wind_speed_70m'],
            'weights': [0.55, 0.45],  # åŸºäºSHAPæ€»ä½“æƒé‡æ¯”ä¾‹
            'description': 'GFSå›ºå®šæƒé‡èåˆ(åŸºäºSHAPæ€»ä½“æ¯”ä¾‹)'
        },
        {
            'name': 'G-M3-TimeAdaptive',
            'wind_cols': ['gfs_wind_speed_10m', 'gfs_wind_speed_70m'],
            'adaptive_weights': True,
            'description': 'GFSæ˜¼å¤œåŠ¨æ€æƒé‡èåˆ(åŸºäºSHAPæ˜¼å¤œåˆ†æ)'
        },
        
        # ECè¯•éªŒ
        {
            'name': 'E-M1-10m',
            'wind_col': 'ec_wind_speed_10m',
            'description': 'ECåŸå§‹10mé£é€Ÿ'
        },
        {
            'name': 'E-M2-10m',
            'wind_col': 'ec_wind_speed_10m',
            'use_correction': True,
            'correction_factor': 1.03,
            'description': 'ECæ ¡æ­£å10mé£é€Ÿ'
        },
        {
            'name': 'E-M1-70m',
            'wind_col': 'ec_wind_speed_70m',
            'description': 'ECåŸå§‹70mé£é€Ÿ'
        },
        {
            'name': 'E-M2-70m',
            'wind_col': 'ec_wind_speed_70m',
            'use_correction': True,
            'correction_factor': 1.01,
            'description': 'ECæ ¡æ­£å70mé£é€Ÿ'
        },
        {
            'name': 'E-M3-Fixed',
            'wind_cols': ['ec_wind_speed_10m', 'ec_wind_speed_70m'],
            'weights': [0.55, 0.45],  # åŸºäºSHAPæ€»ä½“æƒé‡æ¯”ä¾‹
            'description': 'ECå›ºå®šæƒé‡èåˆ(åŸºäºSHAPæ€»ä½“æ¯”ä¾‹)'
        },
        {
            'name': 'E-M3-TimeAdaptive',
            'wind_cols': ['ec_wind_speed_10m', 'ec_wind_speed_70m'],
            'adaptive_weights': True,
            'description': 'ECæ˜¼å¤œåŠ¨æ€æƒé‡èåˆ(åŸºäºSHAPæ˜¼å¤œåˆ†æ)'
        },
        
        # èåˆè¯•éªŒ
        {
            'name': 'Fusion-M1',
            'wind_cols': ['ec_wind_speed_10m', 'ec_wind_speed_70m', 'gfs_wind_speed_10m', 'gfs_wind_speed_70m'],
            'fusion_strategy': 'corrected_average',
            'description': 'è”åˆæ ¡æ­£åèåˆ'
        },
        {
            'name': 'Fusion-M2',
            'wind_cols': ['ec_wind_speed_10m', 'ec_wind_speed_70m', 'gfs_wind_speed_10m', 'gfs_wind_speed_70m'],
            'fusion_strategy': 'optimal_adaptive',
            'description': 'åŸºäºè¯•éªŒç»“æœçš„åŠ¨æ€æƒé‡èåˆ(ECä¸»å¯¼+SHAPæ˜¼å¤œæƒé‡)'
        }
    ]
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = []
    
    for i, exp_config in enumerate(experiments, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ”¬ è¯•éªŒ {i}/14: {exp_config['name']}")
        print(f"ğŸ“ æè¿°: {exp_config['description']}")
        print(f"{'='*60}")
        
        try:
            # è¿è¡Œå•ä¸ªè¯•éªŒ
            save_dir = os.path.join(base_save_dir, exp_config['name'])
            metrics = run_single_experiment(data_path, save_dir, indices_path, exp_config)
            
            # è®°å½•ç»“æœ
            result = {
                'experiment': exp_config['name'],
                'description': exp_config['description'],
                'RMSE': metrics['RMSE'],
                'Correlation': metrics['Correlation']
            }
            all_results.append(result)
            
            print(f"âœ… {exp_config['name']} å®Œæˆ")
            print(f"   RMSE: {metrics['RMSE']:.4f}")
            print(f"   ç›¸å…³ç³»æ•°: {metrics['Correlation']:.4f}")
            
        except Exception as e:
            print(f"âŒ {exp_config['name']} å¤±è´¥: {str(e)}")
            result = {
                'experiment': exp_config['name'],
                'description': exp_config['description'],
                'RMSE': None,
                'Correlation': None,
                'error': str(e)
            }
            all_results.append(result)
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    create_summary_report(all_results, base_save_dir)
    
    print(f"\n{'='*80}")
    print(f"ğŸ‰ æ‰€æœ‰è¯•éªŒå®Œæˆ!")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {base_save_dir}")
    print(f"ğŸ“Š æ±‡æ€»æŠ¥å‘Š: {base_save_dir}/experiment_summary.csv")
    print(f"{'='*80}")
    
    return all_results

def create_summary_report(all_results, base_save_dir):
    """åˆ›å»ºæ±‡æ€»æŠ¥å‘Š"""
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(all_results)
    
    # æŒ‰RMSEæ’åº
    df_valid = df[df['RMSE'].notna()].copy()
    df_valid = df_valid.sort_values('RMSE')
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    df.to_csv(os.path.join(base_save_dir, 'experiment_summary.csv'), index=False)
    
    # åˆ›å»ºæ’åæŠ¥å‘Š
    print(f"\nğŸ“Š è¯•éªŒç»“æœæ’å (æŒ‰RMSEæ’åº):")
    print(f"{'æ’å':<4} {'è¯•éªŒåç§°':<20} {'RMSE':<10} {'ç›¸å…³ç³»æ•°':<10} {'æè¿°'}")
    print(f"-" * 80)
    
    for i, (_, row) in enumerate(df_valid.iterrows(), 1):
        print(f"{i:<4} {row['experiment']:<20} {row['RMSE']:<10.4f} {row['Correlation']:<10.4f} {row['description']}")
    
    # ä¿å­˜æ’å
    with open(os.path.join(base_save_dir, 'ranking_summary.txt'), 'w') as f:
        f.write("è¯•éªŒç»“æœæ’å (æŒ‰RMSEæ’åº):\n")
        f.write("="*80 + "\n")
        for i, (_, row) in enumerate(df_valid.iterrows(), 1):
            f.write(f"{i}. {row['experiment']}: RMSE={row['RMSE']:.4f}, Corr={row['Correlation']:.4f}\n")
    
    # åˆ†ææœ€ä½³ç­–ç•¥
    if len(df_valid) > 0:
        best_result = df_valid.iloc[0]
        print(f"\nğŸ† æœ€ä½³è¯•éªŒ: {best_result['experiment']}")
        print(f"   RMSE: {best_result['RMSE']:.4f}")
        print(f"   ç›¸å…³ç³»æ•°: {best_result['Correlation']:.4f}")
        print(f"   ç­–ç•¥: {best_result['description']}")

if __name__ == "__main__":
    # é…ç½®è·¯å¾„
    DATA_PATH = "/Users/xiaxin/work/WindForecast_Project/01_Data/processed/imputed_data/changma_imputed_complete.csv"
    BASE_SAVE_DIR = "/Users/xiaxin/work/WindForecast_Project/03_Results/third_part_experiments"
    INDICES_PATH = "/Users/xiaxin/work/WindForecast_Project/03_Results/third_part_experiments/train_test_split.json"
    
    # è¿è¡Œæ‰€æœ‰è¯•éªŒ
    results = run_all_experiments(DATA_PATH, BASE_SAVE_DIR, INDICES_PATH)
    
    print(f"\nğŸ’¡ è¯•éªŒå®Œæˆï¼")
    print(f"ç°åœ¨å¯ä»¥åˆ†æä¸åŒç­–ç•¥çš„æ•ˆæœï¼Œæ‰¾å‡ºæœ€ä¼˜çš„é£é€Ÿèåˆæ–¹æ¡ˆï¼")