#!/usr/bin/env python3
"""
é¢„æŠ¥åˆ‡å˜ä¸çœŸå®åˆ‡å˜å…³ç³»åˆ†æå™¨
ç›®æ ‡ï¼šéªŒè¯ECã€GFSã€EC+GFSå¹³å‡çš„é¢„æŠ¥åˆ‡å˜æ˜¯å¦å¯ä»¥ä½œä¸ºçœŸå®åˆ‡å˜çš„ä»£ç†å˜é‡
é‡ç‚¹ï¼šæ‰¾å‡ºæœ€æ¥è¿‘çœŸå®åˆ‡å˜çš„é¢„æŠ¥åˆ‡å˜ï¼Œç”¨äºæƒé‡åˆ†é…ç­–ç•¥
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error
from scipy.stats import pearsonr, spearmanr
import seaborn as sns
import os
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®è‹±æ–‡æ˜¾ç¤º
plt.rcParams['font.family'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']
plt.rcParams['axes.unicode_minus'] = False

def analyze_forecast_vs_observed_shear(data_path, save_path):
    """åˆ†æé¢„æŠ¥åˆ‡å˜ä¸è§‚æµ‹åˆ‡å˜çš„å…³ç³»"""
    print("=" * 80)
    print("ğŸŒ¬ï¸ é¢„æŠ¥åˆ‡å˜ä¸çœŸå®åˆ‡å˜å…³ç³»åˆ†æ")
    print("ç›®æ ‡ï¼šæ‰¾å‡ºæœ€æ¥è¿‘çœŸå®åˆ‡å˜çš„é¢„æŠ¥åˆ‡å˜ï¼Œç”¨ä½œæƒé‡åˆ†é…çš„æ¡¥æ¢å˜é‡")
    print("å¯¹æ¯”ï¼šECé¢„æŠ¥åˆ‡å˜ vs GFSé¢„æŠ¥åˆ‡å˜ vs EC+GFSå¹³å‡åˆ‡å˜")
    print("=" * 80)
    
    os.makedirs(save_path, exist_ok=True)
    
    # 1. æ•°æ®åŠ è½½
    print("\nğŸ”„ æ­¥éª¤1: æ•°æ®åŠ è½½")
    data = pd.read_csv(data_path)
    data['datetime'] = pd.to_datetime(data['datetime'])
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {data.shape}")
    
    # 2. æ£€æŸ¥å¿…éœ€åˆ—
    required_cols = [
        'obs_wind_speed_10m', 'obs_wind_speed_70m',
        'ec_wind_speed_10m', 'ec_wind_speed_70m', 
        'gfs_wind_speed_10m', 'gfs_wind_speed_70m'
    ]
    
    missing_cols = [col for col in required_cols if col not in data.columns]
    if missing_cols:
        print(f"âŒ ç¼ºå°‘å¿…éœ€åˆ—: {missing_cols}")
        return False
    
    print("âœ… æ‰€æœ‰å¿…éœ€åˆ—éƒ½å­˜åœ¨")
    
    # 3. æ•°æ®æ¸…ç†
    print("\nğŸ”„ æ­¥éª¤2: æ•°æ®æ¸…ç†")
    
    # æ¸…ç†å¼‚å¸¸å€¼
    for col in required_cols:
        data[col] = data[col].where((data[col] >= 0) & (data[col] <= 50))
    
    # ç­›é€‰æœ‰æ•ˆæ•°æ®
    valid_obs = (data['obs_wind_speed_10m'] > 0.5) & (data['obs_wind_speed_70m'] > 0.5)
    valid_ec = (data['ec_wind_speed_10m'] > 0.5) & (data['ec_wind_speed_70m'] > 0.5)
    valid_gfs = (data['gfs_wind_speed_10m'] > 0.5) & (data['gfs_wind_speed_70m'] > 0.5)
    
    # å»é™¤NaN
    valid_obs = valid_obs & (~data['obs_wind_speed_10m'].isna()) & (~data['obs_wind_speed_70m'].isna())
    valid_ec = valid_ec & (~data['ec_wind_speed_10m'].isna()) & (~data['ec_wind_speed_70m'].isna())
    valid_gfs = valid_gfs & (~data['gfs_wind_speed_10m'].isna()) & (~data['gfs_wind_speed_70m'].isna())
    
    # åªä¿ç•™æ‰€æœ‰æ•°æ®éƒ½æœ‰æ•ˆçš„æ ·æœ¬
    valid_all = valid_obs & valid_ec & valid_gfs
    data = data[valid_all].copy()
    
    print(f"æœ‰æ•ˆæ•°æ®ç‚¹: {len(data)} (æ¸…ç†å)")
    
    # 4. è®¡ç®—é£åˆ‡å˜
    print("\nğŸ”„ æ­¥éª¤3: è®¡ç®—é£åˆ‡å˜")
    
    # è§‚æµ‹é£åˆ‡å˜ (alphaæ³• - å¯¹æ•°é£å»“çº¿)
    data['obs_shear_alpha'] = np.log(data['obs_wind_speed_70m'] / data['obs_wind_speed_10m']) / np.log(70 / 10)
    
    # ECé¢„æŠ¥é£åˆ‡å˜
    data['ec_shear_alpha'] = np.log(data['ec_wind_speed_70m'] / data['ec_wind_speed_10m']) / np.log(70 / 10)
    
    # GFSé¢„æŠ¥é£åˆ‡å˜
    data['gfs_shear_alpha'] = np.log(data['gfs_wind_speed_70m'] / data['gfs_wind_speed_10m']) / np.log(70 / 10)
    
    # EC+GFSå¹³å‡é£åˆ‡å˜
    avg_10m = (data['ec_wind_speed_10m'] + data['gfs_wind_speed_10m']) / 2
    avg_70m = (data['ec_wind_speed_70m'] + data['gfs_wind_speed_70m']) / 2
    data['avg_shear_alpha'] = np.log(avg_70m / avg_10m) / np.log(70 / 10)
    
    # å¦ä¸€ç§å¹³å‡æ–¹æ³•ï¼šç›´æ¥å¯¹åˆ‡å˜æ±‚å¹³å‡
    data['avg_shear_alpha_direct'] = (data['ec_shear_alpha'] + data['gfs_shear_alpha']) / 2
    
    # è¿‡æ»¤å¼‚å¸¸åˆ‡å˜å€¼
    shear_cols = ['obs_shear_alpha', 'ec_shear_alpha', 'gfs_shear_alpha', 'avg_shear_alpha', 'avg_shear_alpha_direct']
    for col in shear_cols:
        valid_shear = (~np.isnan(data[col])) & (~np.isinf(data[col])) & (data[col] > -1) & (data[col] < 2)
        data = data[valid_shear].copy()
    
    print(f"æœ€ç»ˆæœ‰æ•ˆæ•°æ®ç‚¹: {len(data)}")
    
    # æ·»åŠ æ—¶é—´ä¿¡æ¯
    data['hour'] = data['datetime'].dt.hour
    data['is_daytime'] = (data['hour'] >= 6) & (data['hour'] < 18)
    
    # 5. ç›¸å…³æ€§åˆ†æ
    print("\nğŸ”„ æ­¥éª¤4: ç›¸å…³æ€§åˆ†æ")
    
    obs_shear = data['obs_shear_alpha']
    ec_shear = data['ec_shear_alpha']
    gfs_shear = data['gfs_shear_alpha']
    avg_shear = data['avg_shear_alpha']
    avg_shear_direct = data['avg_shear_alpha_direct']
    
    # è®¡ç®—ç›¸å…³ç³»æ•°
    ec_corr, ec_p = pearsonr(obs_shear, ec_shear)
    gfs_corr, gfs_p = pearsonr(obs_shear, gfs_shear)
    avg_corr, avg_p = pearsonr(obs_shear, avg_shear)
    avg_direct_corr, avg_direct_p = pearsonr(obs_shear, avg_shear_direct)
    
    print(f"é¢„æŠ¥åˆ‡å˜ä¸è§‚æµ‹åˆ‡å˜çš„ç›¸å…³æ€§:")
    print(f"  ECé¢„æŠ¥åˆ‡å˜:      r = {ec_corr:.4f}, p = {ec_p:.6f}")
    print(f"  GFSé¢„æŠ¥åˆ‡å˜:     r = {gfs_corr:.4f}, p = {gfs_p:.6f}")
    print(f"  EC+GFSå¹³å‡åˆ‡å˜:  r = {avg_corr:.4f}, p = {avg_p:.6f}")
    print(f"  åˆ‡å˜ç›´æ¥å¹³å‡:    r = {avg_direct_corr:.4f}, p = {avg_direct_p:.6f}")
    
    # 6. é¢„æŠ¥ç²¾åº¦åˆ†æ
    print("\nğŸ”„ æ­¥éª¤5: é¢„æŠ¥ç²¾åº¦åˆ†æ")
    
    # RMSE
    ec_rmse = np.sqrt(mean_squared_error(obs_shear, ec_shear))
    gfs_rmse = np.sqrt(mean_squared_error(obs_shear, gfs_shear))
    avg_rmse = np.sqrt(mean_squared_error(obs_shear, avg_shear))
    avg_direct_rmse = np.sqrt(mean_squared_error(obs_shear, avg_shear_direct))
    
    # MAE
    ec_mae = mean_absolute_error(obs_shear, ec_shear)
    gfs_mae = mean_absolute_error(obs_shear, gfs_shear)
    avg_mae = mean_absolute_error(obs_shear, avg_shear)
    avg_direct_mae = mean_absolute_error(obs_shear, avg_shear_direct)
    
    # åå·®
    ec_bias = np.mean(ec_shear - obs_shear)
    gfs_bias = np.mean(gfs_shear - obs_shear)
    avg_bias = np.mean(avg_shear - obs_shear)
    avg_direct_bias = np.mean(avg_shear_direct - obs_shear)
    
    print(f"é¢„æŠ¥åˆ‡å˜ç²¾åº¦æŒ‡æ ‡ (è¶Šå°è¶Šå¥½):")
    print(f"                    RMSE     MAE      åå·®")
    print(f"  ECé¢„æŠ¥åˆ‡å˜:      {ec_rmse:.4f}  {ec_mae:.4f}  {ec_bias:+.4f}")
    print(f"  GFSé¢„æŠ¥åˆ‡å˜:     {gfs_rmse:.4f}  {gfs_mae:.4f}  {gfs_bias:+.4f}")
    print(f"  EC+GFSå¹³å‡åˆ‡å˜:  {avg_rmse:.4f}  {avg_mae:.4f}  {avg_bias:+.4f}")
    print(f"  åˆ‡å˜ç›´æ¥å¹³å‡:    {avg_direct_rmse:.4f}  {avg_direct_mae:.4f}  {avg_direct_bias:+.4f}")
    
    # 7. æ‰¾å‡ºæœ€ä¼˜é¢„æŠ¥åˆ‡å˜
    print("\nğŸ”„ æ­¥éª¤6: æœ€ä¼˜é¢„æŠ¥åˆ‡å˜è¯„ä¼°")
    
    correlations = {'EC': ec_corr, 'GFS': gfs_corr, 'EC+GFSå¹³å‡': avg_corr, 'åˆ‡å˜ç›´æ¥å¹³å‡': avg_direct_corr}
    rmses = {'EC': ec_rmse, 'GFS': gfs_rmse, 'EC+GFSå¹³å‡': avg_rmse, 'åˆ‡å˜ç›´æ¥å¹³å‡': avg_direct_rmse}
    
    best_corr_model = max(correlations, key=correlations.get)
    best_rmse_model = min(rmses, key=rmses.get)
    
    print(f"ğŸ† æœ€é«˜ç›¸å…³æ€§: {best_corr_model} (r = {correlations[best_corr_model]:.4f})")
    print(f"ğŸ† æœ€å°è¯¯å·®:   {best_rmse_model} (RMSE = {rmses[best_rmse_model]:.4f})")
    
    # ç»¼åˆè¯„åˆ† (ç›¸å…³æ€§æƒé‡0.6ï¼Œç²¾åº¦æƒé‡0.4)
    scores = {}
    for model in correlations.keys():
        # æ ‡å‡†åŒ–åˆ†æ•°
        corr_score = correlations[model]  # ç›¸å…³æ€§è¶Šé«˜è¶Šå¥½
        rmse_score = 1 / (1 + rmses[model])  # RMSEè¶Šå°è¶Šå¥½
        scores[model] = 0.6 * corr_score + 0.4 * rmse_score
    
    best_overall_model = max(scores, key=scores.get)
    print(f"ğŸ† ç»¼åˆæœ€ä¼˜:   {best_overall_model} (ç»¼åˆå¾—åˆ† = {scores[best_overall_model]:.4f})")
    
    # 8. åˆ†æ—¶æ®µåˆ†æ
    print("\nğŸ”„ æ­¥éª¤7: åˆ†æ—¶æ®µåˆ†æ")
    
    time_periods = {'ç™½å¤©(6-18h)': True, 'å¤œé—´(18-6h)': False}
    time_results = {}
    
    for period_name, is_day in time_periods.items():
        period_data = data[data['is_daytime'] == is_day]
        if len(period_data) > 50:
            obs_period = period_data['obs_shear_alpha']
            ec_period = period_data['ec_shear_alpha']
            gfs_period = period_data['gfs_shear_alpha']
            avg_period = period_data['avg_shear_alpha']
            
            ec_corr_period, _ = pearsonr(obs_period, ec_period)
            gfs_corr_period, _ = pearsonr(obs_period, gfs_period)
            avg_corr_period, _ = pearsonr(obs_period, avg_period)
            
            time_results[period_name] = {
                'EC': ec_corr_period,
                'GFS': gfs_corr_period,
                'EC+GFSå¹³å‡': avg_corr_period,
                'sample_size': len(period_data)
            }
            
            print(f"  {period_name} (N={len(period_data)}):")
            print(f"    EC: {ec_corr_period:.3f}, GFS: {gfs_corr_period:.3f}, å¹³å‡: {avg_corr_period:.3f}")
    
    # 9. å¯è§†åŒ–åˆ†æ
    print("\nğŸ”„ æ­¥éª¤8: åˆ›å»ºå¯è§†åŒ–")
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # 1. EC vs è§‚æµ‹æ•£ç‚¹å›¾
    ax1 = axes[0, 0]
    ax1.scatter(obs_shear, ec_shear, alpha=0.6, s=15, c='blue')
    ax1.plot([-1, 2], [-1, 2], 'r--', alpha=0.8, label='å®Œç¾é¢„æŠ¥çº¿')
    ax1.set_xlabel('è§‚æµ‹é£åˆ‡å˜ Alpha')
    ax1.set_ylabel('ECé¢„æŠ¥é£åˆ‡å˜ Alpha')
    ax1.set_title(f'ECé¢„æŠ¥ vs è§‚æµ‹\nr={ec_corr:.3f}, RMSE={ec_rmse:.3f}')
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    
    # 2. GFS vs è§‚æµ‹æ•£ç‚¹å›¾
    ax2 = axes[0, 1]
    ax2.scatter(obs_shear, gfs_shear, alpha=0.6, s=15, c='green')
    ax2.plot([-1, 2], [-1, 2], 'r--', alpha=0.8, label='å®Œç¾é¢„æŠ¥çº¿')
    ax2.set_xlabel('è§‚æµ‹é£åˆ‡å˜ Alpha')
    ax2.set_ylabel('GFSé¢„æŠ¥é£åˆ‡å˜ Alpha')
    ax2.set_title(f'GFSé¢„æŠ¥ vs è§‚æµ‹\nr={gfs_corr:.3f}, RMSE={gfs_rmse:.3f}')
    ax2.grid(True, alpha=0.3)
    ax2.legend()
    
    # 3. å¹³å‡ vs è§‚æµ‹æ•£ç‚¹å›¾
    ax3 = axes[0, 2]
    ax3.scatter(obs_shear, avg_shear, alpha=0.6, s=15, c='orange')
    ax3.plot([-1, 2], [-1, 2], 'r--', alpha=0.8, label='å®Œç¾é¢„æŠ¥çº¿')
    ax3.set_xlabel('è§‚æµ‹é£åˆ‡å˜ Alpha')
    ax3.set_ylabel('EC+GFSå¹³å‡é£åˆ‡å˜ Alpha')
    ax3.set_title(f'EC+GFSå¹³å‡ vs è§‚æµ‹\nr={avg_corr:.3f}, RMSE={avg_rmse:.3f}')
    ax3.grid(True, alpha=0.3)
    ax3.legend()
    
    # 4. ç›¸å…³æ€§å¯¹æ¯”æŸ±çŠ¶å›¾
    ax4 = axes[1, 0]
    models = ['EC', 'GFS', 'EC+GFS\nå¹³å‡', 'åˆ‡å˜ç›´æ¥\nå¹³å‡']
    corrs = [ec_corr, gfs_corr, avg_corr, avg_direct_corr]
    colors = ['blue', 'green', 'orange', 'red']
    
    bars = ax4.bar(models, corrs, color=colors, alpha=0.7)
    ax4.set_ylabel('ç›¸å…³ç³»æ•°')
    ax4.set_title('é¢„æŠ¥åˆ‡å˜ç›¸å…³æ€§å¯¹æ¯”')
    ax4.grid(True, alpha=0.3)
    ax4.set_ylim(0, 1)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, corr in zip(bars, corrs):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{corr:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # 5. RMSEå¯¹æ¯”æŸ±çŠ¶å›¾
    ax5 = axes[1, 1]
    rmses_list = [ec_rmse, gfs_rmse, avg_rmse, avg_direct_rmse]
    
    bars2 = ax5.bar(models, rmses_list, color=colors, alpha=0.7)
    ax5.set_ylabel('RMSE')
    ax5.set_title('é¢„æŠ¥åˆ‡å˜è¯¯å·®å¯¹æ¯”')
    ax5.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, rmse in zip(bars2, rmses_list):
        ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,
                f'{rmse:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # 6. åå·®åˆ†å¸ƒå›¾
    ax6 = axes[1, 2]
    biases = [ec_shear - obs_shear, gfs_shear - obs_shear, avg_shear - obs_shear]
    labels = ['ECåå·®', 'GFSåå·®', 'å¹³å‡åå·®']
    colors_hist = ['blue', 'green', 'orange']
    
    ax6.hist(biases, bins=30, alpha=0.6, label=labels, color=colors_hist)
    ax6.axvline(x=0, color='red', linestyle='--', alpha=0.8, label='é›¶åå·®çº¿')
    ax6.set_xlabel('é¢„æŠ¥åå·® (é¢„æŠ¥å€¼ - è§‚æµ‹å€¼)')
    ax6.set_ylabel('é¢‘æ¬¡')
    ax6.set_title('é¢„æŠ¥åˆ‡å˜åå·®åˆ†å¸ƒ')
    ax6.legend()
    ax6.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f"{save_path}/forecast_shear_analysis.png", dpi=300, bbox_inches='tight')
    plt.show()
    
    # 10. ç»“è®ºå’Œå»ºè®®
    print("\n" + "=" * 80)
    print("ğŸ“‹ åˆ†æç»“è®ºå’Œå»ºè®®")
    print("=" * 80)
    
    best_corr_value = correlations[best_corr_model]
    best_rmse_value = rmses[best_rmse_model]
    
    print(f"\nğŸ” ä¸»è¦å‘ç°:")
    print(f"  ç›¸å…³æ€§æœ€é«˜: {best_corr_model} (r = {best_corr_value:.4f})")
    print(f"  è¯¯å·®æœ€å°:   {best_rmse_model} (RMSE = {best_rmse_value:.4f})")
    print(f"  ç»¼åˆæœ€ä¼˜:   {best_overall_model}")
    
    # æ¡¥æ¢å˜é‡è¯„ä¼°
    print(f"\nğŸŒ‰ æ¡¥æ¢å˜é‡è¯„ä¼°:")
    if best_corr_value > 0.8:
        bridge_quality = "âœ… ä¼˜ç§€æ¡¥æ¢å˜é‡"
        recommendation = f"å¼ºçƒˆå»ºè®®ä½¿ç”¨{best_corr_model}é¢„æŠ¥åˆ‡å˜ç›´æ¥æŒ‡å¯¼æƒé‡åˆ†é…"
        usable = True
    elif best_corr_value > 0.6:
        bridge_quality = "âœ… è‰¯å¥½æ¡¥æ¢å˜é‡"
        recommendation = f"å»ºè®®ä½¿ç”¨{best_corr_model}é¢„æŠ¥åˆ‡å˜æŒ‡å¯¼æƒé‡åˆ†é…"
        usable = True
    elif best_corr_value > 0.4:
        bridge_quality = "âš ï¸ ä¸­ç­‰æ¡¥æ¢å˜é‡"
        recommendation = f"å¯ä»¥å°è¯•ä½¿ç”¨{best_corr_model}é¢„æŠ¥åˆ‡å˜ï¼Œä½†éœ€è¦è°¨æ…"
        usable = True
    elif best_corr_value > 0.2:
        bridge_quality = "âš ï¸ è¾ƒå¼±æ¡¥æ¢å˜é‡"
        recommendation = "å»ºè®®ä½œä¸ºè¾…åŠ©å‚è€ƒï¼Œä¸è¦å®Œå…¨ä¾èµ–"
        usable = False
    else:
        bridge_quality = "âŒ ä¸é€‚åˆä½œæ¡¥æ¢å˜é‡"
        recommendation = "å¯»æ‰¾å…¶ä»–æ›´å¯é çš„æ–¹æ³•"
        usable = False
    
    print(f"  {bridge_quality}")
    print(f"  å»ºè®®: {recommendation}")
    
    # å…·ä½“æƒé‡ç­–ç•¥
    if usable and best_corr_value > 0.4:
        print(f"\nğŸ”§ å…·ä½“æƒé‡åˆ†é…ç­–ç•¥ (åŸºäº{best_overall_model}é¢„æŠ¥åˆ‡å˜):")
        print("```python")
        print("def calculate_weights_by_forecast_shear(forecast_shear_alpha):")
        print("    # åŸºäºé¢„æŠ¥åˆ‡å˜åŠ¨æ€è°ƒæ•´10må’Œ70mæƒé‡")
        print("    if forecast_shear_alpha < 0.05:")
        print("        # æå¼±åˆ‡å˜ - 10mé£é€Ÿæ›´é‡è¦")
        print("        w_10m, w_70m = 0.85, 0.15")
        print("    elif forecast_shear_alpha < 0.15:")
        print("        # å¼±åˆ‡å˜ - 10mé£é€Ÿä¸»å¯¼")
        print("        w_10m, w_70m = 0.75, 0.25")
        print("    elif forecast_shear_alpha < 0.25:")
        print("        # ä¸­ç­‰åˆ‡å˜ - å‡è¡¡æƒé‡")
        print("        w_10m, w_70m = 0.60, 0.40")
        print("    elif forecast_shear_alpha < 0.35:")
        print("        # è¾ƒå¼ºåˆ‡å˜ - 70mé‡è¦æ€§å¢åŠ ")
        print("        w_10m, w_70m = 0.45, 0.55")
        print("    else:")
        print("        # å¼ºåˆ‡å˜ - 70mé£é€Ÿæ›´é‡è¦")
        print("        w_10m, w_70m = 0.35, 0.65")
        print("    return w_10m, w_70m")
        print("```")
        print("\nåŸç†: åˆ‡å˜è¶Šå¼ºï¼Œè¡¨ç¤ºé«˜åº¦é—´é£é€Ÿå·®å¼‚è¶Šå¤§ï¼Œè½®æ¯‚é«˜åº¦(70m)é£é€Ÿè¶Šé‡è¦")
    
    # 11. ä¿å­˜ç»“æœ
    print(f"\nğŸ“ ä¿å­˜ç»“æœ")
    
    # è¯¦ç»†æ•°æ®
    results_df = data[['datetime', 'obs_shear_alpha', 'ec_shear_alpha', 'gfs_shear_alpha', 
                      'avg_shear_alpha', 'avg_shear_alpha_direct', 'hour', 'is_daytime']].copy()
    results_df['ec_bias'] = results_df['ec_shear_alpha'] - results_df['obs_shear_alpha']
    results_df['gfs_bias'] = results_df['gfs_shear_alpha'] - results_df['obs_shear_alpha']
    results_df['avg_bias'] = results_df['avg_shear_alpha'] - results_df['obs_shear_alpha']
    
    results_df.to_csv(f"{save_path}/forecast_shear_detailed_data.csv", index=False)
    
    # æ±‡æ€»æŠ¥å‘Š
    summary_report = {
        'analysis_summary': {
            'sample_size': len(data),
            'best_correlation_model': best_corr_model,
            'best_correlation_value': best_corr_value,
            'best_rmse_model': best_rmse_model,
            'best_rmse_value': best_rmse_value,
            'best_overall_model': best_overall_model,
            'bridge_quality': bridge_quality,
            'recommendation': recommendation,
            'usable_as_bridge': usable
        },
        'correlation_results': correlations,
        'accuracy_results': {
            'RMSE': rmses,
            'MAE': {'EC': ec_mae, 'GFS': gfs_mae, 'EC+GFSå¹³å‡': avg_mae, 'åˆ‡å˜ç›´æ¥å¹³å‡': avg_direct_mae},
            'BIAS': {'EC': ec_bias, 'GFS': gfs_bias, 'EC+GFSå¹³å‡': avg_bias, 'åˆ‡å˜ç›´æ¥å¹³å‡': avg_direct_bias}
        },
        'time_period_results': time_results
    }
    
    import json
    with open(f"{save_path}/forecast_shear_analysis_report.json", 'w', encoding='utf-8') as f:
        json.dump(summary_report, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"  âœ… è¯¦ç»†æ•°æ®: forecast_shear_detailed_data.csv")
    print(f"  âœ… åˆ†æå›¾è¡¨: forecast_shear_analysis.png")
    print(f"  âœ… æ±‡æ€»æŠ¥å‘Š: forecast_shear_analysis_report.json")
    
    return True

if __name__ == "__main__":
    # é…ç½®è·¯å¾„
    DATA_PATH = "/Users/xiaxin/work/WindForecast_Project/01_Data/processed/imputed_data/changma_imputed_complete.csv"
    SAVE_PATH = "/Users/xiaxin/work/WindForecast_Project/03_Results/forecast_shear_bridge_analysis"
    
    success = analyze_forecast_vs_observed_shear(DATA_PATH, SAVE_PATH)
    
    if success:
        print("\nğŸ‰ é¢„æŠ¥åˆ‡å˜æ¡¥æ¢å˜é‡åˆ†æå®Œæˆ!")
        print("\nğŸ’¡ å¦‚æœåˆ†ææ˜¾ç¤ºé¢„æŠ¥åˆ‡å˜å¯ä»¥ä½œä¸ºæ¡¥æ¢å˜é‡ï¼Œ")
        print("   æ‚¨å°±å¯ä»¥åœ¨ç¬¬ä¸‰éƒ¨åˆ†è¯•éªŒä¸­ä½¿ç”¨é¢„æŠ¥åˆ‡å˜æ¥åŠ¨æ€è°ƒæ•´æƒé‡äº†ï¼")
    else:
        print("\nâš ï¸ åˆ†æå¤±è´¥")